{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "b2NSA-U_0DhS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([ToTensor(), Normalize((0.5), (0.5))])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform)"
      ],
      "metadata": {
        "id": "8Y-slf1KBtep"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting datasets for two tasks\n",
        "train_dataset_task1 = Subset(train_dataset, [i for i in range(len(train_dataset)) if train_dataset.targets[i] <= 5])\n",
        "train_dataset_task2 = Subset(train_dataset, [i for i in range(len(train_dataset)) if train_dataset.targets[i] > 5])\n",
        "test_dataset_task1 = Subset(test_dataset, [i for i in range(len(test_dataset)) if test_dataset.targets[i] < 5])\n",
        "test_dataset_task2 = Subset(test_dataset, [i for i in range(len(test_dataset)) if test_dataset.targets[i] >= 5])"
      ],
      "metadata": {
        "id": "0HCo5OQKCscZ"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUz-SUDKsZuX",
        "outputId": "e2ebe115-60ad-454e-f61c-775c70ca1909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First: {0, 1, 2, 3, 4, 5}\n",
            "Second: {8, 9, 6, 7}\n"
          ]
        }
      ],
      "source": [
        "unique_labels_1 = set()\n",
        "unique_labels_2 = set()\n",
        "\n",
        "for _, target in train_dataset_task1:\n",
        "    unique_labels_1.add(target)\n",
        "\n",
        "for _, target in train_dataset_task2:\n",
        "    unique_labels_2.add(target)\n",
        "\n",
        "print(f\"First: {unique_labels_1}\")\n",
        "print(f\"Second: {unique_labels_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_J6o_FR2Mh8",
        "outputId": "77732968-ed5a-4f23-c617-c62e0f0dfdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "SHape of y: torch.Size([64]), dtype: torch.int64\n"
          ]
        }
      ],
      "source": [
        "train_dataloader_task1 = DataLoader(train_dataset_task1, batch_size=64, shuffle=True)\n",
        "test_dataloader_task1 = DataLoader(test_dataset_task1, batch_size=256, shuffle=False)\n",
        "\n",
        "for X, y in train_dataloader_task1:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"SHape of y: {y.shape}, dtype: {y.dtype}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "OFGpnxZq2NYt"
      },
      "outputs": [],
      "source": [
        "train_dataloader_task2 = DataLoader(train_dataset_task2, batch_size=64, shuffle=True)\n",
        "test_dataloader_task2 = DataLoader(test_dataset_task2, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7raJ2RC2Toq",
        "outputId": "634054da-eaf4-40d8-85de-6617c21afc5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "C5nQoXPk2Wbo"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    #     self._initialize_weights()\n",
        "\n",
        "    # def _initialize_weights(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Linear):\n",
        "    #             nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "    #         elif isinstance(m, nn.Conv2d):\n",
        "    #             nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "tyhb4Zo4230I"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "hZThNdfT26Kg"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "O8cy0wv-28X4"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork(num_classes=10, hidden_size=512).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO9q7xFU4UUI",
        "outputId": "51f99b4c-fe5a-41e6-c5d9-24436b75dd26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Loss: 2.318275,    64/36000\n",
            "Loss: 0.276660,  6464/36000\n",
            "Loss: 0.346340, 12864/36000\n",
            "Loss: 0.211132, 19264/36000\n",
            "Loss: 0.343147, 25664/36000\n",
            "Loss: 0.272822, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 87.3, Avg Loss: 0.343310\n",
            "\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Loss: 0.367829,    64/36000\n",
            "Loss: 0.167150,  6464/36000\n",
            "Loss: 0.339057, 12864/36000\n",
            "Loss: 0.272547, 19264/36000\n",
            "Loss: 0.201382, 25664/36000\n",
            "Loss: 0.185659, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 87.1, Avg Loss: 0.342122\n",
            "\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Loss: 0.228486,    64/36000\n",
            "Loss: 0.124392,  6464/36000\n",
            "Loss: 0.233071, 12864/36000\n",
            "Loss: 0.249532, 19264/36000\n",
            "Loss: 0.357625, 25664/36000\n",
            "Loss: 0.437023, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 88.6, Avg Loss: 0.306197\n",
            "\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Loss: 0.243054,    64/36000\n",
            "Loss: 0.124935,  6464/36000\n",
            "Loss: 0.124939, 12864/36000\n",
            "Loss: 0.110449, 19264/36000\n",
            "Loss: 0.140190, 25664/36000\n",
            "Loss: 0.248078, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 88.2, Avg Loss: 0.313941\n",
            "\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Loss: 0.135470,    64/36000\n",
            "Loss: 0.100860,  6464/36000\n",
            "Loss: 0.182589, 12864/36000\n",
            "Loss: 0.181453, 19264/36000\n",
            "Loss: 0.178594, 25664/36000\n",
            "Loss: 0.167324, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.2, Avg Loss: 0.285789\n",
            "\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Loss: 0.264785,    64/36000\n",
            "Loss: 0.128500,  6464/36000\n",
            "Loss: 0.217538, 12864/36000\n",
            "Loss: 0.167788, 19264/36000\n",
            "Loss: 0.164272, 25664/36000\n",
            "Loss: 0.298559, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.2, Avg Loss: 0.271135\n",
            "\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Loss: 0.146233,    64/36000\n",
            "Loss: 0.316017,  6464/36000\n",
            "Loss: 0.234697, 12864/36000\n",
            "Loss: 0.268774, 19264/36000\n",
            "Loss: 0.145967, 25664/36000\n",
            "Loss: 0.213602, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 88.9, Avg Loss: 0.303694\n",
            "\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Loss: 0.140032,    64/36000\n",
            "Loss: 0.162745,  6464/36000\n",
            "Loss: 0.282142, 12864/36000\n",
            "Loss: 0.195165, 19264/36000\n",
            "Loss: 0.223862, 25664/36000\n",
            "Loss: 0.202644, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.1, Avg Loss: 0.288266\n",
            "\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Loss: 0.114000,    64/36000\n",
            "Loss: 0.203198,  6464/36000\n",
            "Loss: 0.142017, 12864/36000\n",
            "Loss: 0.190742, 19264/36000\n",
            "Loss: 0.405914, 25664/36000\n",
            "Loss: 0.103446, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.1, Avg Loss: 0.290979\n",
            "\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Loss: 0.209318,    64/36000\n",
            "Loss: 0.214692,  6464/36000\n",
            "Loss: 0.121788, 12864/36000\n",
            "Loss: 0.194875, 19264/36000\n",
            "Loss: 0.093313, 25664/36000\n",
            "Loss: 0.182196, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.8, Avg Loss: 0.286573\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------------\")\n",
        "  train(train_dataloader_task1, model, loss_fn, optimizer)\n",
        "  test(test_dataloader_task1, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "torch.save(model.state_dict(), \"model_old.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "RZmhDHNfIypc"
      },
      "outputs": [],
      "source": [
        "def val(epoch):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(test_dataloader_task1):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_50xg7ygJGMA",
        "outputId": "7416ca27-e681-487f-c7af-4691a1824c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 89.82\n",
            "\n"
          ]
        }
      ],
      "source": [
        "val(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2xq-SiG4Uvg"
      },
      "source": [
        "________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "GPZXh0Ga4rZy"
      },
      "outputs": [],
      "source": [
        "# model = NeuralNetwork()\n",
        "# model.load_state_dict(torch.load(\"model_old.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "LhRRttbd3Bxo"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def get_fisher_diag(model, dataloader, params, empirical=False):\n",
        "    fisher = {}\n",
        "    params_dict = dict(params)\n",
        "    for n, p in deepcopy(params_dict).items():\n",
        "        p.data.zero_()\n",
        "        fisher[n] = p.data.clone().detach().requires_grad_()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for input, gt_label in dataloader:\n",
        "        input, gt_label = input.to(device), gt_label.to(device)\n",
        "        model.zero_grad()\n",
        "        output = model(input)\n",
        "\n",
        "        if empirical:\n",
        "            label = output.max(1)[1]\n",
        "        else:\n",
        "            label = gt_label\n",
        "\n",
        "        # label = gt_label.repeat(output.size(0))\n",
        "        negloglikelihood = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(output, dim=1), label)\n",
        "        negloglikelihood.backward()\n",
        "\n",
        "        for n, p in model.named_parameters():\n",
        "            fisher[n].data += p.grad.data ** 2 / len(dataloader.dataset)\n",
        "\n",
        "    fisher = {n: p for n, p in fisher.items()}\n",
        "    return fisher\n",
        "\n",
        "\n",
        "def get_ewc_loss(model, fisher, p_old):\n",
        "    loss = 0\n",
        "    for n, p in model.named_parameters():\n",
        "        _loss = fisher[n] * (p - p_old[n]) ** 2\n",
        "        loss += _loss.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "RLlF7JXB57n4"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "\n",
        "ewc_lambda = 500_000\n",
        "\n",
        "fisher_matrix = get_fisher_diag(model, train_dataloader_task1, model.named_parameters())\n",
        "prev_params = {n: p.data.clone() for n, p in model.named_parameters()}\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEtwEedKrqPc",
        "outputId": "2a9106ae-4fd7-40ed-ab7f-654d185a6f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1.weight': tensor([[1.6301e-08, 1.6300e-08, 1.6294e-08,  ..., 1.6124e-08, 1.6255e-08,\n",
            "         1.6296e-08],\n",
            "        [6.2606e-07, 6.2606e-07, 6.2592e-07,  ..., 5.8690e-07, 6.1910e-07,\n",
            "         6.2564e-07],\n",
            "        [1.4374e-07, 1.4373e-07, 1.4374e-07,  ..., 1.4306e-07, 1.4363e-07,\n",
            "         1.4371e-07],\n",
            "        ...,\n",
            "        [3.6598e-07, 3.6601e-07, 3.6603e-07,  ..., 3.5811e-07, 3.6451e-07,\n",
            "         3.6574e-07],\n",
            "        [9.1493e-08, 9.1490e-08, 9.1445e-08,  ..., 8.8371e-08, 9.1086e-08,\n",
            "         9.1487e-08],\n",
            "        [3.2441e-07, 3.2441e-07, 3.2436e-07,  ..., 2.9740e-07, 3.1868e-07,\n",
            "         3.2434e-07]], requires_grad=True), 'fc1.bias': tensor([1.6302e-08, 6.2606e-07, 1.4373e-07, 2.4174e-08, 1.9960e-10, 3.0725e-07,\n",
            "        1.1680e-07, 4.1918e-08, 1.0328e-07, 1.5689e-08, 1.6209e-07, 9.6254e-09,\n",
            "        5.4251e-09, 2.2509e-07, 7.0144e-10, 1.1952e-08, 1.5106e-07, 1.6801e-07,\n",
            "        2.1807e-08, 1.8777e-08, 2.1331e-08, 2.3167e-07, 5.8824e-08, 2.0246e-08,\n",
            "        3.3673e-09, 1.1142e-08, 7.7822e-08, 1.3186e-08, 3.6629e-08, 1.4018e-07,\n",
            "        6.5030e-09, 7.7266e-08, 1.1128e-08, 1.2303e-07, 3.5936e-07, 5.8604e-08,\n",
            "        7.3487e-08, 1.0831e-11, 1.2237e-07, 3.4806e-08, 4.7966e-08, 1.4504e-07,\n",
            "        5.9223e-08, 2.5219e-10, 2.8330e-08, 5.9419e-07, 9.2977e-08, 2.1252e-07,\n",
            "        1.2926e-07, 2.9885e-07, 5.0316e-08, 1.0865e-07, 2.9571e-09, 8.0439e-10,\n",
            "        2.4651e-08, 4.5749e-09, 8.6419e-08, 9.8383e-08, 5.1140e-08, 1.1368e-07,\n",
            "        3.0389e-08, 1.3785e-07, 9.0969e-09, 1.1098e-08, 1.3960e-07, 5.7521e-09,\n",
            "        1.0545e-07, 6.7784e-08, 5.4862e-10, 1.1874e-08, 1.1843e-07, 3.4016e-08,\n",
            "        6.8626e-08, 5.3953e-08, 5.7125e-08, 2.3468e-07, 1.1838e-10, 3.9103e-07,\n",
            "        4.9292e-08, 1.2813e-08, 1.3851e-07, 1.7413e-07, 1.1273e-07, 5.1407e-09,\n",
            "        7.9488e-07, 4.0308e-09, 2.1242e-07, 9.6670e-11, 2.2911e-07, 1.3687e-08,\n",
            "        6.4713e-12, 4.3254e-09, 9.1275e-09, 2.1245e-07, 8.4988e-08, 9.6832e-08,\n",
            "        1.7210e-07, 6.2787e-08, 3.0119e-07, 1.2191e-07, 2.0736e-08, 5.0863e-08,\n",
            "        1.7874e-07, 3.9377e-07, 1.1391e-07, 3.2097e-08, 7.2069e-08, 1.0325e-07,\n",
            "        9.3026e-08, 9.6607e-08, 1.0100e-07, 8.1197e-08, 6.2580e-08, 1.0516e-07,\n",
            "        1.3836e-07, 8.2856e-08, 2.0816e-07, 4.4271e-08, 1.5190e-07, 2.8010e-08,\n",
            "        3.9402e-08, 1.1803e-07, 8.5051e-10, 3.5587e-08, 5.1423e-08, 2.7032e-08,\n",
            "        1.9057e-10, 3.5355e-08, 8.2254e-08, 1.7232e-07, 2.4645e-12, 5.2886e-08,\n",
            "        1.2561e-07, 8.7144e-12, 9.4973e-08, 5.2932e-07, 7.4658e-08, 9.1687e-08,\n",
            "        7.0254e-08, 3.2709e-08, 5.4530e-09, 2.2543e-08, 3.3135e-08, 8.0878e-08,\n",
            "        7.8555e-11, 1.2668e-09, 1.2898e-07, 1.1954e-07, 2.5523e-07, 9.8653e-10,\n",
            "        3.2269e-08, 1.6163e-07, 2.2448e-07, 2.5881e-09, 6.6917e-08, 1.0850e-07,\n",
            "        6.1687e-08, 1.5821e-07, 7.5008e-08, 1.4057e-09, 4.4582e-08, 1.0203e-07,\n",
            "        8.7110e-08, 1.3659e-07, 3.2959e-08, 1.2503e-07, 3.3118e-07, 7.6680e-08,\n",
            "        3.0969e-07, 3.0646e-08, 1.8881e-09, 8.3324e-08, 5.7020e-07, 2.1961e-08,\n",
            "        1.2663e-08, 7.1110e-08, 1.0718e-08, 1.7280e-07, 2.8842e-08, 2.1603e-08,\n",
            "        2.1699e-07, 1.2821e-07, 1.1711e-10, 6.9099e-08, 2.7859e-09, 7.4900e-09,\n",
            "        1.4162e-09, 8.2335e-09, 9.1370e-08, 1.6662e-07, 5.2210e-08, 1.4471e-07,\n",
            "        8.7743e-08, 4.5673e-08, 3.1733e-08, 1.3632e-07, 1.0000e-07, 3.4289e-09,\n",
            "        8.1892e-08, 5.2391e-08, 8.6583e-08, 1.2993e-07, 1.2961e-07, 4.9816e-08,\n",
            "        1.5787e-07, 7.0394e-08, 1.0795e-07, 4.4559e-07, 1.5109e-07, 8.0510e-08,\n",
            "        3.6236e-07, 9.8431e-08, 1.4618e-07, 5.7605e-08, 2.3628e-07, 1.6059e-12,\n",
            "        3.8034e-08, 6.9977e-08, 3.1553e-08, 3.6595e-08, 1.8053e-07, 1.1691e-07,\n",
            "        5.7183e-08, 2.9163e-08, 9.3296e-08, 8.6439e-08, 1.7501e-07, 1.1402e-07,\n",
            "        1.7294e-08, 4.8505e-09, 7.3312e-08, 8.6110e-08, 3.7410e-08, 3.4790e-08,\n",
            "        3.9814e-08, 1.5119e-07, 3.4505e-08, 7.5854e-09, 8.7914e-08, 1.8872e-08,\n",
            "        5.8356e-08, 1.8338e-08, 8.6573e-08, 1.8181e-08, 3.9596e-08, 2.1536e-07,\n",
            "        1.1356e-11, 4.4531e-08, 9.1618e-08, 4.0699e-08, 1.1003e-07, 2.1415e-07,\n",
            "        8.9262e-08, 5.2099e-08, 1.7042e-07, 1.8908e-08, 3.6601e-08, 2.9812e-10,\n",
            "        2.2868e-07, 7.2879e-08, 4.4506e-07, 2.9078e-07, 2.5787e-08, 1.5717e-07,\n",
            "        1.2625e-11, 1.1801e-08, 7.2324e-09, 1.5559e-08, 5.4277e-08, 2.0509e-07,\n",
            "        6.2271e-08, 6.4606e-08, 8.8205e-08, 1.2158e-07, 1.6167e-07, 2.1047e-08,\n",
            "        9.4589e-08, 9.0298e-09, 1.1914e-07, 2.6069e-08, 2.3132e-07, 7.9087e-08,\n",
            "        6.8030e-08, 2.4984e-08, 7.0017e-08, 6.7149e-08, 2.7444e-08, 1.3950e-07,\n",
            "        1.6123e-07, 6.9098e-08, 8.9987e-10, 4.7196e-09, 2.0650e-11, 1.0626e-07,\n",
            "        1.5980e-06, 4.0211e-10, 1.0753e-08, 2.2142e-08, 6.1831e-08, 4.7883e-08,\n",
            "        9.1619e-08, 5.6623e-08, 2.4387e-07, 6.9566e-08, 1.3683e-10, 5.6515e-08,\n",
            "        1.6674e-10, 4.5687e-08, 3.2047e-07, 2.8171e-08, 6.0726e-08, 9.5314e-08,\n",
            "        3.8525e-07, 5.5490e-08, 6.5778e-09, 3.1919e-08, 1.1786e-07, 1.7800e-07,\n",
            "        2.3407e-10, 2.9654e-08, 2.9488e-08, 5.8893e-08, 5.1607e-09, 2.5279e-07,\n",
            "        1.3704e-07, 2.9159e-08, 3.7125e-11, 5.1583e-08, 1.9177e-08, 3.2482e-10,\n",
            "        1.3603e-07, 9.5354e-09, 2.0978e-07, 1.2285e-07, 8.0857e-08, 5.0041e-08,\n",
            "        1.7583e-07, 4.1620e-09, 2.5744e-07, 5.7725e-09, 3.3246e-08, 6.9707e-08,\n",
            "        1.1779e-07, 1.4027e-08, 1.9415e-07, 1.5293e-08, 2.6932e-07, 1.2935e-07,\n",
            "        6.4973e-08, 1.8416e-08, 1.1559e-08, 1.0700e-09, 2.5255e-08, 1.6281e-07,\n",
            "        2.0430e-07, 7.6403e-09, 3.0116e-08, 1.4165e-08, 1.9913e-07, 9.1686e-09,\n",
            "        1.0927e-07, 2.9270e-08, 3.2072e-08, 3.2618e-08, 2.1262e-07, 2.9589e-08,\n",
            "        4.8600e-08, 1.8275e-07, 9.3685e-08, 9.8477e-08, 3.1479e-08, 6.0707e-08,\n",
            "        5.2377e-08, 2.4574e-07, 2.3118e-08, 4.7930e-08, 1.4365e-08, 1.7023e-07,\n",
            "        3.3200e-07, 1.1230e-08, 6.1078e-07, 5.8943e-10, 1.3742e-07, 9.3269e-08,\n",
            "        1.6055e-07, 6.1395e-08, 8.0505e-08, 2.9709e-08, 8.2219e-07, 6.5622e-08,\n",
            "        3.0350e-09, 4.1907e-08, 2.7905e-08, 5.1295e-08, 1.7858e-09, 1.5037e-07,\n",
            "        1.9326e-08, 1.2816e-07, 2.0205e-08, 1.7562e-08, 1.9485e-07, 1.8889e-08,\n",
            "        2.1495e-07, 4.1774e-08, 3.5247e-08, 6.7343e-08, 1.3746e-06, 5.0276e-07,\n",
            "        4.6306e-07, 5.3222e-08, 1.7089e-09, 2.3116e-07, 1.1134e-08, 1.7829e-08,\n",
            "        2.2675e-08, 7.4368e-11, 3.3542e-08, 7.4823e-11, 6.3250e-08, 8.7585e-08,\n",
            "        1.9267e-07, 2.0397e-07, 2.3958e-07, 1.0396e-07, 1.1510e-08, 6.5473e-08,\n",
            "        2.9880e-11, 5.4324e-08, 1.4739e-07, 3.2328e-07, 2.5007e-07, 6.2957e-08,\n",
            "        2.0148e-07, 3.0566e-08, 5.3547e-08, 3.2510e-07, 4.0880e-08, 7.6062e-08,\n",
            "        3.5155e-07, 3.7610e-08, 3.3426e-08, 5.7689e-07, 1.5060e-08, 4.1466e-07,\n",
            "        5.5733e-08, 1.1227e-08, 5.3386e-08, 1.8350e-08, 3.1928e-09, 7.1758e-08,\n",
            "        4.2023e-08, 4.3582e-07, 1.1549e-07, 1.5835e-07, 2.6550e-08, 2.7276e-07,\n",
            "        8.0789e-08, 7.1167e-08, 1.9499e-08, 3.3977e-08, 8.9755e-09, 1.8580e-12,\n",
            "        5.0001e-08, 1.7789e-07, 1.4110e-09, 4.3267e-09, 6.0004e-08, 2.6197e-07,\n",
            "        1.9928e-07, 8.2784e-08, 5.9523e-07, 1.0782e-10, 2.1592e-07, 5.5867e-08,\n",
            "        5.5199e-07, 7.9950e-08, 2.5633e-07, 1.5215e-07, 7.5045e-10, 1.5166e-08,\n",
            "        1.7069e-12, 2.0430e-09, 1.0030e-11, 4.2228e-10, 4.1680e-08, 6.1939e-08,\n",
            "        2.9077e-07, 1.2668e-07, 4.6688e-08, 1.1197e-08, 1.0173e-07, 6.3126e-08,\n",
            "        7.7439e-09, 1.2690e-07, 6.4834e-09, 6.2398e-07, 2.3591e-08, 2.2285e-08,\n",
            "        1.5887e-07, 7.6231e-09, 7.8562e-08, 1.9680e-09, 1.6256e-08, 6.7482e-08,\n",
            "        6.0639e-08, 3.5285e-09, 8.3722e-08, 1.9753e-08, 2.3392e-08, 3.6598e-07,\n",
            "        9.1490e-08, 3.2441e-07], requires_grad=True), 'fc2.weight': tensor([[4.4494e-08, 4.7762e-08, 5.3007e-07,  ..., 1.8116e-07, 1.6911e-08,\n",
            "         1.1601e-07],\n",
            "        [3.9721e-09, 2.7478e-08, 4.5777e-08,  ..., 2.8816e-08, 2.6107e-08,\n",
            "         7.7364e-09],\n",
            "        [1.9554e-10, 4.4156e-10, 4.3771e-10,  ..., 2.6405e-09, 2.3761e-09,\n",
            "         4.5408e-10],\n",
            "        ...,\n",
            "        [1.3594e-10, 1.4163e-09, 1.6294e-09,  ..., 1.0814e-09, 1.4425e-09,\n",
            "         4.0497e-10],\n",
            "        [2.9147e-08, 3.4330e-08, 3.6626e-07,  ..., 1.2908e-07, 1.6051e-08,\n",
            "         7.4272e-08],\n",
            "        [4.5122e-09, 2.1800e-08, 6.2539e-08,  ..., 5.9082e-08, 2.8017e-08,\n",
            "         1.1116e-08]], requires_grad=True), 'fc2.bias': tensor([2.9226e-07, 6.8017e-08, 7.1398e-09, 1.6826e-07, 2.5226e-08, 2.1626e-07,\n",
            "        3.8631e-10, 4.0758e-08, 5.8340e-11, 5.4963e-09, 8.5599e-08, 8.9847e-11,\n",
            "        3.0094e-07, 3.0075e-08, 2.1517e-07, 3.5491e-07, 3.3660e-09, 2.6152e-07,\n",
            "        7.5175e-08, 2.6463e-07, 2.9330e-08, 1.8142e-07, 7.5266e-08, 5.2569e-08,\n",
            "        2.1615e-11, 2.2953e-07, 1.2558e-10, 7.0226e-11, 8.9752e-07, 4.9745e-08,\n",
            "        1.2586e-07, 2.4001e-07, 5.9851e-08, 1.2065e-09, 1.4415e-10, 1.5967e-07,\n",
            "        1.2114e-08, 4.8131e-08, 1.5432e-08, 1.0115e-07, 1.4377e-09, 1.8300e-07,\n",
            "        1.0835e-07, 5.5313e-08, 5.0998e-08, 1.2650e-09, 3.0251e-08, 1.2268e-07,\n",
            "        3.1572e-07, 1.2758e-07, 6.7151e-08, 3.2159e-09, 3.4295e-08, 1.0562e-07,\n",
            "        2.5688e-08, 2.7130e-07, 1.2507e-08, 6.9273e-08, 1.4026e-07, 1.8746e-11,\n",
            "        1.8578e-07, 1.6114e-09, 9.5459e-08, 1.2302e-10, 9.6542e-08, 2.7867e-08,\n",
            "        5.0900e-09, 2.9304e-08, 1.2822e-07, 3.1244e-08, 3.0589e-07, 1.0790e-07,\n",
            "        2.7819e-10, 3.4594e-07, 2.1185e-07, 5.9948e-09, 1.2630e-07, 8.1562e-08,\n",
            "        9.3262e-09, 1.2122e-11, 6.1397e-08, 5.9227e-09, 2.9927e-08, 1.5006e-08,\n",
            "        1.9655e-07, 6.2888e-10, 7.7208e-08, 2.4610e-08, 3.0369e-09, 1.5447e-08,\n",
            "        2.1394e-09, 1.3285e-07, 2.3675e-08, 2.4221e-09, 6.3096e-12, 2.2164e-08,\n",
            "        4.8814e-07, 6.6184e-09, 2.5501e-10, 3.4030e-08, 4.3653e-08, 5.1842e-09,\n",
            "        1.4709e-07, 1.2057e-08, 2.2740e-07, 9.4677e-08, 1.0807e-07, 2.5459e-07,\n",
            "        3.2126e-09, 4.1564e-08, 4.2254e-10, 2.9800e-09, 2.1977e-07, 2.5359e-07,\n",
            "        1.1872e-07, 9.1051e-08, 1.4942e-07, 3.9277e-08, 1.1038e-07, 1.7317e-08,\n",
            "        3.4299e-09, 1.4087e-06, 1.4347e-07, 8.3275e-07, 3.3367e-07, 1.5003e-07,\n",
            "        4.5000e-09, 2.7491e-11, 8.6218e-09, 3.4119e-08, 5.3716e-09, 8.9566e-08,\n",
            "        2.6339e-07, 4.7285e-08, 1.3758e-08, 8.0419e-09, 2.4388e-07, 6.2590e-08,\n",
            "        6.3115e-08, 1.3078e-07, 1.8420e-07, 2.6383e-10, 2.6988e-08, 1.8570e-09,\n",
            "        1.3918e-07, 3.1383e-09, 5.8485e-08, 1.8911e-07, 1.5654e-08, 2.4200e-08,\n",
            "        1.4405e-08, 3.7521e-07, 5.7482e-10, 9.5096e-09, 6.8822e-08, 6.8834e-07,\n",
            "        1.2162e-07, 4.4253e-07, 1.4479e-09, 8.4155e-10, 9.6497e-08, 1.1116e-07,\n",
            "        1.9715e-08, 6.5541e-07, 1.9458e-10, 1.0233e-07, 3.0034e-10, 6.4111e-09,\n",
            "        1.5153e-09, 1.8259e-08, 1.1024e-07, 5.8269e-08, 2.0574e-09, 1.7294e-08,\n",
            "        2.9738e-08, 1.2073e-08, 1.8209e-09, 3.7358e-09, 8.4493e-08, 2.7361e-12,\n",
            "        8.5372e-07, 2.1681e-07, 4.3961e-08, 1.0268e-06, 5.7390e-08, 1.2862e-09,\n",
            "        3.0385e-07, 6.1475e-09, 2.6243e-10, 1.2940e-09, 4.3663e-08, 9.3804e-08,\n",
            "        7.4350e-08, 4.0463e-07, 4.2536e-08, 9.9310e-08, 1.4962e-07, 4.0766e-08,\n",
            "        1.2144e-07, 3.9191e-07, 3.8768e-07, 6.9035e-08, 1.7572e-08, 1.2410e-07,\n",
            "        1.7028e-10, 3.5648e-08, 2.4635e-08, 1.3580e-07, 4.7262e-08, 5.2375e-07,\n",
            "        4.2773e-08, 4.4605e-09, 6.8079e-09, 1.9100e-07, 1.6336e-08, 2.3753e-07,\n",
            "        2.3953e-08, 3.6187e-08, 1.5237e-08, 2.5170e-08, 1.0201e-07, 3.9194e-09,\n",
            "        7.0639e-08, 1.5067e-08, 3.7524e-07, 3.1865e-08, 1.9992e-10, 2.8503e-10,\n",
            "        1.1689e-09, 2.0647e-08, 9.6958e-09, 1.4679e-07, 4.0681e-07, 4.3437e-09,\n",
            "        8.2691e-07, 3.6117e-07, 8.2285e-08, 1.9143e-08, 1.5829e-07, 2.9269e-08,\n",
            "        1.9816e-08, 4.4982e-08, 1.8358e-07, 4.1717e-07, 1.7148e-08, 1.1187e-07,\n",
            "        1.9575e-07, 4.3991e-09, 1.1028e-07, 1.5110e-07, 3.7213e-07, 5.5256e-07,\n",
            "        2.2588e-10, 4.5136e-08, 4.2837e-08, 3.0656e-08, 3.4475e-10, 2.8670e-08,\n",
            "        2.0171e-08, 2.4528e-08, 9.2945e-08, 2.2361e-07, 2.1731e-08, 6.0598e-08,\n",
            "        2.3834e-08, 5.2686e-10, 1.3103e-07, 8.8992e-08, 1.1113e-07, 7.3743e-10,\n",
            "        2.6492e-07, 2.0654e-07, 5.4477e-09, 4.7351e-09, 4.4205e-09, 2.5317e-10,\n",
            "        3.3164e-11, 7.2973e-08, 2.3318e-07, 1.1253e-07, 6.4149e-08, 2.9516e-08,\n",
            "        5.0406e-08, 1.3362e-08, 6.2038e-08, 9.8229e-10, 1.2616e-07, 1.2717e-08,\n",
            "        6.3839e-08, 3.6544e-09, 3.0019e-09, 3.9675e-07, 3.0877e-09, 5.8061e-08,\n",
            "        3.3891e-08, 7.8844e-08, 6.8076e-08, 6.8142e-11, 2.9320e-07, 4.3114e-09,\n",
            "        1.6720e-08, 1.3884e-10, 7.1544e-07, 1.9336e-09, 2.1710e-09, 2.1914e-08,\n",
            "        7.9941e-08, 1.6389e-07, 1.8203e-07, 7.2106e-08, 1.5067e-07, 8.9451e-11,\n",
            "        6.2259e-09, 9.1456e-10, 4.7182e-08, 2.0694e-08, 7.1310e-08, 1.2859e-10,\n",
            "        3.5186e-07, 6.2161e-08, 1.4519e-08, 7.7142e-08, 1.3054e-07, 2.1230e-07,\n",
            "        7.4835e-08, 1.3415e-08, 1.7537e-07, 3.4013e-08, 2.9659e-07, 1.2064e-07,\n",
            "        6.7029e-09, 1.3335e-10, 2.6929e-08, 2.0700e-08, 1.1488e-07, 2.6488e-10,\n",
            "        2.0540e-08, 1.1827e-07, 1.0513e-07, 1.8894e-07, 6.2637e-08, 6.3170e-08,\n",
            "        1.7319e-09, 3.1839e-08, 3.3188e-07, 4.3599e-07, 8.5122e-08, 2.2457e-07,\n",
            "        1.3244e-08, 1.5641e-07, 3.3846e-08, 6.7668e-08, 2.2953e-08, 8.8974e-07,\n",
            "        2.3699e-07, 9.9183e-08, 2.0722e-07, 2.4725e-07, 7.3896e-09, 2.4431e-08,\n",
            "        2.1001e-07, 1.0198e-07, 2.2272e-07, 3.0846e-08, 5.3037e-10, 3.1358e-08,\n",
            "        1.0630e-08, 1.0344e-07, 9.3014e-09, 2.8359e-08, 8.7075e-08, 4.8094e-08,\n",
            "        8.2732e-08, 1.7170e-08, 7.1507e-08, 1.0524e-07, 5.0624e-08, 1.6340e-07,\n",
            "        4.8990e-08, 1.6412e-10, 1.1913e-08, 4.7270e-08, 5.1421e-09, 8.7602e-08,\n",
            "        1.8232e-08, 2.3721e-09, 1.4078e-07, 4.5185e-08, 5.8630e-08, 1.8265e-09,\n",
            "        1.6237e-08, 6.9500e-08, 8.1092e-08, 1.6548e-08, 6.1643e-10, 4.3261e-08,\n",
            "        3.6539e-11, 2.8174e-07, 4.9095e-08, 4.6350e-07, 2.9033e-08, 1.1880e-07,\n",
            "        2.0844e-08, 1.4383e-07, 7.7835e-09, 5.3723e-08, 1.8184e-07, 3.2907e-09,\n",
            "        8.9696e-08, 5.1558e-08, 3.0987e-07, 1.5457e-07, 1.6689e-07, 4.3304e-08,\n",
            "        4.4804e-07, 4.2498e-08, 5.3562e-08, 5.8459e-07, 4.1475e-07, 1.8504e-07,\n",
            "        2.8570e-08, 1.5433e-07, 4.7194e-07, 4.9151e-09, 4.2963e-08, 3.5126e-07,\n",
            "        2.7576e-08, 5.5720e-07, 1.0112e-07, 5.9748e-10, 4.4393e-11, 1.3100e-08,\n",
            "        6.3752e-07, 8.9244e-08, 1.1713e-08, 6.7131e-09, 1.5547e-07, 9.7919e-10,\n",
            "        9.7436e-08, 1.6558e-08, 2.9672e-07, 5.7499e-08, 6.6184e-08, 1.2222e-07,\n",
            "        9.7468e-08, 7.2639e-10, 1.6649e-07, 2.0142e-07, 1.9853e-11, 1.1690e-07,\n",
            "        6.8914e-09, 1.6330e-10, 1.0486e-07, 1.8521e-07, 3.3569e-08, 1.3989e-07,\n",
            "        3.7609e-07, 5.9580e-10, 3.7181e-08, 3.3320e-08, 5.2334e-08, 2.7784e-07,\n",
            "        6.0556e-08, 1.8557e-07, 3.6414e-08, 1.0699e-10, 8.2199e-08, 1.8366e-10,\n",
            "        4.2105e-09, 1.4942e-07, 2.2349e-07, 2.8237e-07, 3.1830e-09, 5.7610e-07,\n",
            "        4.4186e-07, 3.0342e-07, 3.0136e-08, 1.7716e-10, 2.0911e-07, 2.4010e-10,\n",
            "        7.9769e-08, 7.1805e-08, 4.3476e-08, 4.2685e-11, 3.1281e-10, 2.2402e-08,\n",
            "        5.2761e-08, 1.6330e-07, 3.2512e-09, 1.1938e-08, 8.2376e-08, 2.3323e-09,\n",
            "        2.2609e-07, 1.3389e-07, 1.3549e-09, 2.0088e-07, 9.8969e-08, 5.7738e-08,\n",
            "        4.4518e-08, 7.6489e-08, 1.6145e-08, 1.3119e-07, 5.9981e-08, 9.8936e-10,\n",
            "        3.3869e-07, 3.1355e-08, 3.7284e-09, 1.4597e-07, 1.6575e-08, 3.0018e-09,\n",
            "        1.8774e-07, 7.7632e-08], requires_grad=True), 'classifier.weight': tensor([[1.1660e-07, 2.0409e-07, 7.4358e-08,  ..., 1.3616e-08, 4.3937e-09,\n",
            "         1.3197e-06],\n",
            "        [1.3058e-08, 2.3878e-07, 1.8519e-08,  ..., 7.4360e-10, 1.1538e-09,\n",
            "         4.2283e-08],\n",
            "        [6.1116e-07, 3.0937e-07, 5.2517e-08,  ..., 1.5603e-08, 1.4520e-06,\n",
            "         7.2996e-06],\n",
            "        ...,\n",
            "        [1.7061e-12, 1.8234e-12, 3.0425e-13,  ..., 6.7776e-14, 1.2454e-13,\n",
            "         7.2961e-12],\n",
            "        [1.9353e-12, 1.8690e-12, 3.2929e-13,  ..., 7.2550e-14, 1.3130e-13,\n",
            "         8.0194e-12],\n",
            "        [1.7887e-12, 1.8496e-12, 3.1337e-13,  ..., 7.2884e-14, 1.3263e-13,\n",
            "         8.1440e-12]], requires_grad=True), 'classifier.bias': tensor([2.4681e-06, 5.9041e-07, 1.1850e-05, 4.9292e-06, 1.6000e-05, 6.1527e-08,\n",
            "        4.0070e-11, 3.8663e-11, 4.2151e-11, 4.1683e-11], requires_grad=True)}\n"
          ]
        }
      ],
      "source": [
        "print(fisher_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "k9cHcxAN3s3Q"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, fisher_matrix, prev_params):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "\n",
        "        # Original loss\n",
        "        ce_loss = loss_fn(pred, y)\n",
        "\n",
        "        # EWC loss\n",
        "        ewc_loss = get_ewc_loss(model, fisher_matrix, prev_params)\n",
        "\n",
        "        loss = ce_loss + ewc_lambda * ewc_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch+1)*len(X)\n",
        "            print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "YOE0I97T8HAH"
      },
      "outputs": [],
      "source": [
        "def val(epoch):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(test_dataloader_task1):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            print(torch.unique(predicted_old))\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv-BzHy78l06",
        "outputId": "398e1b76-c664-4011-86ca-f0e94bf5f436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: ----------------------\n",
            "Loss: 13.574834,    64/24000\n",
            "Loss: 3.256116,  6464/24000\n",
            "Loss: 2.654275, 12864/24000\n",
            "Loss: 2.459614, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 32.8, Avg Loss: 2.174725\n",
            "\n",
            "Epoch 2: ----------------------\n",
            "Loss: 2.337003,    64/24000\n",
            "Loss: 2.032220,  6464/24000\n",
            "Loss: 1.761280, 12864/24000\n",
            "Loss: 1.554754, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 44.5, Avg Loss: 1.721210\n",
            "\n",
            "Epoch 3: ----------------------\n",
            "Loss: 1.518960,    64/24000\n",
            "Loss: 1.375294,  6464/24000\n",
            "Loss: 1.237617, 12864/24000\n",
            "Loss: 1.081668, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 65.1, Avg Loss: 1.673280\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    print(f\"Epoch {epoch+1}: ----------------------\")\n",
        "    train(train_dataloader_task2, model, loss_fn, optimizer, fisher_matrix, prev_params)\n",
        "    test(test_dataloader_task2, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_oObVs6s35e",
        "outputId": "6e6ac3a0-977e-4524-d4be-0d30d6ec3830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 4, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 4, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "tensor([0, 1, 2, 3, 6, 7, 8])\n",
            "Validation Acc: 27.54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "val(1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}