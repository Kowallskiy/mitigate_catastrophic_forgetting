{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "b2NSA-U_0DhS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask1(datasets.FashionMNIST):\n",
        "  def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "    super(FashionMNISTtask1, self).__init__(root, train=train, transform=transform,\n",
        "                                            target_transform=target_transform,download=download)\n",
        "    self.classes = self.classes[:6]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img, target = super(FashionMNISTtask1, self).__getitem__(index)\n",
        "    if target < 6:\n",
        "        return img, target\n",
        "    else:\n",
        "        return img, -1"
      ],
      "metadata": {
        "id": "qSfxWcOn18kp"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask2(datasets.FashionMNIST):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(FashionMNISTtask2, self).__init__(root, train=train, transform=transform,\n",
        "                                                target_transform=target_transform,download=download)\n",
        "        self.classes = self.classes[6:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super(FashionMNISTtask2, self).__getitem__(index)\n",
        "        if target >= 6:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img, -1"
      ],
      "metadata": {
        "id": "Nl-_I6Ry2BMR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([ToTensor(), Normalize((0.5), (0.5))])\n",
        "\n",
        "train_dataset_1 = FashionMNISTtask1(root='./data1', train=True, transform=transform, download=True)\n",
        "test_dataset_1 = FashionMNISTtask1(root='./data1', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "JvtnT5b32GBQ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_2 = FashionMNISTtask2(root='./data2', train=True, transform=transform, download=True)\n",
        "test_dataset_2 = FashionMNISTtask2(root='./data2', train=False, transform=transform, download=True)\n",
        "test_dataset_3 = datasets.FashionMNIST(\n",
        "    root='data3',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "yr4F5agU2Gio"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_filtered_first = [data for data in train_dataset_1 if data[1] != -1]\n",
        "test_dataset_filtered_first = [data for data in test_dataset_1 if data[1] != -1]\n",
        "\n",
        "train_dataset_filtered_second = [data for data in train_dataset_2 if data[1] != -1]\n",
        "test_dataset_filtered_second = [data for data in test_dataset_2 if data[1] != -1]"
      ],
      "metadata": {
        "id": "N8HwMD0z2IZc"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels_1 = set()\n",
        "unique_labels_2 = set()\n",
        "\n",
        "for _, target in train_dataset_filtered_first:\n",
        "    unique_labels_1.add(target)\n",
        "\n",
        "for _, target in train_dataset_filtered_second:\n",
        "    unique_labels_2.add(target)\n",
        "\n",
        "print(f\"First: {unique_labels_1}\")\n",
        "print(f\"Second: {unique_labels_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUz-SUDKsZuX",
        "outputId": "8d459bbd-b50e-4c07-8a18-15bc1760a050"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First: {0, 1, 2, 3, 4, 5}\n",
            "Second: {8, 9, 6, 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_first = DataLoader(train_dataset_filtered_first, batch_size=64, shuffle=True)\n",
        "test_dataloader_first = DataLoader(test_dataset_filtered_first, batch_size=256, shuffle=False)\n",
        "\n",
        "for X, y in train_dataloader_first:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"SHape of y: {y.shape}, dtype: {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_J6o_FR2Mh8",
        "outputId": "f13d356b-a24a-4096-9a2b-05cbe979c9aa"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "SHape of y: torch.Size([64]), dtype: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_second = DataLoader(train_dataset_filtered_second, batch_size=64, shuffle=True)\n",
        "test_dataloader_second = DataLoader(test_dataset_filtered_second, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "OFGpnxZq2NYt"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataloader = DataLoader(test_dataset_3, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "dpjhj4M82QGR"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7raJ2RC2Toq",
        "outputId": "2e584637-3b63-425d-e492-9663d6954290"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    #     self._initialize_weights()\n",
        "\n",
        "    # def _initialize_weights(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Linear):\n",
        "    #             nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "    #         elif isinstance(m, nn.Conv2d):\n",
        "    #             nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "C5nQoXPk2Wbo"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ],
      "metadata": {
        "id": "tyhb4Zo4230I"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\\n\")"
      ],
      "metadata": {
        "id": "hZThNdfT26Kg"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(num_classes=10, hidden_size=512).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "O8cy0wv-28X4"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------------\")\n",
        "  train(train_dataloader_first, model, loss_fn, optimizer)\n",
        "  test(test_dataloader_first, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "torch.save(model.state_dict(), \"model_old.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO9q7xFU4UUI",
        "outputId": "232086bf-875f-4a5a-9221-e73f82305346"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Loss: 2.298927,    64/36000\n",
            "Loss: 0.396251,  6464/36000\n",
            "Loss: 0.468095, 12864/36000\n",
            "Loss: 0.442391, 19264/36000\n",
            "Loss: 0.337982, 25664/36000\n",
            "Loss: 0.302926, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.2, Avg Loss: 0.288838\n",
            "\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Loss: 0.237493,    64/36000\n",
            "Loss: 0.282463,  6464/36000\n",
            "Loss: 0.448286, 12864/36000\n",
            "Loss: 0.237903, 19264/36000\n",
            "Loss: 0.211039, 25664/36000\n",
            "Loss: 0.153253, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.5, Avg Loss: 0.282752\n",
            "\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Loss: 0.253466,    64/36000\n",
            "Loss: 0.154371,  6464/36000\n",
            "Loss: 0.113784, 12864/36000\n",
            "Loss: 0.242314, 19264/36000\n",
            "Loss: 0.179281, 25664/36000\n",
            "Loss: 0.199503, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.4, Avg Loss: 0.262568\n",
            "\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Loss: 0.184641,    64/36000\n",
            "Loss: 0.401508,  6464/36000\n",
            "Loss: 0.286990, 12864/36000\n",
            "Loss: 0.275291, 19264/36000\n",
            "Loss: 0.305218, 25664/36000\n",
            "Loss: 0.141361, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 91.1, Avg Loss: 0.251770\n",
            "\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Loss: 0.232474,    64/36000\n",
            "Loss: 0.215906,  6464/36000\n",
            "Loss: 0.185564, 12864/36000\n",
            "Loss: 0.195200, 19264/36000\n",
            "Loss: 0.201853, 25664/36000\n",
            "Loss: 0.138440, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.7, Avg Loss: 0.264064\n",
            "\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Loss: 0.342787,    64/36000\n",
            "Loss: 0.231555,  6464/36000\n",
            "Loss: 0.084295, 12864/36000\n",
            "Loss: 0.187570, 19264/36000\n",
            "Loss: 0.372791, 25664/36000\n",
            "Loss: 0.159252, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.7, Avg Loss: 0.288960\n",
            "\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Loss: 0.325739,    64/36000\n",
            "Loss: 0.373738,  6464/36000\n",
            "Loss: 0.267622, 12864/36000\n",
            "Loss: 0.191345, 19264/36000\n",
            "Loss: 0.083131, 25664/36000\n",
            "Loss: 0.199329, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.5, Avg Loss: 0.262594\n",
            "\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Loss: 0.154647,    64/36000\n",
            "Loss: 0.254725,  6464/36000\n",
            "Loss: 0.162051, 12864/36000\n",
            "Loss: 0.209335, 19264/36000\n",
            "Loss: 0.180097, 25664/36000\n",
            "Loss: 0.181868, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.2, Avg Loss: 0.270327\n",
            "\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Loss: 0.077055,    64/36000\n",
            "Loss: 0.254104,  6464/36000\n",
            "Loss: 0.197548, 12864/36000\n",
            "Loss: 0.165099, 19264/36000\n",
            "Loss: 0.223897, 25664/36000\n",
            "Loss: 0.048280, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.6, Avg Loss: 0.256107\n",
            "\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Loss: 0.081217,    64/36000\n",
            "Loss: 0.158014,  6464/36000\n",
            "Loss: 0.376560, 12864/36000\n",
            "Loss: 0.327156, 19264/36000\n",
            "Loss: 0.145564, 25664/36000\n",
            "Loss: 0.123579, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.1, Avg Loss: 0.262313\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val(epoch):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(eval_dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ],
      "metadata": {
        "id": "RZmhDHNfIypc"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_50xg7ygJGMA",
        "outputId": "801ee5a9-be34-418f-99dc-c9653a3aad79"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 54.07\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "________"
      ],
      "metadata": {
        "id": "U2xq-SiG4Uvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = NeuralNetwork()\n",
        "# model.load_state_dict(torch.load(\"model_old.pth\"))"
      ],
      "metadata": {
        "id": "GPZXh0Ga4rZy"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def get_fisher_diag(model, dataloader, params, empirical=False):\n",
        "    fisher = {}\n",
        "    params_dict = dict(params)\n",
        "    for n, p in deepcopy(params_dict).items():\n",
        "        p.data.zero_()\n",
        "        fisher[n] = p.data.clone().detach().requires_grad_()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for input, gt_label in dataloader:\n",
        "        input, gt_label = input.to(device), gt_label.to(device)\n",
        "        model.zero_grad()\n",
        "        output = model(input)\n",
        "\n",
        "        if empirical:\n",
        "            label = output.max(1)[1]\n",
        "        else:\n",
        "            label = gt_label\n",
        "\n",
        "        # label = gt_label.repeat(output.size(0))\n",
        "        negloglikelihood = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(output, dim=1), label)\n",
        "        negloglikelihood.backward()\n",
        "\n",
        "        for n, p in model.named_parameters():\n",
        "            fisher[n].data += p.grad.data ** 2 / len(dataloader.dataset)\n",
        "\n",
        "    fisher = {n: p for n, p in fisher.items()}\n",
        "    return fisher\n",
        "\n",
        "\n",
        "def get_ewc_loss(model, fisher, p_old):\n",
        "    loss = 0\n",
        "    for n, p in model.named_parameters():\n",
        "        _loss = fisher[n] * (p - p_old[n]) ** 2\n",
        "        loss += _loss.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LhRRttbd3Bxo"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "ewc_lambda = 500_000\n",
        "\n",
        "fisher_matrix = get_fisher_diag(model, train_dataloader_first, model.named_parameters())\n",
        "prev_params = {n: p.data.clone() for n, p in model.named_parameters()}\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "RLlF7JXB57n4"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fisher_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEtwEedKrqPc",
        "outputId": "4a66b36b-456f-4461-ce76-dab6319e17a1"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1.weight': tensor([[3.7443e-09, 3.7439e-09, 3.7436e-09,  ..., 2.5215e-09, 3.4699e-09,\n",
            "         3.7169e-09],\n",
            "        [1.6326e-11, 1.6238e-11, 1.6049e-11,  ..., 1.4991e-11, 1.6045e-11,\n",
            "         1.6240e-11],\n",
            "        [2.9330e-08, 2.9330e-08, 2.9311e-08,  ..., 2.8306e-08, 2.9205e-08,\n",
            "         2.9335e-08],\n",
            "        ...,\n",
            "        [6.1033e-08, 6.1013e-08, 6.0929e-08,  ..., 5.7351e-08, 5.9659e-08,\n",
            "         6.0806e-08],\n",
            "        [7.6741e-13, 7.6732e-13, 7.6720e-13,  ..., 7.3480e-13, 7.3585e-13,\n",
            "         7.6046e-13],\n",
            "        [1.2944e-09, 1.2944e-09, 1.2947e-09,  ..., 1.2968e-09, 1.2910e-09,\n",
            "         1.2945e-09]], device='cuda:0', requires_grad=True), 'fc1.bias': tensor([3.7443e-09, 1.6326e-11, 2.9330e-08, 5.4086e-08, 1.8312e-10, 1.0067e-09,\n",
            "        1.3593e-08, 7.5410e-13, 8.1404e-08, 5.7398e-08, 4.0001e-09, 2.2646e-12,\n",
            "        3.4448e-09, 2.9741e-12, 4.5721e-17, 2.7898e-08, 4.6300e-13, 1.0759e-08,\n",
            "        5.7502e-08, 2.4081e-08, 7.3653e-09, 7.6308e-08, 3.3324e-11, 2.4336e-10,\n",
            "        2.0743e-14, 9.3023e-12, 2.6487e-08, 1.4499e-09, 3.7490e-08, 1.5456e-09,\n",
            "        7.5990e-13, 1.4018e-11, 2.0670e-08, 3.1961e-10, 7.6238e-09, 1.6731e-10,\n",
            "        8.6507e-07, 2.9200e-10, 1.6245e-11, 3.7254e-11, 1.4440e-10, 4.9544e-12,\n",
            "        1.2549e-07, 4.9932e-12, 4.2092e-07, 7.7754e-13, 1.2650e-10, 1.5308e-11,\n",
            "        8.9589e-11, 1.0183e-10, 1.1067e-09, 2.7801e-12, 1.6815e-07, 3.2416e-07,\n",
            "        2.6377e-12, 5.0984e-11, 7.7955e-12, 1.3675e-16, 1.4345e-08, 7.6048e-12,\n",
            "        9.2515e-08, 1.6238e-11, 2.9313e-07, 1.6524e-08, 2.9850e-13, 1.4460e-10,\n",
            "        4.6978e-13, 9.3287e-17, 3.0042e-11, 1.1169e-09, 3.4212e-08, 5.3203e-07,\n",
            "        6.2322e-11, 4.0345e-12, 9.3374e-09, 2.4032e-08, 1.1242e-08, 8.2591e-13,\n",
            "        1.1117e-12, 6.1889e-11, 9.4929e-15, 5.0401e-08, 5.4645e-12, 8.0096e-08,\n",
            "        1.8471e-13, 2.6240e-10, 2.2638e-13, 1.2112e-11, 7.0899e-09, 1.9270e-09,\n",
            "        6.4284e-15, 2.1008e-17, 4.2642e-09, 1.3953e-09, 2.1399e-09, 6.5645e-10,\n",
            "        3.6411e-10, 2.2308e-12, 1.1063e-08, 1.5628e-10, 1.2577e-15, 1.0431e-07,\n",
            "        2.6665e-12, 2.4466e-07, 1.1773e-07, 2.4498e-07, 8.0177e-11, 4.5091e-09,\n",
            "        1.6627e-07, 7.2914e-09, 1.0366e-21, 1.2793e-10, 2.3254e-09, 1.0761e-07,\n",
            "        4.6514e-09, 1.4855e-19, 3.7071e-10, 6.0831e-12, 1.5752e-11, 7.6753e-12,\n",
            "        0.0000e+00, 3.2615e-10, 7.2306e-14, 1.1218e-11, 5.1172e-09, 1.7691e-11,\n",
            "        7.6596e-11, 6.4407e-17, 4.5148e-14, 9.4684e-08, 0.0000e+00, 7.9231e-12,\n",
            "        1.9904e-12, 1.9971e-10, 2.3799e-11, 3.9764e-09, 4.3915e-08, 1.3624e-07,\n",
            "        5.5647e-13, 6.7331e-10, 1.2273e-11, 8.4775e-09, 8.8093e-10, 3.1761e-09,\n",
            "        7.6713e-08, 4.3722e-10, 6.5108e-11, 1.1852e-09, 8.9746e-09, 1.9868e-14,\n",
            "        2.3106e-08, 4.5166e-08, 3.8981e-10, 1.8830e-07, 1.1311e-08, 1.7243e-14,\n",
            "        1.1813e-13, 7.4679e-11, 1.9084e-07, 1.3560e-08, 8.6096e-10, 1.1389e-12,\n",
            "        7.8839e-09, 1.1360e-10, 3.1698e-11, 1.2702e-12, 3.6536e-18, 4.4223e-12,\n",
            "        1.7130e-08, 2.9294e-11, 2.9525e-13, 3.2799e-08, 1.3211e-12, 3.5881e-12,\n",
            "        6.6864e-07, 5.3399e-08, 1.8896e-12, 3.0884e-09, 3.9899e-11, 1.8348e-08,\n",
            "        7.0542e-09, 2.2642e-08, 4.7490e-10, 5.6615e-11, 8.0366e-09, 1.0682e-10,\n",
            "        2.0982e-08, 1.9349e-08, 3.4594e-11, 4.9027e-12, 5.6819e-09, 1.6615e-09,\n",
            "        2.3155e-09, 5.2140e-12, 6.6010e-12, 1.0176e-10, 7.1471e-09, 9.1642e-11,\n",
            "        1.1518e-09, 6.2255e-10, 1.2403e-07, 9.0372e-10, 3.8100e-07, 1.7744e-10,\n",
            "        1.2803e-08, 3.9859e-07, 3.3619e-10, 3.3393e-11, 1.1987e-09, 7.4681e-09,\n",
            "        1.3339e-09, 5.8371e-12, 2.6618e-16, 5.7307e-08, 1.0155e-17, 1.6662e-07,\n",
            "        6.3050e-11, 4.8490e-10, 1.3645e-14, 1.3678e-10, 2.3986e-11, 1.1744e-07,\n",
            "        1.7836e-10, 1.2999e-11, 7.3149e-12, 4.5729e-09, 3.7885e-10, 1.9897e-08,\n",
            "        2.2871e-10, 2.8028e-13, 2.7267e-09, 7.0512e-12, 8.5451e-09, 4.3484e-09,\n",
            "        1.4918e-10, 2.2960e-08, 3.3379e-11, 4.7373e-10, 2.4493e-08, 5.5233e-08,\n",
            "        1.3356e-11, 6.0303e-17, 4.2499e-10, 1.1048e-10, 1.5633e-13, 3.0792e-16,\n",
            "        1.5702e-15, 2.6076e-08, 5.4083e-10, 1.3862e-08, 2.2858e-08, 4.7115e-12,\n",
            "        2.0048e-11, 1.3941e-08, 2.0204e-10, 1.2424e-11, 7.2805e-12, 1.6499e-11,\n",
            "        1.2538e-09, 5.0825e-09, 2.9788e-10, 2.3706e-08, 5.0847e-08, 1.5288e-13,\n",
            "        6.4002e-11, 1.4316e-09, 4.3394e-13, 5.3201e-10, 2.5078e-12, 1.9225e-09,\n",
            "        4.1000e-09, 1.3911e-10, 1.2860e-09, 3.6832e-10, 9.2192e-14, 5.1212e-12,\n",
            "        3.7617e-16, 1.0848e-16, 2.5971e-10, 7.6832e-13, 4.3202e-08, 2.8652e-14,\n",
            "        7.2484e-11, 6.9051e-09, 1.0685e-13, 1.6491e-11, 2.9286e-09, 4.0262e-09,\n",
            "        2.5486e-10, 9.0729e-08, 1.3171e-10, 4.0587e-09, 9.4424e-09, 8.1766e-14,\n",
            "        3.0660e-10, 1.4118e-11, 5.5784e-12, 2.0201e-10, 1.0798e-12, 5.1355e-11,\n",
            "        2.0892e-09, 4.6804e-08, 9.1575e-10, 1.3250e-10, 5.3857e-10, 3.0611e-12,\n",
            "        2.3087e-07, 1.7749e-12, 1.2368e-07, 2.2505e-14, 4.5234e-11, 8.1510e-10,\n",
            "        8.5250e-10, 3.5653e-09, 8.7269e-10, 2.8390e-10, 5.1777e-10, 3.3788e-10,\n",
            "        9.4033e-13, 3.5026e-07, 1.0051e-09, 1.3996e-08, 2.6899e-07, 1.9304e-13,\n",
            "        1.3873e-08, 1.0988e-08, 2.5492e-07, 8.1489e-12, 3.1718e-11, 2.2653e-07,\n",
            "        3.1078e-08, 1.8496e-07, 9.8270e-09, 1.6257e-07, 4.2598e-13, 0.0000e+00,\n",
            "        0.0000e+00, 6.3619e-12, 5.1872e-14, 9.6585e-11, 1.9281e-11, 2.3074e-11,\n",
            "        7.0945e-10, 4.8711e-08, 7.7319e-10, 8.7653e-08, 2.0235e-11, 2.8051e-12,\n",
            "        5.4618e-12, 5.6315e-09, 5.7154e-08, 2.6585e-10, 1.6001e-15, 2.2241e-09,\n",
            "        1.8479e-12, 6.0521e-12, 4.0525e-09, 1.0587e-07, 7.8697e-09, 1.0757e-10,\n",
            "        3.2358e-16, 3.4611e-08, 4.5178e-08, 0.0000e+00, 1.8133e-11, 7.0457e-09,\n",
            "        3.2070e-07, 3.9768e-08, 9.2345e-07, 3.0315e-09, 2.2966e-13, 6.4951e-08,\n",
            "        1.8019e-08, 5.2066e-10, 6.3766e-09, 3.2625e-10, 1.5287e-08, 3.9174e-09,\n",
            "        4.4094e-09, 1.1240e-13, 3.3522e-15, 2.2169e-10, 3.2016e-07, 1.0300e-09,\n",
            "        1.6735e-08, 6.6167e-09, 1.7474e-11, 1.1488e-09, 2.3766e-16, 3.3886e-08,\n",
            "        2.8607e-16, 2.9769e-08, 7.8544e-12, 1.3962e-10, 1.3935e-12, 6.8114e-08,\n",
            "        3.1079e-18, 2.6239e-12, 2.3072e-10, 8.6124e-10, 6.0480e-12, 1.0605e-13,\n",
            "        4.1128e-08, 9.4128e-11, 5.9638e-14, 0.0000e+00, 4.8292e-08, 4.9123e-08,\n",
            "        1.1272e-08, 6.8295e-10, 8.4947e-13, 7.7367e-16, 3.6192e-14, 1.5852e-07,\n",
            "        7.5911e-08, 8.4418e-12, 1.0058e-09, 8.0009e-08, 5.9104e-08, 1.4160e-07,\n",
            "        1.1835e-12, 1.2304e-08, 8.6946e-09, 7.8242e-09, 3.0848e-13, 1.1757e-07,\n",
            "        3.9407e-08, 4.5319e-09, 2.1568e-09, 1.6035e-07, 1.9750e-20, 5.6476e-08,\n",
            "        1.5861e-14, 9.5563e-11, 4.3511e-07, 1.7513e-08, 3.9327e-09, 2.0680e-11,\n",
            "        1.5527e-08, 7.6964e-13, 1.3793e-09, 7.1730e-18, 5.9531e-11, 5.6901e-11,\n",
            "        4.1361e-16, 6.1524e-11, 3.0783e-26, 5.7485e-08, 2.0991e-09, 1.9624e-08,\n",
            "        3.2282e-11, 1.4939e-07, 4.6217e-11, 4.1060e-09, 0.0000e+00, 5.6741e-10,\n",
            "        5.5218e-09, 1.5670e-11, 4.9241e-08, 4.5993e-12, 3.6505e-10, 1.0642e-07,\n",
            "        1.0810e-08, 1.3983e-13, 1.3476e-10, 7.4710e-10, 3.3850e-07, 1.8757e-08,\n",
            "        1.0748e-18, 6.2535e-10, 6.0213e-14, 9.1160e-12, 9.4470e-10, 1.8677e-11,\n",
            "        4.7032e-11, 3.5255e-10, 1.6469e-11, 1.5614e-12, 3.4447e-07, 1.1495e-09,\n",
            "        4.4249e-09, 2.6709e-15, 5.6241e-08, 1.8328e-09, 2.2086e-11, 6.9411e-11,\n",
            "        5.5425e-11, 1.5097e-09, 6.6006e-12, 6.8268e-09, 1.1531e-07, 1.1371e-09,\n",
            "        9.4038e-08, 1.2369e-11, 3.9795e-09, 1.2568e-12, 4.4806e-11, 5.9377e-08,\n",
            "        9.1409e-08, 7.4187e-10, 1.8647e-17, 1.0020e-10, 1.1370e-10, 3.8214e-10,\n",
            "        1.1299e-12, 4.1291e-07, 9.5342e-08, 3.4649e-08, 8.4813e-08, 6.1033e-08,\n",
            "        7.6741e-13, 1.2950e-09], device='cuda:0', requires_grad=True), 'fc2.weight': tensor([[6.0765e-11, 6.1209e-15, 2.2917e-10,  ..., 5.0858e-09, 0.0000e+00,\n",
            "         5.0510e-12],\n",
            "        [1.8258e-13, 1.2019e-12, 2.6801e-09,  ..., 2.7807e-09, 1.3339e-23,\n",
            "         1.8266e-10],\n",
            "        [5.1674e-10, 1.4081e-11, 7.6893e-09,  ..., 6.2030e-09, 7.3985e-15,\n",
            "         5.1417e-10],\n",
            "        ...,\n",
            "        [3.7937e-10, 9.9575e-13, 5.9185e-09,  ..., 8.5769e-09, 8.3442e-15,\n",
            "         7.7158e-11],\n",
            "        [1.0476e-11, 6.4586e-13, 1.9408e-09,  ..., 6.4160e-09, 3.8845e-16,\n",
            "         3.4217e-11],\n",
            "        [3.0502e-13, 3.7914e-14, 1.3245e-09,  ..., 2.4872e-09, 9.3665e-22,\n",
            "         4.6277e-11]], device='cuda:0', requires_grad=True), 'fc2.bias': tensor([5.9239e-09, 2.5597e-08, 4.5802e-08, 2.3119e-07, 8.9101e-08, 7.6767e-09,\n",
            "        1.2697e-09, 1.3092e-07, 8.4175e-09, 7.8096e-08, 1.4731e-08, 1.1650e-08,\n",
            "        9.9186e-10, 3.9115e-08, 1.7041e-08, 9.9300e-08, 2.0271e-09, 1.3815e-08,\n",
            "        3.0269e-09, 1.3251e-07, 5.9020e-09, 8.8100e-10, 2.1790e-08, 9.9907e-09,\n",
            "        2.3294e-11, 2.9233e-07, 7.2810e-10, 2.3877e-07, 7.3633e-09, 2.9161e-07,\n",
            "        3.1220e-08, 1.3637e-07, 2.2007e-08, 6.5732e-08, 7.8801e-09, 1.3304e-07,\n",
            "        2.3375e-07, 7.6420e-08, 1.2225e-08, 9.1118e-09, 1.5555e-07, 3.8893e-09,\n",
            "        2.4776e-07, 1.3638e-08, 9.9591e-08, 8.8508e-08, 4.7199e-12, 1.1644e-08,\n",
            "        5.3353e-08, 3.7453e-07, 5.8351e-08, 4.6354e-08, 1.0859e-08, 4.2782e-09,\n",
            "        1.7036e-07, 1.6944e-07, 3.8905e-09, 5.4896e-08, 2.5102e-08, 4.5442e-09,\n",
            "        3.7707e-07, 3.2475e-08, 2.0106e-08, 1.4686e-11, 7.6549e-08, 2.5468e-08,\n",
            "        9.4881e-08, 7.0493e-08, 9.5779e-08, 5.3766e-09, 1.3934e-08, 2.8691e-09,\n",
            "        7.2632e-08, 1.3239e-08, 2.4385e-09, 1.8785e-07, 2.8089e-08, 4.4667e-08,\n",
            "        3.0272e-09, 6.5970e-08, 1.0669e-08, 2.0889e-10, 1.2381e-08, 1.1987e-09,\n",
            "        5.5584e-08, 3.3277e-07, 1.2470e-08, 2.0142e-08, 3.8722e-09, 2.6694e-08,\n",
            "        1.8267e-08, 1.9500e-07, 7.7776e-08, 1.1891e-07, 1.7069e-08, 2.0763e-09,\n",
            "        2.1138e-08, 5.6040e-09, 3.7043e-08, 9.7106e-09, 1.5117e-09, 7.1019e-08,\n",
            "        5.5045e-08, 6.2808e-09, 8.6256e-08, 4.4293e-08, 7.2778e-10, 4.5818e-10,\n",
            "        4.8379e-08, 1.3089e-08, 1.3596e-08, 1.6854e-07, 2.1425e-08, 3.6225e-08,\n",
            "        3.4273e-07, 1.8271e-09, 1.3274e-07, 9.3484e-08, 7.0403e-09, 2.8920e-08,\n",
            "        3.3858e-09, 2.1894e-09, 9.1809e-08, 1.3224e-09, 8.2430e-08, 3.4513e-09,\n",
            "        1.2199e-08, 1.3438e-07, 3.0554e-08, 8.6435e-10, 8.7535e-09, 9.3187e-09,\n",
            "        3.5828e-08, 3.8771e-08, 7.0300e-09, 1.2768e-08, 1.9896e-07, 2.0397e-08,\n",
            "        9.5324e-08, 9.3051e-09, 7.2409e-08, 3.5974e-08, 1.2077e-07, 5.5228e-08,\n",
            "        9.1220e-09, 4.0273e-09, 9.4513e-08, 7.8148e-08, 7.5118e-09, 1.2851e-09,\n",
            "        8.3575e-10, 8.3940e-10, 4.7156e-08, 1.0919e-08, 1.5717e-08, 1.8106e-08,\n",
            "        6.4033e-09, 1.9308e-09, 4.3237e-07, 3.3524e-08, 5.1593e-09, 2.5457e-08,\n",
            "        3.2206e-08, 2.8137e-09, 1.5427e-08, 3.3746e-09, 1.6722e-08, 2.6661e-08,\n",
            "        6.4002e-08, 1.4320e-10, 3.1016e-09, 6.0355e-09, 6.7154e-08, 2.1180e-08,\n",
            "        4.3877e-07, 7.4628e-08, 1.2475e-08, 5.0781e-08, 5.1145e-08, 1.5404e-07,\n",
            "        1.5765e-08, 3.0581e-09, 6.4445e-08, 5.6462e-10, 2.1112e-08, 1.2061e-08,\n",
            "        1.4606e-07, 6.7925e-08, 8.6675e-12, 1.5877e-08, 1.5892e-08, 3.1675e-09,\n",
            "        7.8207e-08, 2.3489e-09, 4.1888e-08, 2.5825e-07, 9.9911e-12, 6.1533e-08,\n",
            "        4.7590e-09, 1.5304e-08, 1.0281e-08, 7.2517e-08, 1.7367e-08, 1.9104e-09,\n",
            "        1.1182e-07, 5.0214e-08, 7.5200e-08, 2.4590e-08, 8.3734e-09, 7.1571e-08,\n",
            "        1.0605e-08, 1.6970e-07, 6.9770e-09, 1.0267e-07, 3.7446e-09, 1.3703e-07,\n",
            "        2.5651e-09, 1.7283e-09, 9.8555e-09, 8.2895e-08, 1.9284e-08, 4.3298e-08,\n",
            "        1.7910e-07, 1.9445e-09, 2.5944e-08, 4.7864e-07, 3.9449e-08, 1.2557e-09,\n",
            "        4.3439e-07, 1.3408e-07, 1.6813e-08, 7.6559e-07, 2.5393e-10, 1.6981e-09,\n",
            "        1.5394e-07, 1.9743e-08, 8.3809e-09, 5.7216e-08, 7.5140e-08, 1.4293e-07,\n",
            "        2.8082e-10, 7.1473e-09, 5.0599e-08, 2.8450e-08, 1.4255e-11, 2.4133e-09,\n",
            "        3.3188e-09, 7.8514e-08, 1.0909e-08, 8.7064e-09, 7.7845e-08, 2.4676e-08,\n",
            "        1.9466e-09, 1.1183e-09, 1.4689e-07, 5.0999e-08, 2.9637e-08, 4.0828e-08,\n",
            "        4.5896e-08, 3.4812e-09, 1.5757e-08, 9.1980e-08, 4.0408e-09, 3.2668e-08,\n",
            "        2.5568e-08, 3.6710e-08, 5.3601e-09, 1.9361e-07, 4.9798e-07, 1.5354e-07,\n",
            "        8.0766e-08, 3.7976e-08, 2.2419e-08, 2.3535e-10, 2.0210e-08, 7.4914e-09,\n",
            "        1.4289e-10, 6.4020e-08, 1.3841e-09, 2.2968e-08, 5.1187e-08, 1.7203e-08,\n",
            "        3.7200e-09, 3.7553e-07, 6.7396e-08, 2.9088e-11, 7.5072e-10, 6.0948e-08,\n",
            "        1.1988e-09, 3.3058e-09, 7.8717e-08, 1.5095e-11, 8.5521e-08, 4.9789e-08,\n",
            "        6.1738e-10, 1.1795e-09, 1.7030e-08, 2.5008e-09, 2.0787e-07, 3.7558e-09,\n",
            "        5.7255e-08, 1.6619e-09, 1.0170e-08, 1.1333e-08, 1.5891e-08, 2.6578e-08,\n",
            "        1.2030e-08, 8.1330e-09, 1.1414e-07, 1.0324e-07, 8.6058e-10, 1.6033e-08,\n",
            "        1.0107e-07, 4.0019e-08, 1.1143e-09, 1.0947e-07, 2.3321e-09, 4.2953e-08,\n",
            "        7.1125e-12, 2.1461e-08, 5.3063e-09, 4.4107e-08, 1.6737e-07, 6.6733e-08,\n",
            "        1.7987e-07, 2.2750e-11, 4.6894e-08, 1.1714e-08, 1.7292e-07, 1.6008e-08,\n",
            "        1.6404e-09, 1.1845e-08, 1.2724e-08, 1.3662e-07, 2.1865e-07, 7.4537e-09,\n",
            "        1.4300e-08, 9.2633e-08, 8.1183e-08, 1.2518e-08, 2.2759e-08, 4.0092e-08,\n",
            "        1.6565e-09, 3.5292e-09, 3.8084e-08, 7.9856e-09, 1.1423e-10, 7.3392e-09,\n",
            "        1.0786e-09, 1.6407e-09, 3.3897e-08, 2.9885e-09, 3.2263e-08, 9.4205e-08,\n",
            "        3.9042e-08, 1.8771e-09, 3.0965e-08, 5.6590e-09, 2.6607e-08, 3.7322e-08,\n",
            "        1.2125e-08, 4.9204e-08, 7.0056e-08, 3.5685e-08, 1.0406e-08, 8.2201e-09,\n",
            "        1.4077e-09, 2.5161e-07, 3.3046e-08, 5.1987e-08, 1.6514e-07, 3.0652e-09,\n",
            "        3.5027e-09, 2.2654e-08, 7.4363e-08, 4.2546e-08, 1.1213e-08, 2.9550e-08,\n",
            "        3.2898e-09, 4.2705e-09, 5.3279e-10, 2.6436e-10, 6.5560e-09, 1.0185e-07,\n",
            "        9.6104e-08, 5.6247e-08, 2.3114e-08, 6.4814e-08, 1.1991e-08, 4.8942e-08,\n",
            "        7.5068e-08, 1.1347e-08, 2.7198e-08, 7.0773e-09, 2.0616e-08, 1.2926e-09,\n",
            "        1.1188e-11, 1.4477e-10, 2.5459e-08, 9.4068e-08, 2.5639e-08, 1.2461e-07,\n",
            "        4.7341e-10, 8.7304e-09, 1.6977e-08, 6.9480e-09, 5.9603e-08, 1.0716e-08,\n",
            "        1.3686e-07, 3.6179e-10, 1.3559e-07, 1.2580e-09, 2.4557e-07, 2.5180e-08,\n",
            "        7.0372e-09, 1.3189e-09, 5.9044e-08, 2.5239e-15, 4.1049e-09, 1.4162e-09,\n",
            "        2.0373e-10, 1.4416e-08, 6.2258e-09, 1.7075e-08, 9.9134e-09, 9.6796e-08,\n",
            "        7.0987e-09, 1.0339e-07, 5.2473e-08, 2.0463e-08, 2.2058e-09, 2.6820e-08,\n",
            "        7.8183e-09, 2.4853e-08, 1.0165e-07, 2.5111e-07, 2.5524e-07, 5.9107e-10,\n",
            "        1.6579e-08, 1.5742e-08, 1.8093e-07, 7.8453e-08, 2.6974e-08, 7.5946e-09,\n",
            "        1.1611e-08, 5.8109e-08, 3.1434e-08, 1.1211e-08, 1.2491e-07, 7.6793e-08,\n",
            "        1.7424e-08, 3.3332e-08, 1.0019e-08, 2.9594e-09, 3.1738e-08, 1.0374e-10,\n",
            "        1.3214e-08, 2.0964e-08, 4.2089e-08, 1.6958e-09, 5.7407e-10, 2.9336e-10,\n",
            "        1.0971e-08, 4.2374e-08, 3.4083e-08, 5.6032e-09, 8.6324e-08, 7.2331e-08,\n",
            "        4.5977e-08, 1.1024e-07, 1.7346e-08, 5.4047e-08, 8.4009e-09, 3.6395e-08,\n",
            "        2.5804e-09, 2.4061e-08, 6.6736e-08, 3.8706e-08, 7.1751e-09, 8.8166e-09,\n",
            "        4.3781e-09, 2.5805e-08, 6.6565e-09, 2.9031e-08, 3.9257e-08, 3.1125e-08,\n",
            "        3.6546e-08, 1.6791e-08, 8.5829e-08, 6.0685e-07, 6.6563e-08, 3.4782e-09,\n",
            "        1.6405e-07, 1.3133e-07, 5.8062e-08, 2.5297e-09, 4.4859e-09, 1.4269e-09,\n",
            "        7.5791e-10, 3.6138e-08, 3.1083e-09, 5.5625e-09, 1.0762e-08, 1.3922e-09,\n",
            "        8.6514e-08, 4.1368e-09, 6.8070e-09, 2.8918e-09, 2.9611e-08, 1.1684e-08,\n",
            "        6.0558e-09, 2.1503e-08], device='cuda:0', requires_grad=True), 'classifier.weight': tensor([[3.5700e-09, 2.2127e-07, 1.1460e-06,  ..., 5.5722e-08, 1.8583e-09,\n",
            "         6.1145e-07],\n",
            "        [2.3323e-10, 2.0246e-07, 5.4716e-08,  ..., 1.0002e-08, 1.3250e-10,\n",
            "         2.7842e-07],\n",
            "        [1.3885e-08, 2.7278e-08, 5.4134e-06,  ..., 8.3157e-08, 8.0303e-08,\n",
            "         7.2815e-08],\n",
            "        ...,\n",
            "        [3.3506e-13, 1.5652e-12, 7.4356e-11,  ..., 2.4370e-12, 2.9487e-13,\n",
            "         8.8901e-12],\n",
            "        [3.1861e-13, 1.4978e-12, 7.3005e-11,  ..., 2.4575e-12, 2.8390e-13,\n",
            "         8.6255e-12],\n",
            "        [3.2815e-13, 1.5182e-12, 7.4377e-11,  ..., 2.4715e-12, 2.9295e-13,\n",
            "         8.6529e-12]], device='cuda:0', requires_grad=True), 'classifier.bias': tensor([3.6461e-06, 1.2136e-06, 2.3737e-05, 4.9361e-06, 3.1036e-05, 4.2725e-08,\n",
            "        1.1928e-09, 1.2115e-09, 1.1889e-09, 1.2044e-09], device='cuda:0',\n",
            "       requires_grad=True)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, fisher_matrix, prev_params):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "\n",
        "        # Original loss\n",
        "        ce_loss = loss_fn(pred, y)\n",
        "\n",
        "        # EWC loss\n",
        "        ewc_loss = get_ewc_loss(model, fisher_matrix, prev_params)\n",
        "\n",
        "        loss = ce_loss + ewc_lambda * ewc_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch+1)*len(X)\n",
        "            print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ],
      "metadata": {
        "id": "k9cHcxAN3s3Q"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(epoch):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(eval_dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            print(torch.unique(predicted_old))\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ],
      "metadata": {
        "id": "YOE0I97T8HAH"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch+1}: ----------------------\")\n",
        "    train(train_dataloader_second, model, loss_fn, optimizer, fisher_matrix, prev_params)\n",
        "    test(test_dataloader_second, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv-BzHy78l06",
        "outputId": "7097b717-0857-4161-91fc-a487c43b73ec"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: ----------------------\n",
            "Loss: 14.331015,    64/24000\n",
            "Loss: 0.281682,  6464/24000\n",
            "Loss: 0.269779, 12864/24000\n",
            "Loss: 0.096627, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 95.5, Avg Loss: 0.125358\n",
            "\n",
            "Epoch 2: ----------------------\n",
            "Loss: 0.157882,    64/24000\n",
            "Loss: 0.140260,  6464/24000\n",
            "Loss: 0.069510, 12864/24000\n",
            "Loss: 0.083681, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 96.0, Avg Loss: 0.110670\n",
            "\n",
            "Epoch 3: ----------------------\n",
            "Loss: 0.114369,    64/24000\n",
            "Loss: 0.064815,  6464/24000\n",
            "Loss: 0.142015, 12864/24000\n",
            "Loss: 0.082133, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 96.7, Avg Loss: 0.097747\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val(1)"
      ],
      "metadata": {
        "id": "1_oObVs6s35e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4eda533-2673-4602-d974-2dc6946874b9"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([0, 1, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "tensor([1, 3, 6, 7, 8, 9], device='cuda:0')\n",
            "Validation Acc: 48.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fisher_matrix = get_fisher_diag(model, train_dataloader_first, model.named_parameters())"
      ],
      "metadata": {
        "id": "srlBPxOpN8AQ"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fisher_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSXAnSI3OCIf",
        "outputId": "ea9d8944-4a7e-47b2-8ff0-7a6d08c3752b"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1.weight': tensor([[2.7461e-07, 2.7452e-07, 2.7371e-07,  ..., 1.4784e-07, 2.4882e-07,\n",
            "         2.7414e-07],\n",
            "        [2.0734e-07, 2.0731e-07, 2.0710e-07,  ..., 1.9095e-07, 2.0407e-07,\n",
            "         2.0714e-07],\n",
            "        [3.0424e-06, 3.0421e-06, 3.0385e-06,  ..., 2.6609e-06, 2.9924e-06,\n",
            "         3.0401e-06],\n",
            "        ...,\n",
            "        [4.4956e-06, 4.4947e-06, 4.4922e-06,  ..., 4.3381e-06, 4.5258e-06,\n",
            "         4.4957e-06],\n",
            "        [3.5764e-07, 3.5759e-07, 3.5729e-07,  ..., 3.2840e-07, 3.5391e-07,\n",
            "         3.5759e-07],\n",
            "        [4.5066e-09, 4.5063e-09, 4.5060e-09,  ..., 4.2264e-09, 4.4487e-09,\n",
            "         4.4994e-09]], device='cuda:0', requires_grad=True), 'fc1.bias': tensor([2.7461e-07, 2.0734e-07, 3.0424e-06, 1.1354e-05, 6.0132e-08, 2.2335e-08,\n",
            "        2.1391e-07, 1.6250e-11, 3.5841e-06, 4.4533e-07, 9.3733e-07, 4.0175e-08,\n",
            "        1.0547e-11, 4.9968e-09, 5.6397e-12, 4.9934e-06, 3.7685e-12, 6.9628e-06,\n",
            "        7.3301e-06, 1.4020e-06, 8.9563e-07, 1.1601e-05, 4.6595e-13, 7.9518e-07,\n",
            "        1.5050e-11, 8.1805e-10, 1.3965e-05, 0.0000e+00, 3.2295e-05, 4.8208e-14,\n",
            "        2.7411e-12, 2.3882e-10, 1.0097e-05, 3.1028e-07, 3.3031e-11, 1.7008e-06,\n",
            "        8.9766e-05, 2.1701e-09, 9.9479e-12, 8.1548e-11, 1.3191e-09, 1.3806e-09,\n",
            "        6.6391e-06, 7.8118e-08, 3.2783e-05, 2.1855e-11, 5.2670e-08, 1.9830e-08,\n",
            "        8.3002e-08, 1.7044e-09, 1.3976e-06, 1.6483e-11, 1.5303e-06, 3.0951e-04,\n",
            "        9.5866e-11, 6.7389e-08, 1.4985e-10, 1.4281e-13, 2.1361e-05, 1.6464e-08,\n",
            "        1.3736e-06, 8.4586e-11, 2.4481e-06, 2.2471e-07, 2.3216e-11, 8.2700e-09,\n",
            "        1.5533e-10, 1.2292e-12, 1.0293e-08, 1.3011e-07, 7.5172e-06, 2.9921e-05,\n",
            "        3.3521e-06, 2.3604e-11, 1.9896e-06, 9.6844e-07, 2.2490e-06, 1.3326e-12,\n",
            "        3.6424e-11, 2.8733e-10, 6.4389e-13, 2.6307e-06, 2.3763e-11, 2.8802e-06,\n",
            "        3.2607e-12, 7.2350e-09, 1.9588e-11, 8.1384e-10, 1.1423e-06, 1.0568e-07,\n",
            "        2.0842e-09, 2.2438e-11, 9.6813e-10, 1.7420e-10, 7.1444e-09, 2.6970e-08,\n",
            "        9.7491e-11, 3.1691e-11, 9.4065e-05, 7.8206e-09, 5.8481e-11, 1.0148e-06,\n",
            "        1.2330e-10, 9.4935e-06, 1.6065e-05, 6.0850e-07, 1.0915e-09, 3.4304e-12,\n",
            "        2.3307e-06, 8.5251e-09, 1.3246e-11, 8.9982e-10, 1.4817e-11, 1.4178e-05,\n",
            "        6.2032e-12, 1.5642e-12, 2.3657e-08, 3.7369e-10, 2.4318e-11, 1.3300e-10,\n",
            "        0.0000e+00, 6.5666e-11, 1.4365e-11, 2.0797e-10, 7.8704e-07, 7.8581e-10,\n",
            "        6.7269e-10, 3.3780e-13, 7.3922e-12, 4.1002e-05, 0.0000e+00, 2.0850e-09,\n",
            "        3.8750e-12, 5.2712e-06, 8.3270e-09, 9.6982e-10, 1.2872e-05, 9.1350e-07,\n",
            "        1.2315e-12, 2.3625e-11, 1.0879e-10, 1.9368e-06, 2.5904e-07, 1.8633e-08,\n",
            "        1.5063e-06, 1.5658e-06, 2.0936e-09, 0.0000e+00, 4.4419e-06, 4.8088e-12,\n",
            "        5.6951e-07, 1.6771e-06, 1.6281e-09, 4.4743e-06, 4.9302e-07, 5.4302e-09,\n",
            "        5.9764e-11, 3.1288e-07, 1.4841e-06, 3.0191e-06, 0.0000e+00, 1.6386e-11,\n",
            "        6.2473e-09, 1.4215e-09, 3.1993e-09, 1.9025e-12, 3.2325e-11, 2.9926e-12,\n",
            "        3.4079e-05, 4.2197e-10, 9.8192e-12, 8.8474e-08, 5.2005e-11, 5.1983e-10,\n",
            "        8.9539e-05, 1.2931e-05, 7.3446e-11, 1.6175e-08, 3.9987e-10, 1.0629e-05,\n",
            "        1.8279e-06, 5.8689e-06, 1.1215e-08, 4.2388e-08, 1.9893e-05, 1.6044e-06,\n",
            "        6.1356e-06, 2.5328e-07, 7.3377e-10, 2.2609e-07, 2.3454e-08, 1.4916e-05,\n",
            "        2.1314e-11, 1.2843e-11, 1.1418e-09, 8.2638e-06, 3.9170e-06, 9.2086e-09,\n",
            "        1.7144e-06, 2.1202e-05, 2.8742e-06, 0.0000e+00, 8.2029e-06, 7.2694e-05,\n",
            "        2.4637e-07, 4.6399e-06, 6.1930e-09, 7.5130e-10, 7.8023e-09, 9.3824e-11,\n",
            "        7.9507e-08, 1.0177e-10, 7.8051e-12, 8.1219e-06, 2.7997e-10, 8.2205e-05,\n",
            "        3.0359e-06, 4.1458e-07, 3.5587e-12, 2.4526e-07, 3.4480e-06, 2.7646e-06,\n",
            "        6.8832e-09, 1.2957e-09, 7.2145e-11, 5.6255e-08, 1.7032e-08, 1.1967e-07,\n",
            "        2.1288e-06, 3.9966e-10, 8.4395e-08, 5.3620e-10, 1.3554e-05, 9.0408e-09,\n",
            "        9.1784e-08, 4.0324e-07, 2.6707e-10, 1.1285e-07, 4.4823e-06, 3.7428e-07,\n",
            "        9.9626e-11, 2.6089e-11, 1.2356e-10, 1.2121e-09, 1.8544e-10, 8.3597e-12,\n",
            "        1.2797e-10, 8.8605e-07, 3.7751e-06, 6.9345e-06, 1.7827e-06, 6.9614e-11,\n",
            "        1.1288e-11, 2.8928e-09, 4.6610e-07, 1.6427e-10, 2.3674e-08, 5.2010e-10,\n",
            "        1.0567e-11, 2.7217e-07, 3.8993e-09, 7.3000e-06, 1.8196e-06, 2.8195e-11,\n",
            "        8.2619e-07, 3.5314e-12, 2.0395e-09, 0.0000e+00, 1.5976e-10, 0.0000e+00,\n",
            "        1.0462e-09, 1.4245e-08, 1.4972e-07, 4.7912e-13, 2.2339e-12, 3.2344e-11,\n",
            "        2.3713e-11, 1.8113e-12, 2.8736e-06, 2.0722e-11, 4.8060e-06, 3.0325e-11,\n",
            "        1.8319e-07, 7.7528e-03, 6.7602e-12, 6.7865e-10, 1.5708e-11, 1.4397e-08,\n",
            "        0.0000e+00, 1.6953e-06, 4.1469e-05, 5.0051e-09, 8.2585e-06, 4.1813e-11,\n",
            "        3.5248e-09, 1.2030e-08, 7.5194e-10, 3.6711e-13, 8.6164e-10, 1.9370e-08,\n",
            "        6.3847e-06, 4.2744e-07, 4.9499e-07, 4.8199e-08, 3.1530e-12, 2.3559e-08,\n",
            "        1.8596e-05, 2.1789e-09, 2.0305e-06, 1.3248e-11, 1.3814e-09, 3.1686e-06,\n",
            "        1.1835e-05, 1.1554e-05, 3.3934e-07, 9.5885e-06, 2.2037e-09, 2.3121e-13,\n",
            "        1.7733e-12, 3.8423e-06, 2.9931e-06, 8.3114e-08, 1.5140e-05, 3.7908e-13,\n",
            "        4.7063e-07, 1.0072e-03, 1.4367e-05, 2.5439e-11, 3.7920e-08, 3.3425e-06,\n",
            "        1.4762e-06, 8.7377e-06, 2.0413e-06, 6.8976e-06, 3.7718e-11, 0.0000e+00,\n",
            "        0.0000e+00, 1.9107e-10, 1.9189e-12, 2.6967e-08, 1.2617e-10, 1.3885e-09,\n",
            "        3.8941e-04, 3.5951e-06, 6.0058e-12, 3.3513e-06, 1.4261e-08, 8.9292e-11,\n",
            "        3.6604e-12, 2.1169e-10, 6.3878e-07, 5.3432e-09, 6.3587e-12, 5.8631e-10,\n",
            "        2.0959e-11, 1.2106e-10, 2.5423e-07, 4.5594e-06, 6.8318e-07, 7.5050e-07,\n",
            "        1.7824e-10, 1.4675e-05, 7.2202e-07, 0.0000e+00, 3.9219e-11, 1.0458e-07,\n",
            "        9.4083e-06, 1.7149e-05, 1.4109e-05, 4.1840e-11, 7.1886e-13, 2.7171e-06,\n",
            "        1.5835e-03, 1.4246e-11, 2.1903e-07, 7.6430e-09, 1.2040e-07, 5.9204e-11,\n",
            "        5.6309e-08, 1.7386e-11, 7.8006e-13, 1.0775e-06, 1.9097e-05, 3.0729e-07,\n",
            "        1.4178e-06, 6.8858e-05, 2.8138e-10, 1.0234e-06, 1.3920e-11, 1.8125e-06,\n",
            "        1.1631e-09, 1.6982e-03, 4.7710e-10, 7.4699e-08, 1.9250e-11, 1.0172e-05,\n",
            "        2.7560e-11, 4.1944e-08, 6.8623e-09, 4.0838e-08, 2.7367e-09, 4.1324e-12,\n",
            "        1.5686e-05, 1.5809e-09, 1.2741e-12, 0.0000e+00, 1.0373e-05, 2.1023e-05,\n",
            "        5.3102e-07, 2.8954e-10, 4.9283e-12, 5.1675e-11, 1.7842e-13, 9.7269e-05,\n",
            "        2.8903e-06, 9.6256e-11, 3.7384e-04, 4.2637e-06, 6.4389e-07, 9.5471e-05,\n",
            "        1.9863e-12, 9.3523e-09, 2.9433e-08, 3.7940e-10, 1.5705e-10, 1.1181e-04,\n",
            "        1.2794e-05, 2.0781e-07, 3.8379e-06, 3.2275e-06, 3.8443e-13, 1.8772e-05,\n",
            "        3.3361e-09, 6.2279e-11, 9.0749e-06, 1.1273e-06, 2.8469e-11, 7.5170e-10,\n",
            "        1.0762e-07, 5.1272e-11, 2.2980e-07, 6.1481e-13, 2.0450e-07, 2.8054e-09,\n",
            "        7.7789e-10, 1.3841e-09, 9.8241e-14, 2.6651e-06, 3.5235e-08, 1.1595e-06,\n",
            "        3.3828e-09, 1.8786e-05, 9.9028e-13, 0.0000e+00, 0.0000e+00, 1.1806e-12,\n",
            "        3.1689e-06, 4.2632e-10, 1.2182e-05, 1.6371e-10, 3.7338e-09, 7.8280e-06,\n",
            "        4.2389e-03, 9.0481e-11, 2.9210e-10, 2.0142e-08, 2.8402e-06, 3.9061e-07,\n",
            "        1.4087e-12, 6.8552e-12, 3.3100e-09, 2.5320e-10, 5.3522e-11, 8.0136e-12,\n",
            "        1.3104e-06, 1.0030e-10, 1.4694e-12, 4.8428e-12, 3.4669e-06, 3.6116e-11,\n",
            "        1.4508e-05, 6.5753e-12, 1.2978e-06, 7.6371e-10, 2.3381e-08, 7.6716e-10,\n",
            "        1.3191e-06, 1.5274e-07, 1.7955e-09, 9.7886e-07, 1.2439e-06, 1.0199e-07,\n",
            "        6.3150e-07, 4.1575e-09, 1.7795e-11, 2.1702e-12, 4.9687e-07, 5.3272e-07,\n",
            "        8.4874e-06, 3.0929e-13, 1.1340e-16, 1.5632e-08, 4.4075e-12, 1.7732e-12,\n",
            "        1.8485e-09, 9.5681e-06, 3.4864e-06, 2.8592e-06, 4.5382e-05, 4.4956e-06,\n",
            "        3.5764e-07, 4.5066e-09], device='cuda:0', requires_grad=True), 'fc2.weight': tensor([[4.9009e-10, 9.3645e-11, 8.8826e-10,  ..., 1.2808e-07, 3.5690e-11,\n",
            "         6.1068e-14],\n",
            "        [0.0000e+00, 1.0389e-11, 2.0715e-09,  ..., 2.2200e-08, 1.2038e-10,\n",
            "         1.7983e-12],\n",
            "        [2.1646e-08, 4.5325e-09, 4.4738e-06,  ..., 9.2228e-07, 3.2028e-08,\n",
            "         2.9645e-09],\n",
            "        ...,\n",
            "        [8.3873e-07, 1.9619e-07, 1.0545e-04,  ..., 1.1382e-04, 4.7486e-07,\n",
            "         1.8482e-08],\n",
            "        [3.9683e-10, 1.7777e-10, 2.8262e-07,  ..., 3.9354e-08, 2.9501e-10,\n",
            "         1.0522e-10],\n",
            "        [0.0000e+00, 4.3878e-10, 1.4352e-07,  ..., 5.0102e-08, 7.2995e-09,\n",
            "         1.1165e-10]], device='cuda:0', requires_grad=True), 'fc2.bias': tensor([3.4100e-07, 2.2446e-06, 2.0647e-05, 9.0681e-05, 9.5711e-06, 1.2039e-07,\n",
            "        4.1026e-08, 2.4850e-05, 4.4519e-07, 4.0651e-06, 6.9245e-08, 2.4737e-07,\n",
            "        3.0235e-08, 1.0838e-06, 2.7186e-06, 6.0453e-06, 1.7481e-10, 8.8420e-07,\n",
            "        8.5688e-08, 8.1558e-05, 2.0428e-04, 1.0973e-03, 2.5154e-07, 8.7400e-07,\n",
            "        8.5744e-11, 2.5922e-05, 4.0183e-08, 9.1983e-06, 4.6222e-07, 2.2025e-05,\n",
            "        1.6147e-06, 3.9861e-05, 2.9486e-06, 7.3020e-06, 1.5559e-06, 1.5805e-06,\n",
            "        1.2320e-05, 5.0089e-06, 5.6157e-07, 8.2452e-07, 1.0138e-06, 1.6170e-05,\n",
            "        1.4324e-06, 4.1477e-07, 2.4924e-05, 3.0301e-05, 7.4900e-05, 1.2299e-07,\n",
            "        4.4643e-07, 1.1855e-04, 2.5318e-07, 2.2932e-07, 3.5896e-07, 1.6229e-07,\n",
            "        2.9394e-06, 6.2942e-06, 2.8284e-07, 1.2607e-05, 4.5866e-08, 1.1184e-08,\n",
            "        1.0610e-03, 9.6175e-07, 7.6171e-07, 5.0407e-08, 9.3334e-07, 1.6961e-06,\n",
            "        1.8691e-07, 1.5392e-05, 4.9977e-06, 1.1004e-07, 2.3400e-06, 3.7623e-09,\n",
            "        2.1426e-06, 1.7892e-08, 1.2733e-08, 3.7161e-05, 2.2407e-06, 2.9977e-06,\n",
            "        2.8934e-08, 3.3613e-07, 2.8189e-08, 3.5407e-11, 1.2669e-07, 1.6092e-08,\n",
            "        7.7162e-06, 2.7532e-05, 4.4418e-07, 2.8336e-07, 2.6000e-10, 7.2735e-07,\n",
            "        2.7941e-03, 2.1059e-05, 1.1564e-05, 4.6217e-06, 5.0499e-06, 4.6609e-08,\n",
            "        1.5512e-06, 5.5146e-09, 3.7797e-07, 3.1881e-07, 9.8497e-09, 1.3036e-07,\n",
            "        8.1723e-07, 2.5832e-04, 1.7089e-06, 9.5068e-06, 0.0000e+00, 1.6302e-07,\n",
            "        7.3992e-07, 9.7393e-08, 1.1981e-06, 6.0135e-05, 3.6162e-07, 3.4200e-06,\n",
            "        4.2338e-05, 1.1751e-08, 3.0307e-05, 3.0049e-05, 5.0996e-08, 3.1133e-06,\n",
            "        2.1876e-07, 1.6652e-08, 2.9827e-06, 1.0520e-08, 5.1770e-06, 1.9054e-11,\n",
            "        9.2837e-07, 2.4387e-06, 1.3618e-07, 1.2827e-07, 4.2272e-07, 2.7500e-07,\n",
            "        4.0574e-06, 3.8190e-06, 2.4294e-03, 4.7559e-06, 7.3809e-05, 3.0241e-07,\n",
            "        8.3877e-06, 1.2139e-06, 4.5628e-07, 2.6233e-05, 3.3883e-06, 5.9386e-06,\n",
            "        6.4382e-08, 0.0000e+00, 1.7331e-05, 3.8396e-06, 2.5022e-08, 9.2822e-09,\n",
            "        1.2125e-08, 3.8278e-08, 6.9054e-06, 2.7616e-08, 9.9055e-07, 7.5109e-08,\n",
            "        6.9440e-08, 1.1293e-07, 4.8909e-05, 8.7829e-07, 5.5699e-08, 2.6547e-07,\n",
            "        8.6854e-07, 8.0723e-11, 5.2333e-07, 5.6040e-08, 1.5505e-07, 1.3423e-07,\n",
            "        1.4315e-06, 1.4136e-08, 6.4347e-08, 4.6497e-07, 4.7455e-06, 2.8210e-06,\n",
            "        3.1214e-05, 8.1252e-07, 1.1244e-07, 4.2153e-07, 6.4136e-06, 9.7401e-06,\n",
            "        5.5020e-04, 6.8047e-09, 1.8319e-06, 7.3981e-10, 7.0140e-07, 2.0351e-07,\n",
            "        1.1879e-05, 9.0315e-07, 3.0244e-11, 5.5124e-07, 4.7182e-07, 2.4048e-08,\n",
            "        6.2833e-07, 2.5044e-09, 6.1323e-07, 8.6278e-07, 1.8599e-09, 5.7446e-06,\n",
            "        1.0852e-07, 2.6484e-08, 2.6483e-07, 5.3670e-06, 1.4928e-07, 5.4539e-10,\n",
            "        2.1825e-05, 4.2847e-06, 3.2636e-06, 9.8816e-07, 1.0223e-07, 3.7869e-06,\n",
            "        1.8494e-06, 2.4325e-06, 5.8833e-08, 2.3416e-06, 3.2953e-03, 4.8677e-07,\n",
            "        8.7693e-08, 3.1299e-08, 1.2320e-07, 1.0985e-06, 2.5264e-07, 9.2259e-07,\n",
            "        4.5801e-06, 3.9918e-04, 1.1391e-06, 4.2253e-05, 1.3029e-06, 4.3411e-09,\n",
            "        7.5418e-05, 3.0649e-06, 5.3873e-07, 6.5729e-06, 1.9119e-11, 4.9294e-08,\n",
            "        2.6243e-05, 3.1292e-05, 1.1868e-07, 5.3911e-07, 7.7404e-06, 1.4118e-06,\n",
            "        7.2666e-09, 1.6748e-05, 8.0264e-06, 1.4933e-05, 3.9472e-07, 4.8848e-09,\n",
            "        4.9084e-09, 1.9993e-05, 4.3316e-07, 2.6277e-07, 8.2739e-06, 3.6499e-07,\n",
            "        4.0449e-07, 1.4992e-08, 1.6943e-05, 2.2909e-07, 8.6167e-06, 4.0203e-06,\n",
            "        1.3420e-06, 1.2781e-07, 7.6468e-07, 2.7467e-06, 2.9009e-08, 6.7284e-07,\n",
            "        2.0195e-07, 1.3804e-06, 2.1909e-07, 5.3416e-05, 1.5491e-04, 4.7050e-06,\n",
            "        2.7246e-05, 7.5982e-07, 4.8966e-07, 2.9243e-11, 8.2513e-08, 2.2927e-07,\n",
            "        2.6537e-11, 3.3188e-06, 6.6503e-10, 5.0152e-06, 4.7805e-07, 1.8514e-06,\n",
            "        2.3786e-08, 8.6274e-05, 2.8859e-05, 9.3059e-04, 9.4012e-12, 4.3196e-06,\n",
            "        2.9943e-08, 2.1890e-07, 3.1255e-05, 1.3549e-10, 3.2346e-06, 3.1392e-07,\n",
            "        0.0000e+00, 8.0561e-08, 3.2813e-07, 1.5881e-08, 2.0026e-05, 1.1152e-05,\n",
            "        8.1265e-06, 1.3682e-06, 2.4775e-07, 7.4981e-06, 1.2714e-06, 3.1226e-06,\n",
            "        3.8887e-07, 2.7757e-08, 6.6139e-06, 5.1762e-05, 1.4588e-09, 1.7256e-06,\n",
            "        2.6876e-06, 1.3147e-05, 1.6017e-03, 4.6182e-06, 7.7580e-08, 4.0849e-06,\n",
            "        1.2866e-11, 2.8104e-06, 2.1307e-07, 2.4930e-06, 1.1883e-06, 4.5124e-08,\n",
            "        2.9841e-05, 2.5041e-05, 1.7065e-05, 1.4252e-08, 4.7003e-05, 1.8798e-05,\n",
            "        2.1568e-08, 1.0971e-07, 5.2369e-07, 4.9635e-05, 4.4130e-05, 8.3873e-07,\n",
            "        2.1786e-06, 6.7444e-06, 2.6832e-06, 2.5608e-07, 2.2715e-06, 7.1841e-07,\n",
            "        2.8169e-10, 1.0316e-07, 5.7435e-06, 1.1785e-07, 1.7211e-07, 3.9118e-08,\n",
            "        1.5297e-08, 1.8229e-05, 5.6463e-06, 4.6279e-07, 4.7912e-07, 3.0111e-05,\n",
            "        1.4958e-05, 2.0826e-07, 3.4012e-07, 1.9212e-07, 9.1424e-06, 5.7387e-07,\n",
            "        1.9906e-07, 7.6618e-06, 8.6063e-06, 4.7929e-06, 2.2834e-07, 2.4829e-06,\n",
            "        3.2640e-09, 1.4825e-05, 5.6372e-06, 3.9585e-07, 3.0743e-05, 1.7810e-07,\n",
            "        4.5834e-09, 2.4292e-07, 1.2144e-05, 3.3727e-06, 1.8760e-07, 9.4675e-08,\n",
            "        6.1746e-09, 1.7189e-07, 2.4203e-10, 1.4371e-08, 8.3273e-08, 9.7621e-06,\n",
            "        3.4211e-05, 1.2835e-06, 1.0865e-07, 9.6314e-07, 4.4286e-07, 2.9075e-06,\n",
            "        1.0914e-06, 4.6811e-09, 1.9397e-04, 1.0900e-07, 5.7114e-07, 0.0000e+00,\n",
            "        2.8518e-04, 1.7404e-06, 3.6000e-06, 2.7932e-05, 1.9157e-07, 1.2030e-05,\n",
            "        4.7667e-07, 3.8785e-06, 1.3688e-06, 2.5787e-08, 7.4933e-07, 5.9295e-07,\n",
            "        1.9712e-06, 4.8107e-10, 5.1286e-06, 1.4482e-07, 1.9755e-06, 5.2841e-07,\n",
            "        2.2056e-07, 4.9868e-08, 8.7945e-06, 0.0000e+00, 4.2253e-07, 9.1628e-04,\n",
            "        4.6571e-10, 1.8923e-07, 6.0308e-07, 6.9315e-08, 1.6753e-07, 1.7829e-05,\n",
            "        1.5041e-07, 6.3029e-06, 4.4861e-07, 4.2076e-06, 2.6703e-07, 5.3740e-06,\n",
            "        4.1965e-07, 7.6265e-07, 4.0616e-07, 5.7170e-05, 7.0575e-06, 1.6829e-10,\n",
            "        8.3738e-07, 1.2229e-06, 1.7654e-05, 1.3048e-05, 1.8079e-06, 7.0762e-08,\n",
            "        4.4420e-08, 5.0501e-06, 3.1787e-06, 6.3042e-06, 7.1220e-06, 1.8848e-05,\n",
            "        2.8585e-06, 9.0674e-07, 5.9266e-07, 3.1949e-07, 7.4159e-07, 5.2772e-10,\n",
            "        2.9041e-07, 1.5449e-06, 6.6016e-06, 1.4786e-04, 3.5334e-09, 3.4023e-10,\n",
            "        2.6994e-07, 1.1851e-06, 8.2980e-07, 7.8492e-09, 3.1558e-07, 3.5609e-06,\n",
            "        1.0284e-05, 2.4677e-06, 2.0686e-06, 3.5876e-05, 2.5650e-07, 1.6519e-07,\n",
            "        3.0986e-09, 3.3559e-06, 2.8522e-06, 1.2878e-06, 2.0542e-07, 1.1153e-06,\n",
            "        6.1106e-07, 5.5358e-06, 3.2958e-08, 4.8544e-07, 7.9637e-06, 8.1762e-07,\n",
            "        8.9502e-06, 4.0418e-07, 6.7110e-06, 1.7991e-05, 2.1167e-06, 2.5099e-07,\n",
            "        4.7995e-05, 1.0511e-06, 9.5619e-06, 0.0000e+00, 1.1258e-10, 6.1490e-09,\n",
            "        4.3587e-09, 3.7221e-06, 2.7510e-08, 3.1033e-07, 6.5921e-08, 1.3295e-09,\n",
            "        1.5239e-05, 6.1941e-08, 1.4169e-06, 3.0615e-09, 1.5942e-06, 8.2145e-04,\n",
            "        2.0998e-07, 1.5214e-05], device='cuda:0', requires_grad=True), 'classifier.weight': tensor([[1.7282e-09, 2.1815e-07, 1.1320e-04,  ..., 3.4702e-03, 3.1189e-09,\n",
            "         2.5851e-06],\n",
            "        [3.0747e-11, 1.1231e-07, 5.5266e-09,  ..., 6.6528e-05, 5.8241e-10,\n",
            "         2.4472e-07],\n",
            "        [4.4516e-09, 1.0108e-08, 1.5167e-06,  ..., 1.8494e-03, 4.7452e-06,\n",
            "         5.2347e-09],\n",
            "        ...,\n",
            "        [3.4093e-13, 6.9195e-08, 1.4197e-09,  ..., 1.3664e-04, 2.9797e-13,\n",
            "         7.2091e-10],\n",
            "        [6.6304e-09, 2.0259e-06, 2.8017e-08,  ..., 6.8939e-05, 3.5348e-10,\n",
            "         3.8824e-07],\n",
            "        [2.7398e-13, 5.0920e-08, 8.9504e-10,  ..., 5.2605e-06, 5.4405e-14,\n",
            "         6.9188e-10]], device='cuda:0', requires_grad=True), 'classifier.bias': tensor([4.6187e-04, 1.6159e-05, 4.6481e-04, 3.6119e-04, 4.6764e-04, 4.6549e-04,\n",
            "        6.6019e-03, 1.7708e-04, 2.6192e-05, 4.9840e-05], device='cuda:0',\n",
            "       requires_grad=True)}\n"
          ]
        }
      ]
    }
  ]
}