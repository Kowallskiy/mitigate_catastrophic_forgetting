{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "GzUzjT_SM22m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask1(datasets.FashionMNIST):\n",
        "  def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "    super(FashionMNISTtask1, self).__init__(root, train=train, transform=transform, target_transform=target_transform,download=download)\n",
        "    self.classes = self.classes[:6]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img, target = super(FashionMNISTtask1, self).__getitem__(index)\n",
        "    if target < 6:\n",
        "        return img, target\n",
        "    else:\n",
        "        return img, -1\n"
      ],
      "metadata": {
        "id": "iowt64kIx-pn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask2(datasets.FashionMNIST):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(FashionMNISTtask2, self).__init__(root, train=train, transform=transform, target_transform=target_transform,download=download)\n",
        "        self.classes = self.classes[6:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super(FashionMNISTtask2, self).__getitem__(index)\n",
        "        if target >= 6:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img, -1"
      ],
      "metadata": {
        "id": "X1tlnFkE3jO1"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([ToTensor(), Normalize((0.5), (0.5))])\n",
        "\n",
        "train_dataset_1 = FashionMNISTtask1(root='./data1', train=True, transform=transform, download=True)\n",
        "test_dataset_1 = FashionMNISTtask1(root='./data1', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "Es33ycfn0wgr"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_2 = FashionMNISTtask2(root='./data2', train=True, transform=transform, download=True)\n",
        "test_dataset_2 = FashionMNISTtask2(root='./data2', train=False, transform=transform, download=True)\n",
        "test_dataset_3 = datasets.FashionMNIST(\n",
        "    root='data3',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "ItSNRkOm4vBs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(datasets.FashionMNIST))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GleR8_dIbKq6",
        "outputId": "43682bdc-24e0-4155-cb25-e04d172cf408"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_exists', '_check_legacy_exist', '_format_transform_repr', '_is_protocol', '_load_data', '_load_legacy_data', '_repr_indent', 'class_to_idx', 'classes', 'download', 'extra_repr', 'mirrors', 'processed_folder', 'raw_folder', 'resources', 'test_data', 'test_file', 'test_labels', 'train_data', 'train_labels', 'training_file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_filtered_old = [data for data in train_dataset_1 if data[1] != -1]\n",
        "test_dataset_filtered_old = [data for data in test_dataset_1 if data[1] != -1]\n",
        "\n",
        "train_dataset_filtered_new = [data for data in train_dataset_2 if data[1] != -1]\n",
        "test_dataset_filtered_new = [data for data in test_dataset_2 if data[1] != -1]"
      ],
      "metadata": {
        "id": "PWyzJURmqBjQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_old = DataLoader(train_dataset_filtered_old, batch_size=64, shuffle=True)\n",
        "test_dataloader_old = DataLoader(test_dataset_filtered_old, batch_size=256, shuffle=False)\n",
        "\n",
        "for X, y in train_dataloader_old:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"SHape of y: {y.shape}, dtype: {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "fl2AkOV8POXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a970f887-0818-41af-8e2d-83bb6cc46d24"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "SHape of y: torch.Size([64]), dtype: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_old)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOY9PckQp-O3",
        "outputId": "78fcc021-b281-4e81-b9b7-3c7fbf9842a9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "563"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_new = DataLoader(train_dataset_filtered_new, batch_size=64, shuffle=True)\n",
        "test_dataloader_new = DataLoader(test_dataset_filtered_new, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "HKo3B_gW5ASM"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataloader = DataLoader(test_dataset_3, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "qqZpa1d7Abw4"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "9iNzNEPfQAxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614d78cb-1b2b-4b97-8bf0-d1def4de773b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_normal_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')"
      ],
      "metadata": {
        "id": "ZDHmSu6iXa39"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aRMrQdtuQKpQ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ],
      "metadata": {
        "id": "LTFmmsEBSxEX"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\\n\")\n"
      ],
      "metadata": {
        "id": "iBVle2_DUqna"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model = NeuralNetwork(num_classes=6, hidden_size=512).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(pre_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "iVQn8kUQvKCW"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(nn.Linear(2, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io7f10WMDeWa",
        "outputId": "46a4e248-1372-4144-f754-21b498659690"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in_features', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_parameters', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------------\")\n",
        "  train(train_dataloader_old, pre_model, loss_fn, optimizer)\n",
        "  test(test_dataloader_old, pre_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "torch.save(pre_model.state_dict(), \"model_old.pth\")"
      ],
      "metadata": {
        "id": "os4-XrPyXB3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486afe0a-bed2-418e-ab74-2a160ee2b6c8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Loss: 1.809697,    64/36000\n",
            "Loss: 0.430445,  6464/36000\n",
            "Loss: 0.432301, 12864/36000\n",
            "Loss: 0.354811, 19264/36000\n",
            "Loss: 0.199749, 25664/36000\n",
            "Loss: 0.255814, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 89.1, Avg Loss: 0.288040\n",
            "\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Loss: 0.270974,    64/36000\n",
            "Loss: 0.309610,  6464/36000\n",
            "Loss: 0.311149, 12864/36000\n",
            "Loss: 0.230740, 19264/36000\n",
            "Loss: 0.255295, 25664/36000\n",
            "Loss: 0.417961, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.4, Avg Loss: 0.271922\n",
            "\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Loss: 0.282508,    64/36000\n",
            "Loss: 0.226512,  6464/36000\n",
            "Loss: 0.216481, 12864/36000\n",
            "Loss: 0.094972, 19264/36000\n",
            "Loss: 0.256139, 25664/36000\n",
            "Loss: 0.311094, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.4, Avg Loss: 0.262038\n",
            "\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Loss: 0.212492,    64/36000\n",
            "Loss: 0.123525,  6464/36000\n",
            "Loss: 0.177934, 12864/36000\n",
            "Loss: 0.281333, 19264/36000\n",
            "Loss: 0.154010, 25664/36000\n",
            "Loss: 0.178204, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.6, Avg Loss: 0.253994\n",
            "\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Loss: 0.318273,    64/36000\n",
            "Loss: 0.126871,  6464/36000\n",
            "Loss: 0.242605, 12864/36000\n",
            "Loss: 0.126784, 19264/36000\n",
            "Loss: 0.133120, 25664/36000\n",
            "Loss: 0.317343, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 91.6, Avg Loss: 0.228820\n",
            "\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Loss: 0.252304,    64/36000\n",
            "Loss: 0.193053,  6464/36000\n",
            "Loss: 0.120241, 12864/36000\n",
            "Loss: 0.129232, 19264/36000\n",
            "Loss: 0.228211, 25664/36000\n",
            "Loss: 0.169524, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 91.4, Avg Loss: 0.231242\n",
            "\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Loss: 0.276375,    64/36000\n",
            "Loss: 0.081050,  6464/36000\n",
            "Loss: 0.244902, 12864/36000\n",
            "Loss: 0.153320, 19264/36000\n",
            "Loss: 0.239094, 25664/36000\n",
            "Loss: 0.187068, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 91.6, Avg Loss: 0.224691\n",
            "\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Loss: 0.152825,    64/36000\n",
            "Loss: 0.230383,  6464/36000\n",
            "Loss: 0.201778, 12864/36000\n",
            "Loss: 0.075192, 19264/36000\n",
            "Loss: 0.202985, 25664/36000\n",
            "Loss: 0.133677, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 92.3, Avg Loss: 0.221151\n",
            "\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Loss: 0.120449,    64/36000\n",
            "Loss: 0.111902,  6464/36000\n",
            "Loss: 0.068532, 12864/36000\n",
            "Loss: 0.185970, 19264/36000\n",
            "Loss: 0.121826, 25664/36000\n",
            "Loss: 0.335617, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 90.0, Avg Loss: 0.264430\n",
            "\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Loss: 0.229831,    64/36000\n",
            "Loss: 0.102723,  6464/36000\n",
            "Loss: 0.139064, 12864/36000\n",
            "Loss: 0.254798, 19264/36000\n",
            "Loss: 0.165092, 25664/36000\n",
            "Loss: 0.169840, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 92.3, Avg Loss: 0.215952\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LWF"
      ],
      "metadata": {
        "id": "1xnHT4FuNGVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_new = NeuralNetwork(num_classes=6, hidden_size=512).to(device)\n",
        "net_old = NeuralNetwork(num_classes=6, hidden_size=512).to(device)\n",
        "\n",
        "net_new.load_state_dict(torch.load(\"model_old.pth\"))\n",
        "net_old.load_state_dict(torch.load(\"model_old.pth\"))\n",
        "\n",
        "in_features = net_old.classifier.in_features\n",
        "out_features = net_old.classifier.out_features\n",
        "\n",
        "weight = net_old.classifier.weight.data\n",
        "bias = net_old.classifier.bias.data\n",
        "\n",
        "new_out_features = 6 + 4\n",
        "\n",
        "new_fc = nn.Linear(in_features, new_out_features)\n",
        "kaiming_normal_init(new_fc.weight)\n",
        "\n",
        "new_fc.weight.data[:out_features] = weight\n",
        "new_fc.bias.data[:out_features] = bias\n",
        "\n",
        "net_new.classifier = new_fc\n",
        "net_new = net_new.to(device)\n",
        "print(\"New head numbers: \", net_new.classifier.out_features)\n",
        "\n",
        "for param in net_old.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "ZCbEgdnwNBSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac23c42-4055-4801-f7c0-6879268c98ff"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New head numbers:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in training and testing"
      ],
      "metadata": {
        "id": "GsAe0iSPTSiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(alpha, T):\n",
        "  size = len(train_dataloader_new.dataset)\n",
        "  # We set net_new to evaluation mode to prevent it from being updated\n",
        "  # while computing the distillation loss from the old model\n",
        "  net_new.train()\n",
        "  for batch, (X, y) in enumerate(train_dataloader_new):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    outputs = net_new(X)\n",
        "    soft_y = net_old(X)\n",
        "\n",
        "    loss1 = loss_fn(outputs, y)\n",
        "\n",
        "    outputs_S = nn.functional.softmax(outputs[:, :out_features] / T, dim=1)\n",
        "    outputs_T = nn.functional.softmax(soft_y[:, :out_features] / T, dim=1)\n",
        "\n",
        "    loss2 = outputs_T.mul(-1 * torch.log(outputs_S))\n",
        "    loss2 = loss2.sum(1)\n",
        "    loss2 = loss2.mean() * T * T\n",
        "\n",
        "    loss = loss1 + alpha * loss2\n",
        "\n",
        "    loss.backward(retain_graph=True)\n",
        "    warmup_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")\n"
      ],
      "metadata": {
        "id": "Tqex4EciTVVN"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(alpha, T):\n",
        "  size = len(test_dataloader_new.dataset)\n",
        "  num_batches = len(test_dataloader_new)\n",
        "  net_new.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader_new:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      outputs = net_new(X)\n",
        "      soft_y = net_old(X)\n",
        "\n",
        "      loss1 = loss_fn(outputs, y)\n",
        "\n",
        "      outputs_S = nn.functional.softmax(outputs[:, :out_features] / T, dim=1)\n",
        "      outputs_T = nn.functional.softmax(soft_y[:, :out_features] / T, dim=1)\n",
        "\n",
        "      loss2 = outputs_T.mul(-1 * torch.log(outputs_S))\n",
        "      loss2 = loss2.sum(1)\n",
        "      loss2 = loss2.mean() * T * T\n",
        "\n",
        "      loss = loss1 * alpha + loss2 * (1 - alpha)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      correct += predicted.eq(y).sum().item()\n",
        "      # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\")\n"
      ],
      "metadata": {
        "id": "f31nqZx2WRdr"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(epoch):\n",
        "    net_new.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(eval_dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = net_new(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ],
      "metadata": {
        "id": "DbtfMR9VJW0Z"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 2\n",
        "alpha = 0.5\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net_new.parameters()), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "warmup_epochs = 5\n",
        "initial_lr = 0.0001\n",
        "final_lr = 0.01\n",
        "\n",
        "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer,\n",
        "    lr_lambda= lambda epoch: (epoch+1)/warmup_epochs if epoch < warmup_epochs else final_lr/initial_lr\n",
        ")\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch {epoch+1}: ----------------------\")\n",
        "    train(alpha, T)\n",
        "    test(alpha, T)\n",
        "    val(epoch)\n",
        "\n",
        "torch.save(net_new.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "tJwIbbRBXj-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050850d9-8bb2-48aa-8860-a5e34a394692"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: ----------------------\n",
            "Loss: 9.623205,    64/24000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 9.493693,  6464/24000\n",
            "Loss: 9.840966, 12864/24000\n",
            "Loss: 9.722804, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 2: ----------------------\n",
            "Loss: 8.980170,    64/24000\n",
            "Loss: 9.135077,  6464/24000\n",
            "Loss: 9.483232, 12864/24000\n",
            "Loss: 9.325359, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 3: ----------------------\n",
            "Loss: 9.414894,    64/24000\n",
            "Loss: 9.162929,  6464/24000\n",
            "Loss: 9.336340, 12864/24000\n",
            "Loss: 9.883241, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 4: ----------------------\n",
            "Loss: 9.227899,    64/24000\n",
            "Loss: 9.495513,  6464/24000\n",
            "Loss: 9.634324, 12864/24000\n",
            "Loss: 8.934695, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 5: ----------------------\n",
            "Loss: 9.550862,    64/24000\n",
            "Loss: 9.167891,  6464/24000\n",
            "Loss: 9.827006, 12864/24000\n",
            "Loss: 9.288913, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 6: ----------------------\n",
            "Loss: 9.893551,    64/24000\n",
            "Loss: 9.527443,  6464/24000\n",
            "Loss: 9.597201, 12864/24000\n",
            "Loss: 9.491073, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 7: ----------------------\n",
            "Loss: 10.009179,    64/24000\n",
            "Loss: 8.882726,  6464/24000\n",
            "Loss: 9.663885, 12864/24000\n",
            "Loss: 9.308301, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 8: ----------------------\n",
            "Loss: 9.089739,    64/24000\n",
            "Loss: 9.298028,  6464/24000\n",
            "Loss: 9.523304, 12864/24000\n",
            "Loss: 9.671829, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 9: ----------------------\n",
            "Loss: 9.857317,    64/24000\n",
            "Loss: 8.955281,  6464/24000\n",
            "Loss: 9.162820, 12864/24000\n",
            "Loss: 9.430060, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n",
            "Epoch 10: ----------------------\n",
            "Loss: 9.139697,    64/24000\n",
            "Loss: 9.340073,  6464/24000\n",
            "Loss: 9.878733, 12864/24000\n",
            "Loss: 9.246745, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 0.0, Avg Loss: 5.270410\n",
            "Validation Acc: 55.4\n",
            "\n"
          ]
        }
      ]
    }
  ]
}