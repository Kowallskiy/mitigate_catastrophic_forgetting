{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GzUzjT_SM22m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask1(datasets.FashionMNIST):\n",
        "  def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "    super(FashionMNISTtask1, self).__init__(root, train=train, transform=transform, target_transform=target_transform,download=download)\n",
        "    self.classes = self.classes[:6]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img, target = super(FashionMNISTtask1, self).__getitem__(index)\n",
        "    if target < 6:\n",
        "        return img, target\n",
        "    else:\n",
        "        return img, -1\n"
      ],
      "metadata": {
        "id": "iowt64kIx-pn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask2(datasets.FashionMNIST):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(FashionMNISTtask2, self).__init__(root, train=train, transform=transform, target_transform=target_transform,download=download)\n",
        "        self.classes = self.classes[6:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super(FashionMNISTtask2, self).__getitem__(index)\n",
        "        if target >= 6:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img, -1"
      ],
      "metadata": {
        "id": "X1tlnFkE3jO1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([ToTensor(), Normalize((0.5), (0.5))])\n",
        "\n",
        "train_dataset_1 = FashionMNISTtask1(root='./data1', train=True, transform=transform, download=True)\n",
        "test_dataset_1 = FashionMNISTtask1(root='./data1', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "Es33ycfn0wgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_2 = FashionMNISTtask2(root='./data2', train=True, transform=transform, download=True)\n",
        "test_dataset_2 = FashionMNISTtask2(root='./data2', train=False, transform=transform, download=True)\n",
        "test_dataset_3 = datasets.FashionMNIST(\n",
        "    root='data3',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "ItSNRkOm4vBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(datasets.FashionMNIST))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GleR8_dIbKq6",
        "outputId": "43682bdc-24e0-4155-cb25-e04d172cf408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_exists', '_check_legacy_exist', '_format_transform_repr', '_is_protocol', '_load_data', '_load_legacy_data', '_repr_indent', 'class_to_idx', 'classes', 'download', 'extra_repr', 'mirrors', 'processed_folder', 'raw_folder', 'resources', 'test_data', 'test_file', 'test_labels', 'train_data', 'train_labels', 'training_file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_filtered_old = [data for data in train_dataset_1 if data[1] != -1]\n",
        "test_dataset_filtered_old = [data for data in test_dataset_1 if data[1] != -1]\n",
        "\n",
        "train_dataset_filtered_new = [data for data in train_dataset_2 if data[1] != -1]\n",
        "test_dataset_filtered_new = [data for data in test_dataset_2 if data[1] != -1]"
      ],
      "metadata": {
        "id": "PWyzJURmqBjQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_old = DataLoader(train_dataset_filtered_old, batch_size=64, shuffle=True)\n",
        "test_dataloader_old = DataLoader(test_dataset_filtered_old, batch_size=256, shuffle=False)\n",
        "\n",
        "for X, y in train_dataloader_old:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"SHape of y: {y.shape}, dtype: {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "fl2AkOV8POXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5850d579-a80f-494c-cd46-a0d5f5fcdf11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "SHape of y: torch.Size([64]), dtype: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_old)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOY9PckQp-O3",
        "outputId": "78fcc021-b281-4e81-b9b7-3c7fbf9842a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "563"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_new = DataLoader(train_dataset_filtered_new, batch_size=64, shuffle=True)\n",
        "test_dataloader_new = DataLoader(test_dataset_filtered_new, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "HKo3B_gW5ASM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataloader = DataLoader(test_dataset_3, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "qqZpa1d7Abw4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "9iNzNEPfQAxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73c065c-9504-4001-c7c4-c3ffb9e7d20a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_normal_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')"
      ],
      "metadata": {
        "id": "ZDHmSu6iXa39"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aRMrQdtuQKpQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32*14*14, hidden_size)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "kpdF76WoblBF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ],
      "metadata": {
        "id": "LTFmmsEBSxEX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\\n\")\n"
      ],
      "metadata": {
        "id": "iBVle2_DUqna"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model = CNN(num_classes=6, hidden_size=512).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(pre_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "iVQn8kUQvKCW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model.classifier.out_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcvQ9lA6qv4c",
        "outputId": "f85405d3-6a2c-4657-b478-e111d0b77f28"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(nn.Linear(2, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io7f10WMDeWa",
        "outputId": "46a4e248-1372-4144-f754-21b498659690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in_features', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_parameters', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------------\")\n",
        "  train(train_dataloader_old, pre_model, loss_fn, optimizer)\n",
        "  test(test_dataloader_old, pre_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "torch.save(pre_model.state_dict(), \"model_old.pth\")"
      ],
      "metadata": {
        "id": "os4-XrPyXB3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18803b0-867d-4122-8f59-83407d8130d8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Loss: 1.861183,    64/36000\n",
            "Loss: 0.245489,  6464/36000\n",
            "Loss: 0.245155, 12864/36000\n",
            "Loss: 0.237711, 19264/36000\n",
            "Loss: 0.493534, 25664/36000\n",
            "Loss: 0.243511, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 92.7, Avg Loss: 0.200218\n",
            "\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Loss: 0.229843,    64/36000\n",
            "Loss: 0.207944,  6464/36000\n",
            "Loss: 0.327864, 12864/36000\n",
            "Loss: 0.090289, 19264/36000\n",
            "Loss: 0.120155, 25664/36000\n",
            "Loss: 0.113813, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 93.2, Avg Loss: 0.185565\n",
            "\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Loss: 0.174954,    64/36000\n",
            "Loss: 0.145165,  6464/36000\n",
            "Loss: 0.094031, 12864/36000\n",
            "Loss: 0.158868, 19264/36000\n",
            "Loss: 0.330290, 25664/36000\n",
            "Loss: 0.136209, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 93.9, Avg Loss: 0.168222\n",
            "\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Loss: 0.084361,    64/36000\n",
            "Loss: 0.126217,  6464/36000\n",
            "Loss: 0.097801, 12864/36000\n",
            "Loss: 0.162215, 19264/36000\n",
            "Loss: 0.070744, 25664/36000\n",
            "Loss: 0.077497, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.8, Avg Loss: 0.148748\n",
            "\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Loss: 0.166153,    64/36000\n",
            "Loss: 0.092549,  6464/36000\n",
            "Loss: 0.125631, 12864/36000\n",
            "Loss: 0.108681, 19264/36000\n",
            "Loss: 0.104478, 25664/36000\n",
            "Loss: 0.028887, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.2, Avg Loss: 0.168604\n",
            "\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Loss: 0.058729,    64/36000\n",
            "Loss: 0.087259,  6464/36000\n",
            "Loss: 0.107745, 12864/36000\n",
            "Loss: 0.124980, 19264/36000\n",
            "Loss: 0.022945, 25664/36000\n",
            "Loss: 0.051073, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.7, Avg Loss: 0.150059\n",
            "\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Loss: 0.051192,    64/36000\n",
            "Loss: 0.035968,  6464/36000\n",
            "Loss: 0.124581, 12864/36000\n",
            "Loss: 0.171034, 19264/36000\n",
            "Loss: 0.073099, 25664/36000\n",
            "Loss: 0.074108, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.8, Avg Loss: 0.155725\n",
            "\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Loss: 0.057202,    64/36000\n",
            "Loss: 0.070872,  6464/36000\n",
            "Loss: 0.036444, 12864/36000\n",
            "Loss: 0.019167, 19264/36000\n",
            "Loss: 0.069328, 25664/36000\n",
            "Loss: 0.099069, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.7, Avg Loss: 0.150453\n",
            "\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Loss: 0.070229,    64/36000\n",
            "Loss: 0.099892,  6464/36000\n",
            "Loss: 0.036128, 12864/36000\n",
            "Loss: 0.041774, 19264/36000\n",
            "Loss: 0.061464, 25664/36000\n",
            "Loss: 0.057106, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.8, Avg Loss: 0.163766\n",
            "\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Loss: 0.015695,    64/36000\n",
            "Loss: 0.019779,  6464/36000\n",
            "Loss: 0.027503, 12864/36000\n",
            "Loss: 0.091703, 19264/36000\n",
            "Loss: 0.048307, 25664/36000\n",
            "Loss: 0.037426, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 95.0, Avg Loss: 0.173891\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LWF"
      ],
      "metadata": {
        "id": "1xnHT4FuNGVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_new = CNN(num_classes=6, hidden_size=512).to(device)\n",
        "net_old = CNN(num_classes=6, hidden_size=512).to(device)\n",
        "\n",
        "net_new.load_state_dict(torch.load(\"model_old.pth\"))\n",
        "net_old.load_state_dict(torch.load(\"model_old.pth\"))\n",
        "\n",
        "in_features = net_old.classifier.in_features\n",
        "out_features = net_old.classifier.out_features\n",
        "\n",
        "weight = net_old.classifier.weight.data\n",
        "bias = net_old.classifier.bias.data\n",
        "\n",
        "new_out_features = 10\n",
        "\n",
        "new_fc = nn.Linear(in_features, new_out_features)\n",
        "kaiming_normal_init(new_fc.weight)\n",
        "\n",
        "new_fc.weight.data[:out_features] = weight\n",
        "new_fc.bias.data[:out_features] = bias\n",
        "\n",
        "net_new.classifier = new_fc\n",
        "net_new = net_new.to(device)\n",
        "print(\"New head numbers: \", net_new.classifier.out_features)\n",
        "\n",
        "# for param in net_old.parameters():\n",
        "#   param.requires_grad = False"
      ],
      "metadata": {
        "id": "ZCbEgdnwNBSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db34c23c-6eea-4f84-d4e5-c90654eb2d81"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New head numbers:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in training and testing"
      ],
      "metadata": {
        "id": "GsAe0iSPTSiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(alpha, T):\n",
        "  size = len(train_dataloader_new.dataset)\n",
        "  # We set net_new to evaluation mode to prevent it from being updated\n",
        "  # while computing the distillation loss from the old model\n",
        "  net_new.train()\n",
        "  for batch, (X, y) in enumerate(train_dataloader_new):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    outputs = net_new(X)\n",
        "    soft_y = net_old(X)\n",
        "\n",
        "    loss1 = loss_fn(outputs, y)\n",
        "\n",
        "    outputs_S = nn.functional.softmax(outputs[:, :out_features] / T, dim=1)\n",
        "    outputs_T = nn.functional.softmax(soft_y[:, :out_features] / T, dim=1)\n",
        "\n",
        "    loss2 = outputs_T.mul(-1 * torch.log(outputs_S))\n",
        "    loss2 = loss2.sum(1)\n",
        "    loss2 = loss2.mean() * T * T\n",
        "\n",
        "    loss = loss1 + alpha * loss2\n",
        "\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")\n"
      ],
      "metadata": {
        "id": "Tqex4EciTVVN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(alpha, T):\n",
        "  size = len(test_dataloader_new.dataset)\n",
        "  num_batches = len(test_dataloader_new)\n",
        "  net_new.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader_new:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      outputs = net_new(X)\n",
        "      soft_y = net_old(X)\n",
        "\n",
        "      loss1 = loss_fn(outputs, y)\n",
        "\n",
        "      outputs_S = nn.functional.softmax(outputs[:, :out_features] / T, dim=1)\n",
        "      outputs_T = nn.functional.softmax(soft_y[:, :out_features] / T, dim=1)\n",
        "\n",
        "      loss2 = outputs_T.mul(-1 * torch.log(outputs_S))\n",
        "      loss2 = loss2.sum(1)\n",
        "      loss2 = loss2.mean() * T * T\n",
        "\n",
        "      loss = loss1 * alpha + loss2 * (1 - alpha)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      correct += predicted.eq(y).sum().item()\n",
        "      # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\")\n"
      ],
      "metadata": {
        "id": "f31nqZx2WRdr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(epoch):\n",
        "    net_new.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(eval_dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = net_new(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ],
      "metadata": {
        "id": "DbtfMR9VJW0Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 2\n",
        "alpha = 0.5\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net_new.parameters()), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# warmup_epochs = 5\n",
        "# initial_lr = 0.0001\n",
        "# final_lr = 0.01\n",
        "\n",
        "# warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "#     optimizer,\n",
        "#     lr_lambda= lambda epoch: (epoch+1)/warmup_epochs if epoch < warmup_epochs else final_lr/initial_lr\n",
        "# )\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch {epoch+1}: ----------------------\")\n",
        "    train(alpha, T)\n",
        "    test(alpha, T)\n",
        "    val(epoch)\n",
        "\n",
        "torch.save(net_new.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "tJwIbbRBXj-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89593b4-ae99-4bbc-f9de-a997e9d084f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: ----------------------\n",
            "Loss: 9.069376,    64/24000\n",
            "Loss: 1.284970,  6464/24000\n",
            "Loss: 1.000128, 12864/24000\n",
            "Loss: 1.267044, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 97.7, Avg Loss: 1.158224\n",
            "Validation Acc: 42.59\n",
            "\n",
            "Epoch 2: ----------------------\n",
            "Loss: 1.256586,    64/24000\n",
            "Loss: 1.049819,  6464/24000\n",
            "Loss: 1.073755, 12864/24000\n",
            "Loss: 1.071640, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 97.7, Avg Loss: 1.146203\n",
            "Validation Acc: 43.12\n",
            "\n",
            "Epoch 3: ----------------------\n",
            "Loss: 1.293636,    64/24000\n",
            "Loss: 1.175289,  6464/24000\n",
            "Loss: 1.191710, 12864/24000\n",
            "Loss: 1.160707, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.0, Avg Loss: 1.139596\n",
            "Validation Acc: 43.32\n",
            "\n",
            "Epoch 4: ----------------------\n",
            "Loss: 1.071060,    64/24000\n",
            "Loss: 1.093132,  6464/24000\n",
            "Loss: 1.066819, 12864/24000\n",
            "Loss: 1.128390, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.1, Avg Loss: 1.137410\n",
            "Validation Acc: 41.48\n",
            "\n",
            "Epoch 5: ----------------------\n",
            "Loss: 1.036565,    64/24000\n",
            "Loss: 1.073287,  6464/24000\n",
            "Loss: 1.063673, 12864/24000\n",
            "Loss: 1.331667, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.0, Avg Loss: 1.141631\n",
            "Validation Acc: 41.71\n",
            "\n",
            "Epoch 6: ----------------------\n",
            "Loss: 1.154143,    64/24000\n",
            "Loss: 0.992128,  6464/24000\n",
            "Loss: 1.047446, 12864/24000\n",
            "Loss: 1.234226, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.1, Avg Loss: 1.134738\n",
            "Validation Acc: 40.77\n",
            "\n",
            "Epoch 7: ----------------------\n",
            "Loss: 1.314638,    64/24000\n",
            "Loss: 1.048289,  6464/24000\n",
            "Loss: 1.051072, 12864/24000\n",
            "Loss: 1.151821, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.2, Avg Loss: 1.133854\n",
            "Validation Acc: 40.93\n",
            "\n",
            "Epoch 8: ----------------------\n",
            "Loss: 1.140121,    64/24000\n",
            "Loss: 1.177710,  6464/24000\n",
            "Loss: 1.327350, 12864/24000\n",
            "Loss: 1.148504, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.2, Avg Loss: 1.135264\n",
            "Validation Acc: 39.89\n",
            "\n",
            "Epoch 9: ----------------------\n",
            "Loss: 1.155468,    64/24000\n",
            "Loss: 1.081529,  6464/24000\n",
            "Loss: 1.120250, 12864/24000\n",
            "Loss: 1.319945, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.2, Avg Loss: 1.135188\n",
            "Validation Acc: 39.95\n",
            "\n",
            "Epoch 10: ----------------------\n",
            "Loss: 1.124808,    64/24000\n",
            "Loss: 0.930841,  6464/24000\n",
            "Loss: 1.088142, 12864/24000\n",
            "Loss: 1.226367, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 98.4, Avg Loss: 1.136654\n",
            "Validation Acc: 39.88\n",
            "\n"
          ]
        }
      ]
    }
  ]
}