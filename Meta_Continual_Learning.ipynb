{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Continual Learning"
      ],
      "metadata": {
        "id": "-CeB2Hbzk2zP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhyC2YgHvKV5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "kZahPe6rlTUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to split the FashionMNIST dataset into tasks\n",
        "def create_fashion_mnist_tasks(root_dir, num_tasks=5, transform=None):\n",
        "    # Load the entire FashionMNIST dataset\n",
        "    full_dataset = FashionMNIST(root=root_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    # Determine the number of classes per task\n",
        "    classes_per_task = len(full_dataset.classes) // num_tasks\n",
        "\n",
        "    # Create tasks by splitting the dataset\n",
        "    tasks = []\n",
        "    for task_idx in range(num_tasks):\n",
        "        # Calculate class indices for the current task\n",
        "        class_start = task_idx * classes_per_task\n",
        "        class_end = class_start + classes_per_task\n",
        "        task_classes = list(range(class_start, class_end))\n",
        "\n",
        "        # Find indices of images belonging to the current task's classes\n",
        "        task_indices = [i for i, (_, label) in enumerate(full_dataset) if label in task_classes]\n",
        "\n",
        "        # Create a Subset for the current task\n",
        "        task_dataset = Subset(full_dataset, task_indices)\n",
        "        tasks.append(task_dataset)\n",
        "\n",
        "    return tasks\n",
        "\n",
        "# Create tasks\n",
        "root_dir = './data'\n",
        "tasks_evaluation = create_fashion_mnist_tasks(root_dir, num_tasks=5, transform=transform)"
      ],
      "metadata": {
        "id": "0mBjqCdPTpb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5i42NUAxXeI",
        "outputId": "a832c941-8386-4f19-f254-49ab9217c852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Architecture"
      ],
      "metadata": {
        "id": "JwrUu7mFlaUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv1(x), 2))\n",
        "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SOEo4PZn346v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "gWnicsvNlvkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maml_update(model, optimizer, loss_fn, data_loader, steps=3, alpha=0.0001):\n",
        "    # Create a copy of the model's initial state\n",
        "    initial_state = {name: param.clone() for name, param in model.named_parameters()}\n",
        "\n",
        "    # Task-specific update\n",
        "    for step in range(steps):\n",
        "        for inputs, labels in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # labels = torch.tensor(labels)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Apply the update to the model (simulating gradient descent)\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    param -= alpha * param.grad\n",
        "\n",
        "    # Update the meta-model parameters\n",
        "    meta_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    meta_loss = 0\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs = model(inputs)\n",
        "        meta_loss += loss_fn(outputs, labels)\n",
        "\n",
        "    meta_optimizer.zero_grad()\n",
        "    meta_loss.backward()\n",
        "    meta_optimizer.step()\n",
        "\n",
        "    # Restore the model to its initial state\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            param.copy_(initial_state[name])"
      ],
      "metadata": {
        "id": "JqCFTcQ538aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_dataloaders = [DataLoader(task, batch_size=64, shuffle=True) for task in tasks]\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "iMPvKzNT49UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_id, task_loader in enumerate(task_dataloaders):\n",
        "    print(f\"Training on task {task_id}\")\n",
        "    maml_update(model, optimizer, nn.CrossEntropyLoss(), task_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaINMYwR4KgD",
        "outputId": "8bc5bedc-77a2-46b4-e426-5fa50692f325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on task 0\n",
            "Training on task 1\n",
            "Training on task 2\n",
            "Training on task 3\n",
            "Training on task 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, task_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in task_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f\"Task Accuracy: {accuracy}\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "90-UjFSy4hTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_task_dataloaders = [DataLoader(task, batch_size=64, shuffle=True) for task in tasks_evaluation]"
      ],
      "metadata": {
        "id": "tx8xfeusT87L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on each task and compute average accuracy\n",
        "total_accuracy = 0\n",
        "for task_id, task_loader in enumerate(eval_task_dataloaders):\n",
        "    print(f\"Evaluation on task {task_id}\")\n",
        "    accuracy = evaluate_model(model, task_loader)\n",
        "    total_accuracy += accuracy\n",
        "\n",
        "num_tasks = len(eval_task_dataloaders)\n",
        "print(f\"Average Accuracy: {total_accuracy / num_tasks}\")"
      ],
      "metadata": {
        "id": "_yIrkofS5FxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce92449-3f9d-4b8c-c689-31cd8cd98a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on task 0\n",
            "Task Accuracy: 0.303\n",
            "Evaluation on task 1\n",
            "Task Accuracy: 0.315\n",
            "Evaluation on task 2\n",
            "Task Accuracy: 0.0\n",
            "Evaluation on task 3\n",
            "Task Accuracy: 0.0\n",
            "Evaluation on task 4\n",
            "Task Accuracy: 0.0\n",
            "Average Accuracy: 0.1236\n"
          ]
        }
      ]
    }
  ]
}