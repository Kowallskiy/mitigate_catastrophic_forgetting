{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Without Forgetting"
      ],
      "metadata": {
        "id": "8IV1z-j2B27J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "GzUzjT_SM22m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "ROjAxg-XB8tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask1(datasets.FashionMNIST):\n",
        "  def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "    super(FashionMNISTtask1, self).__init__(root, train=train, transform=transform, target_transform=target_transform,download=download)\n",
        "    self.classes = self.classes[:6]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img, target = super(FashionMNISTtask1, self).__getitem__(index)\n",
        "    if target < 6:\n",
        "        return img, target\n",
        "    else:\n",
        "        return img, -1\n"
      ],
      "metadata": {
        "id": "iowt64kIx-pn"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTtask2(datasets.FashionMNIST):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(FashionMNISTtask2, self).__init__(root, train=train, transform=transform, target_transform=target_transform,download=download)\n",
        "        self.classes = self.classes[6:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super(FashionMNISTtask2, self).__getitem__(index)\n",
        "        if target >= 6:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img, -1"
      ],
      "metadata": {
        "id": "X1tlnFkE3jO1"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([ToTensor(), Normalize((0.5), (0.5))])\n",
        "\n",
        "train_dataset_task1 = FashionMNISTtask1(root='./data1', train=True, transform=transform, download=True)\n",
        "test_dataset_task1 = FashionMNISTtask1(root='./data1', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "Es33ycfn0wgr"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_task2 = FashionMNISTtask2(root='./data2', train=True, transform=transform, download=True)\n",
        "test_dataset_task2 = FashionMNISTtask2(root='./data2', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "ItSNRkOm4vBs"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_filtered_task1 = [data for data in train_dataset_task1 if data[1] != -1]\n",
        "test_dataset_filtered_task1 = [data for data in test_dataset_task1 if data[1] != -1]\n",
        "\n",
        "train_dataset_filtered_task2 = [data for data in train_dataset_task2 if data[1] != -1]\n",
        "test_dataset_filtered_task2 = [data for data in test_dataset_task2 if data[1] != -1]"
      ],
      "metadata": {
        "id": "PWyzJURmqBjQ"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_task1 = DataLoader(train_dataset_filtered_task1, batch_size=64, shuffle=True)\n",
        "test_dataloader_task1 = DataLoader(test_dataset_filtered_task1, batch_size=256, shuffle=False)\n",
        "\n",
        "for X, y in train_dataloader_task1:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"SHape of y: {y.shape}, dtype: {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "fl2AkOV8POXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e32246-2a89-4bf5-e986-37a09484fa6d"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "SHape of y: torch.Size([64]), dtype: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_task1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOY9PckQp-O3",
        "outputId": "c24ec2d2-7b66-425d-9303-30d28d7ccb2a"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "563"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_task2 = DataLoader(train_dataset_filtered_task2, batch_size=64, shuffle=True)\n",
        "test_dataloader_task2 = DataLoader(test_dataset_filtered_task2, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "HKo3B_gW5ASM"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "9iNzNEPfQAxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a27524-68ac-4599-862e-840aa16158b3"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN Architecture"
      ],
      "metadata": {
        "id": "j2yvuHpICA3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_normal_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')"
      ],
      "metadata": {
        "id": "ZDHmSu6iXa39"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aRMrQdtuQKpQ"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_size=512):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32*14*14, hidden_size)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "kpdF76WoblBF"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ZaG5maUTCMf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1) * len(X)\n",
        "      print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")"
      ],
      "metadata": {
        "id": "LTFmmsEBSxEX"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\\n\")\n"
      ],
      "metadata": {
        "id": "iBVle2_DUqna"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_task1 = CNN(num_classes=6, hidden_size=512).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_task1.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "iVQn8kUQvKCW"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_task1.classifier.out_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcvQ9lA6qv4c",
        "outputId": "1ab0c051-e1d2-4855-b7fe-82faef6b1215"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------------\")\n",
        "  train(train_dataloader_task1, model_task1, loss_fn, optimizer)\n",
        "  test(test_dataloader_task1, model_task1, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "torch.save(model_task1.state_dict(), \"model_old.pth\")"
      ],
      "metadata": {
        "id": "os4-XrPyXB3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20052b51-e89f-4998-d7a8-2f560860bae2"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Loss: 2.021631,    64/36000\n",
            "Loss: 0.401100,  6464/36000\n",
            "Loss: 0.176412, 12864/36000\n",
            "Loss: 0.144074, 19264/36000\n",
            "Loss: 0.289997, 25664/36000\n",
            "Loss: 0.307687, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 92.0, Avg Loss: 0.221119\n",
            "\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Loss: 0.107748,    64/36000\n",
            "Loss: 0.194388,  6464/36000\n",
            "Loss: 0.190120, 12864/36000\n",
            "Loss: 0.136756, 19264/36000\n",
            "Loss: 0.106814, 25664/36000\n",
            "Loss: 0.177587, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 93.0, Avg Loss: 0.195162\n",
            "\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Loss: 0.105263,    64/36000\n",
            "Loss: 0.102043,  6464/36000\n",
            "Loss: 0.215910, 12864/36000\n",
            "Loss: 0.161934, 19264/36000\n",
            "Loss: 0.221108, 25664/36000\n",
            "Loss: 0.174191, 32064/36000\n",
            "Test Error: \n",
            " Accuracy: 94.1, Avg Loss: 0.163600\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ModifiedCNN has an additional linear hidden layer and new classifier with all 10 classes"
      ],
      "metadata": {
        "id": "1MzHinw3CbJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedCNN(CNN):\n",
        "    def __init__(self, num_classes, hidden_size, new_hidden_size):\n",
        "        super(ModifiedCNN, self).__init__(num_classes=num_classes, hidden_size=hidden_size)\n",
        "        self.new_fc = nn.Linear(hidden_size, new_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.classifier = nn.Linear(new_hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward_features(x)\n",
        "        x = self.new_fc(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZFcGPrF58n9y"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "model = ModifiedCNN(num_classes=10, hidden_size=512, new_hidden_size=512)"
      ],
      "metadata": {
        "id": "2ECtQDuyAuHh"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the architecture\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKKuAPSZA3mZ",
        "outputId": "2344fdc7-82ca-44a9-9711-584e7ea181bd"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModifiedCNN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=6272, out_features=512, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (new_fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LWF"
      ],
      "metadata": {
        "id": "1xnHT4FuNGVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models with old NN architecture\n",
        "model_task2 = CNN(num_classes=6, hidden_size=512).to(device)\n",
        "model_task1 = CNN(num_classes=6, hidden_size=512).to(device)\n",
        "\n",
        "# Load the wights from trained on task 1 model\n",
        "model_task2.load_state_dict(torch.load(\"model_old.pth\"))\n",
        "model_task1.load_state_dict(torch.load(\"model_old.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfxfQaRHCTIn",
        "outputId": "c3fb6a8a-e29a-4d70-bc28-8c756aae7b7a"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n, p in model_task1.named_parameters():\n",
        "    print(f\"{n}, {p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oyNGp3gFhA9",
        "outputId": "f450b621-284a-4085-9de6-b8130ed80f1e"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight, Parameter containing:\n",
            "tensor([[[[-0.1057,  0.7276, -0.1905],\n",
            "          [-0.1395, -0.1583, -0.6713],\n",
            "          [ 0.6999, -0.0707,  0.0365]]],\n",
            "\n",
            "\n",
            "        [[[-0.1791, -0.0330,  0.0873],\n",
            "          [ 0.0945, -0.5090, -0.1908],\n",
            "          [-0.0037,  0.7521, -1.1215]]],\n",
            "\n",
            "\n",
            "        [[[-0.3484,  0.2708, -0.2843],\n",
            "          [-0.3917, -0.2529,  0.6720],\n",
            "          [-0.4581,  0.4667,  0.3419]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5713,  0.7453, -0.1456],\n",
            "          [-1.1476, -0.1132,  1.0575],\n",
            "          [ 0.5034, -0.4015,  0.7360]]],\n",
            "\n",
            "\n",
            "        [[[-0.0569, -0.4772, -0.5208],\n",
            "          [ 0.2953, -0.1534, -0.3075],\n",
            "          [ 1.3770, -0.2166,  0.1164]]],\n",
            "\n",
            "\n",
            "        [[[-0.6752,  0.2039, -0.1308],\n",
            "          [ 0.4807,  0.4267, -0.2956],\n",
            "          [-0.5032, -0.4513,  0.0596]]],\n",
            "\n",
            "\n",
            "        [[[-0.0339,  0.3412, -0.4523],\n",
            "          [ 0.4974, -0.2907,  0.3372],\n",
            "          [-0.2468,  0.0887, -0.4683]]],\n",
            "\n",
            "\n",
            "        [[[-0.0616, -0.2475, -0.3191],\n",
            "          [-0.3377,  0.5390,  0.4910],\n",
            "          [ 0.2113,  0.8815, -0.1580]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5852, -0.9824, -1.1129],\n",
            "          [ 0.3130, -0.1185, -0.2565],\n",
            "          [-0.0068,  0.4321,  0.2308]]],\n",
            "\n",
            "\n",
            "        [[[-0.3267, -0.0376, -0.2667],\n",
            "          [-0.2720,  0.4811, -0.3584],\n",
            "          [-0.4194,  0.0460,  0.5017]]],\n",
            "\n",
            "\n",
            "        [[[-0.0811, -0.9168,  0.2038],\n",
            "          [-0.2138, -0.3907,  0.1302],\n",
            "          [ 1.1307,  0.5662, -0.5185]]],\n",
            "\n",
            "\n",
            "        [[[-0.1050,  0.5058,  0.7637],\n",
            "          [ 0.0921, -0.8804,  0.0076],\n",
            "          [-0.7233,  0.1418, -0.5117]]],\n",
            "\n",
            "\n",
            "        [[[-0.4833,  0.8893, -0.2594],\n",
            "          [-0.3336, -0.5197,  0.0482],\n",
            "          [-0.1788,  0.3641,  0.4398]]],\n",
            "\n",
            "\n",
            "        [[[-0.2626,  0.1633, -0.2619],\n",
            "          [ 0.1580, -0.3305, -0.5196],\n",
            "          [-0.2975,  0.2313,  0.0608]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0255,  0.0714, -1.1129],\n",
            "          [ 0.0287, -0.2726, -0.3061],\n",
            "          [ 0.8843, -0.0079, -0.1277]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5051,  0.3856, -0.0083],\n",
            "          [-0.4020, -0.3214,  0.0216],\n",
            "          [-0.3466, -0.2337, -0.3959]]]], requires_grad=True)\n",
            "conv1.bias, Parameter containing:\n",
            "tensor([ 0.0715, -0.2574,  0.1850, -0.0819,  0.2385,  0.0189,  0.1050, -0.0434,\n",
            "        -0.2011, -0.0673,  0.3877, -0.4015,  0.3079, -0.0690, -0.1107, -0.5207],\n",
            "       requires_grad=True)\n",
            "conv2.weight, Parameter containing:\n",
            "tensor([[[[ 0.0432, -0.1881,  0.0442],\n",
            "          [ 0.0743,  0.0252, -0.0651],\n",
            "          [-0.1138,  0.1286, -0.0268]],\n",
            "\n",
            "         [[ 0.1832,  0.0445,  0.0612],\n",
            "          [-0.0309, -0.0293, -0.1424],\n",
            "          [-0.0198,  0.2018, -0.0572]],\n",
            "\n",
            "         [[-0.0493,  0.1363,  0.0905],\n",
            "          [-0.0375, -0.0988,  0.0167],\n",
            "          [-0.1180,  0.0529, -0.0517]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0028,  0.0985, -0.0785],\n",
            "          [ 0.0747, -0.2070, -0.0912],\n",
            "          [ 0.0087,  0.1930,  0.0656]],\n",
            "\n",
            "         [[ 0.1246, -0.0165, -0.1043],\n",
            "          [-0.1790,  0.0040, -0.0256],\n",
            "          [ 0.0872,  0.1059,  0.1136]],\n",
            "\n",
            "         [[ 0.1029, -0.1056,  0.1187],\n",
            "          [-0.0567,  0.0516,  0.0526],\n",
            "          [-0.0668, -0.1053, -0.1515]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1064, -0.0780,  0.0043],\n",
            "          [-0.0675,  0.0639, -0.0014],\n",
            "          [-0.0698,  0.1064, -0.0794]],\n",
            "\n",
            "         [[-0.1833,  0.0707,  0.0069],\n",
            "          [-0.2903, -0.0424,  0.0489],\n",
            "          [-0.1096,  0.0777, -0.0889]],\n",
            "\n",
            "         [[-0.0956,  0.2639,  0.0565],\n",
            "          [-0.1408,  0.0982,  0.0337],\n",
            "          [-0.0542,  0.0893, -0.0432]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1780,  0.1133,  0.1826],\n",
            "          [ 0.2187, -0.1287, -0.0333],\n",
            "          [-0.0479, -0.0626, -0.0684]],\n",
            "\n",
            "         [[-0.0769,  0.0184,  0.1107],\n",
            "          [-0.0769,  0.0455, -0.1360],\n",
            "          [-0.1485, -0.1275, -0.1147]],\n",
            "\n",
            "         [[ 0.1368, -0.0254, -0.1570],\n",
            "          [ 0.0326,  0.0440, -0.0986],\n",
            "          [ 0.0186, -0.0716, -0.0562]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0424, -0.2934, -0.0546],\n",
            "          [ 0.0170, -0.1669,  0.0655],\n",
            "          [-0.0921, -0.3579, -0.0235]],\n",
            "\n",
            "         [[ 0.0126, -0.0100,  0.2527],\n",
            "          [ 0.0641,  0.1312, -0.0753],\n",
            "          [ 0.0501, -0.0349,  0.0983]],\n",
            "\n",
            "         [[ 0.0235, -0.0751, -0.0114],\n",
            "          [ 0.0904, -0.0478, -0.0872],\n",
            "          [ 0.0069,  0.0594, -0.2249]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0407, -0.0595, -0.0146],\n",
            "          [ 0.0925, -0.0911, -0.1197],\n",
            "          [-0.1177, -0.1637, -0.0472]],\n",
            "\n",
            "         [[-0.1056, -0.0274,  0.0813],\n",
            "          [ 0.1783, -0.0872, -0.0424],\n",
            "          [-0.0676, -0.0051,  0.1083]],\n",
            "\n",
            "         [[ 0.0755, -0.0044, -0.0163],\n",
            "          [ 0.0548,  0.0645, -0.0242],\n",
            "          [ 0.1159, -0.0710, -0.1881]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0457,  0.0187, -0.0976],\n",
            "          [-0.1666,  0.0283, -0.1140],\n",
            "          [-0.0622, -0.0983,  0.0596]],\n",
            "\n",
            "         [[-0.0591,  0.1756, -0.1401],\n",
            "          [ 0.0359,  0.0523, -0.1487],\n",
            "          [-0.0341,  0.1002,  0.0749]],\n",
            "\n",
            "         [[ 0.1558,  0.1084,  0.1496],\n",
            "          [-0.1279,  0.0873, -0.1678],\n",
            "          [-0.0270, -0.1377, -0.0493]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0087,  0.0094,  0.1415],\n",
            "          [-0.0726,  0.1216, -0.0441],\n",
            "          [ 0.0999, -0.0339,  0.2416]],\n",
            "\n",
            "         [[-0.0113,  0.0526,  0.0207],\n",
            "          [-0.0493, -0.1486,  0.2034],\n",
            "          [ 0.0390,  0.1405, -0.2219]],\n",
            "\n",
            "         [[-0.1489,  0.0762, -0.1665],\n",
            "          [-0.1944, -0.0235,  0.1022],\n",
            "          [ 0.0022,  0.0119,  0.1236]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3077, -0.0420, -0.0479],\n",
            "          [-0.0142,  0.0525, -0.0173],\n",
            "          [-0.0251, -0.1236, -0.1431]],\n",
            "\n",
            "         [[-0.0206,  0.0169,  0.0364],\n",
            "          [-0.2457, -0.0196, -0.0827],\n",
            "          [-0.2283,  0.1292, -0.1618]],\n",
            "\n",
            "         [[-0.1468,  0.0102,  0.0700],\n",
            "          [-0.0990, -0.1027,  0.1352],\n",
            "          [ 0.0989, -0.0941,  0.0375]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3321, -0.2501,  0.0065],\n",
            "          [-0.1044,  0.1748,  0.0270],\n",
            "          [ 0.0059, -0.0048, -0.0598]],\n",
            "\n",
            "         [[-0.0324, -0.1559, -0.0146],\n",
            "          [-0.0559, -0.1326, -0.3170],\n",
            "          [ 0.1084, -0.1009, -0.1625]],\n",
            "\n",
            "         [[-0.0568, -0.0752, -0.0127],\n",
            "          [-0.1102, -0.0030, -0.0496],\n",
            "          [ 0.0564, -0.0436, -0.0827]]],\n",
            "\n",
            "\n",
            "        [[[-0.0343, -0.1033,  0.1041],\n",
            "          [ 0.2344,  0.0018, -0.2402],\n",
            "          [ 0.1031,  0.0029, -0.0521]],\n",
            "\n",
            "         [[-0.0740, -0.0387, -0.1974],\n",
            "          [ 0.2292,  0.1001, -0.2874],\n",
            "          [ 0.0639,  0.1087, -0.1019]],\n",
            "\n",
            "         [[-0.0091, -0.1733, -0.0391],\n",
            "          [ 0.0900,  0.1020,  0.0237],\n",
            "          [ 0.1134,  0.0669,  0.0736]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1309,  0.1414,  0.2268],\n",
            "          [ 0.0548,  0.0440,  0.1264],\n",
            "          [-0.0478, -0.0342, -0.0362]],\n",
            "\n",
            "         [[-0.0383,  0.1656,  0.0985],\n",
            "          [-0.1942, -0.2181,  0.2656],\n",
            "          [-0.0279, -0.0955, -0.1692]],\n",
            "\n",
            "         [[-0.0108,  0.0767, -0.0337],\n",
            "          [-0.0108,  0.1427, -0.0338],\n",
            "          [-0.0016,  0.0235,  0.3846]]]], requires_grad=True)\n",
            "conv2.bias, Parameter containing:\n",
            "tensor([ 0.0379,  0.0353, -0.0176,  0.0319,  0.0245, -0.0720,  0.0766, -0.0135,\n",
            "         0.1093,  0.0035,  0.0052,  0.0477,  0.0718, -0.1344,  0.1031, -0.0862,\n",
            "         0.0627,  0.0796, -0.0597, -0.2733,  0.1012,  0.0237, -0.0063, -0.0316,\n",
            "        -0.0257,  0.0461, -0.0974, -0.0071,  0.1198,  0.0942, -0.1338,  0.0678],\n",
            "       requires_grad=True)\n",
            "fc1.weight, Parameter containing:\n",
            "tensor([[-0.0032,  0.0111, -0.0150,  ..., -0.0033, -0.0172, -0.0036],\n",
            "        [-0.0055,  0.0045,  0.0029,  ..., -0.0171,  0.0034, -0.0101],\n",
            "        [ 0.0019,  0.0001, -0.0068,  ...,  0.0021, -0.0060, -0.0066],\n",
            "        ...,\n",
            "        [ 0.0105,  0.0040,  0.0181,  ...,  0.0073,  0.0250, -0.0231],\n",
            "        [ 0.0175,  0.0050, -0.0099,  ..., -0.0016, -0.0184, -0.0188],\n",
            "        [-0.0113, -0.0038, -0.0187,  ..., -0.0263,  0.0019, -0.0052]],\n",
            "       requires_grad=True)\n",
            "fc1.bias, Parameter containing:\n",
            "tensor([ 1.1072e-02, -6.6445e-03,  7.1063e-03,  4.8839e-03, -1.0830e-02,\n",
            "         2.5894e-03,  1.5522e-05, -1.2418e-03, -8.7706e-03, -5.2989e-03,\n",
            "        -5.3494e-03,  5.6749e-03,  5.9087e-03, -5.6115e-04, -1.1048e-02,\n",
            "         5.9719e-03,  7.5784e-03,  5.2520e-03, -1.1285e-02,  2.1654e-03,\n",
            "         6.9247e-03,  8.4048e-04,  1.0189e-02,  1.2038e-02,  7.4966e-03,\n",
            "         7.4328e-03,  4.2427e-03, -3.9403e-03, -5.0483e-03,  3.4941e-04,\n",
            "        -5.4818e-03, -1.4632e-02,  6.6476e-04,  5.8489e-03,  5.0480e-04,\n",
            "         2.2496e-03, -2.7536e-03, -7.7400e-03, -1.5426e-03,  1.3410e-03,\n",
            "        -1.3781e-02, -2.8608e-03,  5.3486e-03, -8.0961e-03, -3.2327e-03,\n",
            "        -2.3984e-03,  1.5162e-03, -1.3352e-02,  4.5167e-03, -3.8917e-03,\n",
            "         7.2104e-03,  3.5453e-03,  1.8506e-03, -1.3689e-03, -1.0062e-02,\n",
            "        -2.5898e-04,  7.3579e-04,  7.3209e-04,  1.1709e-03,  2.0172e-02,\n",
            "        -6.9201e-03,  1.0179e-02,  6.3485e-03,  7.9693e-03,  1.0104e-02,\n",
            "        -1.2394e-02, -6.7227e-03, -1.1418e-03, -5.6199e-03, -5.2432e-03,\n",
            "         1.0470e-02, -5.5766e-03,  1.3175e-02, -4.4467e-03,  1.6297e-03,\n",
            "        -2.3943e-03, -6.3895e-03, -4.0902e-04, -4.7161e-03, -3.3696e-03,\n",
            "        -7.2354e-03, -2.9882e-04, -7.3909e-03,  3.7436e-03,  1.7892e-03,\n",
            "        -6.0562e-03,  4.1904e-03,  5.6578e-03,  1.1050e-02,  8.5526e-03,\n",
            "        -2.8809e-03, -3.1054e-03,  1.0833e-02,  1.0873e-02, -1.0962e-02,\n",
            "         2.0150e-02, -1.4258e-03, -1.2238e-02,  6.9092e-03,  1.0704e-02,\n",
            "         4.5239e-03, -6.5267e-03, -8.5668e-04,  1.5871e-03, -8.6557e-03,\n",
            "         1.4620e-02,  7.9282e-03,  6.8398e-03,  9.7502e-03, -1.7820e-03,\n",
            "        -9.8607e-05, -7.7984e-03,  6.0538e-03, -3.5643e-03,  8.3478e-03,\n",
            "        -3.7428e-03,  3.6929e-03, -7.8180e-03, -8.7228e-03, -2.3131e-03,\n",
            "        -1.0759e-02,  1.2270e-02, -1.2464e-02, -4.6187e-03,  9.5586e-03,\n",
            "        -2.6832e-04,  1.1558e-02,  7.7165e-03,  4.6975e-03,  9.0431e-03,\n",
            "         7.8922e-03, -5.7671e-03, -1.0789e-02, -8.1822e-03,  7.0524e-04,\n",
            "        -2.8997e-03, -7.8233e-03, -6.4945e-03, -2.5276e-03, -2.7055e-03,\n",
            "        -6.1433e-03,  3.8946e-03,  6.9053e-03, -6.2446e-03,  5.3110e-03,\n",
            "         1.5493e-03,  1.2203e-02,  6.5794e-03,  3.5462e-03,  6.4855e-03,\n",
            "        -1.1017e-02,  3.7107e-03,  7.9055e-03, -2.1649e-03, -1.6580e-03,\n",
            "         2.2041e-03,  8.1061e-04, -7.8678e-03,  6.0076e-03, -5.2368e-04,\n",
            "         5.7008e-04,  6.5348e-03,  1.4658e-02,  2.0092e-03,  6.7844e-03,\n",
            "         1.3325e-02,  2.0140e-03, -7.3003e-03,  2.3791e-03,  7.6367e-03,\n",
            "         7.1888e-03, -7.5051e-05,  1.2043e-02,  6.2707e-03, -1.8353e-03,\n",
            "        -3.5981e-03,  1.3284e-02, -6.3974e-03, -4.0611e-03, -1.8722e-03,\n",
            "         3.7599e-03,  6.5866e-03,  5.5877e-03,  9.5509e-03,  7.6790e-03,\n",
            "         1.5633e-02, -1.0601e-02,  1.5275e-02,  6.3151e-03,  1.8648e-04,\n",
            "         7.4097e-03,  2.0439e-03,  1.6548e-03, -1.3207e-02, -1.1606e-03,\n",
            "         7.0459e-03, -5.5649e-03,  1.9606e-03,  1.0768e-02, -9.4711e-03,\n",
            "         6.9532e-03,  5.6053e-03, -2.2705e-03,  4.1339e-03, -6.9988e-03,\n",
            "        -6.2762e-03, -8.2178e-04,  1.2739e-02,  1.8880e-03, -1.0253e-02,\n",
            "        -8.7494e-03, -1.1302e-02, -3.0640e-03, -6.3962e-03, -1.5775e-03,\n",
            "        -2.9621e-03, -3.4142e-03,  9.2213e-03,  5.1307e-03,  3.2290e-03,\n",
            "        -2.8564e-03, -9.1945e-04, -1.3998e-03,  1.7860e-02, -7.7595e-03,\n",
            "         7.4562e-03, -4.7973e-03, -4.3676e-03, -9.8791e-03, -3.9496e-03,\n",
            "         3.7120e-04,  3.4888e-04,  4.4530e-03,  1.2395e-02,  1.0471e-02,\n",
            "         7.0980e-03, -6.9679e-03, -1.1456e-02,  7.7072e-03,  2.1652e-03,\n",
            "         1.6197e-02, -2.3741e-03, -3.5153e-03, -7.1295e-03, -3.0465e-03,\n",
            "        -7.2928e-03,  5.2501e-03,  3.3519e-03, -3.6850e-04,  6.3026e-03,\n",
            "         6.6592e-03, -3.5174e-03, -3.5144e-03,  4.9569e-03,  1.6520e-02,\n",
            "         3.7007e-03,  9.2444e-03, -7.1086e-03, -3.7412e-03, -9.4778e-03,\n",
            "        -9.7106e-04,  1.0163e-02,  5.5679e-03, -5.4084e-03,  5.8326e-03,\n",
            "         5.1040e-03, -3.7457e-03,  6.7772e-03,  3.4302e-03, -3.7228e-03,\n",
            "         7.7923e-03,  5.5800e-03,  6.6441e-03, -8.8373e-03, -4.9424e-03,\n",
            "         6.8504e-03, -1.1694e-02, -2.6708e-03,  5.5121e-03,  1.0748e-02,\n",
            "         1.3545e-02,  2.7785e-03,  4.5409e-03,  2.6857e-04,  4.6255e-03,\n",
            "        -2.9270e-03,  7.0746e-03,  1.2585e-03,  1.3028e-03, -1.0476e-02,\n",
            "         5.7758e-03, -9.9256e-03,  1.5838e-02, -1.6095e-03, -8.5653e-03,\n",
            "        -9.9527e-03, -9.6922e-03, -9.0025e-03, -1.6161e-03, -3.0872e-03,\n",
            "        -3.8476e-03,  7.2299e-03,  4.7419e-03,  1.1847e-03, -6.2408e-04,\n",
            "        -6.7026e-03,  3.0000e-03, -4.1936e-03, -2.3870e-03,  6.9167e-03,\n",
            "         1.2646e-03,  2.9325e-03,  1.0819e-02, -1.4090e-03, -1.5475e-02,\n",
            "         9.4779e-03,  7.8599e-03,  4.0650e-03, -9.1944e-03, -3.0205e-03,\n",
            "        -3.8084e-03,  7.7049e-03,  5.1738e-03,  9.7666e-03, -1.4126e-03,\n",
            "         6.6496e-03,  5.4554e-03,  6.0382e-03,  3.0760e-03,  5.9289e-03,\n",
            "         2.2701e-03, -8.6601e-03, -1.6233e-02,  6.6303e-03, -2.7744e-03,\n",
            "        -1.1597e-02, -6.1931e-03,  6.2522e-03, -5.7803e-03, -5.6991e-04,\n",
            "        -6.4852e-06, -1.3262e-03,  4.4084e-04, -5.9887e-03, -6.0576e-03,\n",
            "         8.7574e-03,  1.4596e-02,  7.9578e-03,  1.3598e-02, -5.3815e-03,\n",
            "         1.7218e-03,  9.2931e-03,  2.1738e-03,  8.7505e-03, -8.1233e-03,\n",
            "         1.3067e-02,  4.0794e-03, -3.0010e-05, -9.9144e-03, -1.1807e-02,\n",
            "        -5.4039e-03,  4.9735e-03,  7.0501e-03,  1.8633e-03, -2.7059e-03,\n",
            "         3.2005e-03,  8.2330e-03,  9.9640e-04,  5.6309e-03, -1.1650e-02,\n",
            "         3.8081e-03,  1.0005e-03, -2.9278e-03, -8.2586e-03, -8.8200e-03,\n",
            "         4.0975e-03, -2.5973e-03,  1.0675e-02, -4.9990e-03, -1.1033e-02,\n",
            "        -4.3205e-03,  1.3001e-03, -7.3672e-03,  6.8852e-03, -1.0245e-02,\n",
            "         7.9758e-03,  3.2867e-03,  9.4611e-03,  1.5310e-02, -1.1904e-02,\n",
            "         1.7913e-03,  1.1105e-02,  1.8254e-03,  6.9989e-03,  4.1576e-03,\n",
            "         3.4106e-03,  1.0686e-02, -1.2019e-03, -7.5355e-03, -2.6464e-03,\n",
            "        -3.0858e-03, -7.5773e-03, -8.7092e-03, -9.3451e-03, -1.5007e-03,\n",
            "        -3.7165e-03,  5.6688e-03, -8.9380e-03,  5.3537e-03, -4.7236e-03,\n",
            "         1.4797e-02, -5.1794e-03,  9.3719e-03, -6.2269e-04, -3.7638e-03,\n",
            "         5.1067e-03,  1.4531e-03, -1.0088e-02, -1.9288e-04,  9.8788e-03,\n",
            "        -1.8280e-03,  2.4062e-03,  1.0279e-02,  5.2115e-03, -7.5243e-03,\n",
            "        -1.1863e-02, -5.8946e-03,  8.8486e-03,  1.2365e-02,  1.8356e-03,\n",
            "         9.8932e-03,  1.4998e-02, -1.2691e-02,  9.1905e-03,  3.5983e-03,\n",
            "        -3.5611e-03, -9.7359e-03,  9.9033e-03,  1.3809e-02, -4.1566e-03,\n",
            "        -1.8260e-03,  6.4960e-03, -3.5218e-03,  8.3415e-03, -1.2634e-02,\n",
            "        -9.8903e-03,  5.3443e-04, -2.7253e-03,  4.8292e-03,  2.9448e-03,\n",
            "        -1.0435e-03, -7.6866e-03,  8.7056e-04,  7.7250e-03,  8.2026e-03,\n",
            "         1.0383e-03,  4.2695e-03,  1.5085e-02,  9.2389e-03,  6.6438e-03,\n",
            "         8.8234e-03,  1.0316e-02,  1.1158e-03, -1.0789e-02, -1.0561e-02,\n",
            "         4.0443e-03,  2.6127e-03, -9.8118e-03,  8.0698e-03, -9.6934e-04,\n",
            "        -1.2706e-02,  7.3588e-03, -3.5275e-03,  5.0125e-03, -6.0469e-04,\n",
            "         4.7994e-04,  1.0329e-02, -3.4834e-03, -7.1455e-03, -7.4492e-03,\n",
            "         1.0961e-02,  1.7164e-03, -2.2029e-03, -1.0708e-02,  2.8248e-04,\n",
            "        -8.9241e-03,  7.3392e-03, -1.3499e-02,  6.2000e-03, -2.5323e-04,\n",
            "         7.7981e-03,  2.7678e-03, -2.7587e-03,  8.3014e-03, -4.0793e-03,\n",
            "         2.3439e-02, -1.3265e-02,  1.8158e-02, -1.1004e-02, -4.2285e-03,\n",
            "        -1.0435e-02,  3.9357e-03,  8.7667e-04,  2.5497e-03, -2.1873e-03,\n",
            "         1.3859e-02,  6.4078e-03,  3.6346e-04,  4.3622e-03, -3.6130e-03,\n",
            "         6.0657e-03,  1.0341e-02], requires_grad=True)\n",
            "classifier.weight, Parameter containing:\n",
            "tensor([[ 0.0359,  0.0103, -0.0372,  ...,  0.0387,  0.0418, -0.0350],\n",
            "        [ 0.0887, -0.0033, -0.0498,  ..., -0.0822, -0.0123,  0.0434],\n",
            "        [ 0.0436,  0.0154, -0.0520,  ...,  0.0721, -0.0191, -0.0233],\n",
            "        [ 0.0813, -0.0383,  0.1036,  ..., -0.0864, -0.0380,  0.0220],\n",
            "        [-0.0208, -0.0674, -0.0162,  ..., -0.0632, -0.0303, -0.0616],\n",
            "        [ 0.0258, -0.0105,  0.0341,  ...,  0.0470,  0.0022, -0.0353]],\n",
            "       requires_grad=True)\n",
            "classifier.bias, Parameter containing:\n",
            "tensor([ 0.0272, -0.0366, -0.0044,  0.0545, -0.0790,  0.0608],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2.classifier.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gguxiQzEQUf",
        "outputId": "a9fd5f05-5d48-48f7-eac4-ca0801783864"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0359,  0.0103, -0.0372,  ...,  0.0387,  0.0418, -0.0350],\n",
              "        [ 0.0887, -0.0033, -0.0498,  ..., -0.0822, -0.0123,  0.0434],\n",
              "        [ 0.0436,  0.0154, -0.0520,  ...,  0.0721, -0.0191, -0.0233],\n",
              "        [ 0.0813, -0.0383,  0.1036,  ..., -0.0864, -0.0380,  0.0220],\n",
              "        [-0.0208, -0.0674, -0.0162,  ..., -0.0632, -0.0303, -0.0616],\n",
              "        [ 0.0258, -0.0105,  0.0341,  ...,  0.0470,  0.0022, -0.0353]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2 = ModifiedCNN(num_classes=10, hidden_size=512, new_hidden_size=512)"
      ],
      "metadata": {
        "id": "s97QN3INCVSn"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, p in model_task2.named_parameters():\n",
        "    print(f\"{n}, {p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPjiNociDopa",
        "outputId": "b5ae24fe-22b4-47b8-d2fe-62c4863b18cc"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight, Parameter containing:\n",
            "tensor([[[[-0.0424, -0.7092,  0.8925],\n",
            "          [-0.2007, -0.3538,  0.4573],\n",
            "          [-0.0191,  0.3872,  0.1740]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2868,  0.4410, -0.1939],\n",
            "          [ 0.8882, -0.0712,  0.0279],\n",
            "          [ 0.5257,  0.3444, -0.4839]]],\n",
            "\n",
            "\n",
            "        [[[-0.9819,  0.4272, -0.4246],\n",
            "          [ 0.4682,  0.3654, -0.6178],\n",
            "          [-0.0928, -0.2722,  0.1935]]],\n",
            "\n",
            "\n",
            "        [[[-1.0172, -0.8041, -0.3511],\n",
            "          [-0.5816,  0.3598,  0.4628],\n",
            "          [ 0.3011,  0.3005, -0.0158]]],\n",
            "\n",
            "\n",
            "        [[[-0.2432, -0.6164,  0.0258],\n",
            "          [-1.2331, -0.1153, -0.6541],\n",
            "          [ 0.5195,  1.0962,  0.7474]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2384, -0.3966,  0.9313],\n",
            "          [-0.2341, -0.2060, -0.0087],\n",
            "          [-0.6331,  0.4961, -0.5190]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1002,  0.5174,  0.2105],\n",
            "          [-0.3143, -0.0340,  0.0095],\n",
            "          [ 0.6088,  0.6862,  0.5214]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1544, -0.2725, -0.6826],\n",
            "          [-0.2867,  0.2775, -0.7737],\n",
            "          [-0.0959, -0.9020, -0.0610]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1470, -0.3820,  0.2753],\n",
            "          [ 0.1025, -0.4426, -0.5400],\n",
            "          [-0.5557,  0.6900,  0.0230]]],\n",
            "\n",
            "\n",
            "        [[[-0.1286, -0.4810, -0.0842],\n",
            "          [-0.3000, -0.5569,  0.4666],\n",
            "          [ 0.4263,  0.2863, -0.4407]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3147,  1.1796,  0.4933],\n",
            "          [ 0.5721,  0.5297,  0.9832],\n",
            "          [ 0.2682, -0.1975,  0.0163]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9253, -0.0928, -0.2276],\n",
            "          [ 0.1199, -0.6204, -0.1290],\n",
            "          [ 0.5906,  0.4621, -0.4691]]],\n",
            "\n",
            "\n",
            "        [[[-0.2868,  0.3081,  0.9165],\n",
            "          [-0.5682, -0.2902,  0.2147],\n",
            "          [-0.1768, -0.7842,  0.3770]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3104, -0.7860, -0.5293],\n",
            "          [ 0.1425, -0.0532,  0.7367],\n",
            "          [ 0.4155, -0.0470,  0.3405]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2305, -0.3227, -0.0152],\n",
            "          [ 0.0063,  0.0241,  0.1234],\n",
            "          [ 0.7175,  0.1135,  0.3605]]],\n",
            "\n",
            "\n",
            "        [[[-0.6464, -0.9498, -0.1309],\n",
            "          [ 0.5725,  0.8211, -1.0046],\n",
            "          [-0.3003,  0.1123,  0.3385]]]], requires_grad=True)\n",
            "conv1.bias, Parameter containing:\n",
            "tensor([ 0.3182,  0.2113,  0.3220,  0.0499, -0.1007,  0.2825,  0.0568, -0.2058,\n",
            "         0.2861, -0.0577,  0.0713,  0.3142, -0.1222,  0.1828,  0.3241,  0.3267],\n",
            "       requires_grad=True)\n",
            "conv2.weight, Parameter containing:\n",
            "tensor([[[[ 0.0860,  0.0981, -0.1749],\n",
            "          [ 0.0134, -0.0111, -0.0901],\n",
            "          [-0.0734, -0.0876, -0.0918]],\n",
            "\n",
            "         [[ 0.1525,  0.0798, -0.1035],\n",
            "          [ 0.1021,  0.0398, -0.0651],\n",
            "          [-0.0627, -0.0079,  0.1001]],\n",
            "\n",
            "         [[-0.0249, -0.1735, -0.0945],\n",
            "          [ 0.1597, -0.1501,  0.0051],\n",
            "          [-0.0272, -0.2157, -0.0286]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0635,  0.0872, -0.0360],\n",
            "          [ 0.0399,  0.1805, -0.0435],\n",
            "          [-0.0253,  0.0221,  0.0492]],\n",
            "\n",
            "         [[ 0.0543,  0.0073, -0.0443],\n",
            "          [-0.1687,  0.0307, -0.0161],\n",
            "          [ 0.1456, -0.0170, -0.1001]],\n",
            "\n",
            "         [[ 0.1748, -0.1574,  0.0579],\n",
            "          [ 0.0100, -0.2165,  0.0352],\n",
            "          [ 0.1213, -0.0252, -0.2145]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1191, -0.0975, -0.0135],\n",
            "          [ 0.0516,  0.1128, -0.0874],\n",
            "          [-0.0392,  0.1567,  0.0151]],\n",
            "\n",
            "         [[ 0.0029,  0.0808, -0.0043],\n",
            "          [ 0.0505, -0.1111, -0.0232],\n",
            "          [ 0.0874, -0.1946,  0.1646]],\n",
            "\n",
            "         [[ 0.0764, -0.2672,  0.1321],\n",
            "          [ 0.2006,  0.0967, -0.0377],\n",
            "          [ 0.2814, -0.0418,  0.0915]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1032, -0.1849,  0.0254],\n",
            "          [ 0.0958, -0.0977, -0.0370],\n",
            "          [-0.0817,  0.0023, -0.0377]],\n",
            "\n",
            "         [[ 0.1555,  0.1614,  0.0401],\n",
            "          [-0.0746,  0.0202,  0.1301],\n",
            "          [ 0.1579, -0.0866,  0.1274]],\n",
            "\n",
            "         [[-0.1977, -0.0248, -0.1084],\n",
            "          [ 0.0691,  0.0407, -0.1231],\n",
            "          [ 0.1614,  0.0013, -0.0434]]],\n",
            "\n",
            "\n",
            "        [[[-0.0167, -0.0345, -0.0586],\n",
            "          [-0.1593,  0.0528, -0.0499],\n",
            "          [-0.1804, -0.0131,  0.1048]],\n",
            "\n",
            "         [[-0.0351, -0.1451,  0.0680],\n",
            "          [ 0.1347,  0.1406,  0.1194],\n",
            "          [ 0.0377,  0.0346,  0.0355]],\n",
            "\n",
            "         [[ 0.0428,  0.1426,  0.0525],\n",
            "          [ 0.0196, -0.0624,  0.0830],\n",
            "          [ 0.0263,  0.0032, -0.0087]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1096, -0.0146,  0.0179],\n",
            "          [ 0.0162, -0.2370, -0.0857],\n",
            "          [-0.0579,  0.0470, -0.1572]],\n",
            "\n",
            "         [[ 0.0350, -0.0483, -0.1334],\n",
            "          [ 0.0153, -0.0866,  0.3111],\n",
            "          [ 0.0507, -0.2004,  0.1455]],\n",
            "\n",
            "         [[-0.0138, -0.0217,  0.1798],\n",
            "          [ 0.1194, -0.0028, -0.0057],\n",
            "          [ 0.1173,  0.1577,  0.0437]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0504,  0.0082,  0.1649],\n",
            "          [-0.1315, -0.0148,  0.1291],\n",
            "          [-0.1888,  0.2084, -0.1078]],\n",
            "\n",
            "         [[ 0.1420,  0.1608,  0.0299],\n",
            "          [-0.0288,  0.2254,  0.3654],\n",
            "          [-0.0707,  0.0079,  0.1555]],\n",
            "\n",
            "         [[-0.0948,  0.1942, -0.0007],\n",
            "          [-0.0858,  0.0080, -0.0979],\n",
            "          [ 0.0375, -0.0934,  0.0306]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1347,  0.1620,  0.2160],\n",
            "          [ 0.0944, -0.1145,  0.1076],\n",
            "          [ 0.0435, -0.1220, -0.1966]],\n",
            "\n",
            "         [[-0.1998,  0.0162,  0.1270],\n",
            "          [ 0.0626, -0.0007, -0.0955],\n",
            "          [-0.0785, -0.0508, -0.1556]],\n",
            "\n",
            "         [[-0.2697, -0.1759, -0.0078],\n",
            "          [-0.0778,  0.0643, -0.1059],\n",
            "          [-0.0986,  0.0644,  0.0728]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0804,  0.0842, -0.0068],\n",
            "          [ 0.0565, -0.0784,  0.0046],\n",
            "          [-0.0202,  0.1317,  0.0587]],\n",
            "\n",
            "         [[ 0.0466,  0.0221, -0.1749],\n",
            "          [ 0.1202, -0.2435, -0.0258],\n",
            "          [ 0.1397,  0.0355, -0.2213]],\n",
            "\n",
            "         [[-0.0819,  0.2090, -0.1249],\n",
            "          [-0.0883, -0.0135, -0.1513],\n",
            "          [ 0.1074, -0.0027,  0.0050]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0023, -0.0587, -0.2249],\n",
            "          [ 0.0647, -0.0611,  0.0688],\n",
            "          [ 0.0834, -0.0084, -0.0395]],\n",
            "\n",
            "         [[-0.0719,  0.0140,  0.0749],\n",
            "          [-0.1203,  0.0474, -0.0507],\n",
            "          [-0.0940, -0.1687,  0.2054]],\n",
            "\n",
            "         [[ 0.0016,  0.1749, -0.0054],\n",
            "          [ 0.0696,  0.0474,  0.0215],\n",
            "          [-0.0120, -0.0385,  0.0080]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0814, -0.1636,  0.1674],\n",
            "          [-0.1519, -0.1541, -0.0248],\n",
            "          [-0.0426,  0.0724, -0.0009]],\n",
            "\n",
            "         [[-0.1314,  0.0806, -0.0412],\n",
            "          [ 0.1600,  0.1436, -0.0065],\n",
            "          [ 0.1283,  0.1907,  0.0778]],\n",
            "\n",
            "         [[ 0.0464, -0.3221,  0.0693],\n",
            "          [ 0.0816, -0.2483,  0.1547],\n",
            "          [-0.1672, -0.0576,  0.1749]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1096,  0.3525,  0.0437],\n",
            "          [-0.0099, -0.0054,  0.0240],\n",
            "          [-0.1821,  0.0376,  0.0828]],\n",
            "\n",
            "         [[-0.0392,  0.0365, -0.0531],\n",
            "          [ 0.0497, -0.0756, -0.1232],\n",
            "          [ 0.0230,  0.1367, -0.0444]],\n",
            "\n",
            "         [[ 0.0009,  0.0758, -0.0523],\n",
            "          [ 0.1498, -0.0476, -0.1039],\n",
            "          [ 0.0262,  0.1002, -0.0626]]]], requires_grad=True)\n",
            "conv2.bias, Parameter containing:\n",
            "tensor([-0.0650,  0.0147,  0.0706, -0.0805,  0.0479, -0.0248, -0.0791, -0.0582,\n",
            "        -0.0223, -0.0015,  0.0770,  0.0779, -0.0144, -0.0451,  0.0208, -0.0493,\n",
            "        -0.0299,  0.0806,  0.0224, -0.0694, -0.0714, -0.0536,  0.0620,  0.0117,\n",
            "         0.0293, -0.0831, -0.0103, -0.0222, -0.0216, -0.0100,  0.0376, -0.0690],\n",
            "       requires_grad=True)\n",
            "fc1.weight, Parameter containing:\n",
            "tensor([[ 0.0018,  0.0057,  0.0145,  ...,  0.0124,  0.0135, -0.0132],\n",
            "        [ 0.0054, -0.0108, -0.0051,  ..., -0.0117, -0.0193,  0.0112],\n",
            "        [-0.0023,  0.0209,  0.0045,  ...,  0.0109, -0.0129, -0.0011],\n",
            "        ...,\n",
            "        [-0.0054,  0.0324, -0.0068,  ...,  0.0131,  0.0054, -0.0022],\n",
            "        [-0.0072,  0.0073,  0.0057,  ..., -0.0229, -0.0018,  0.0175],\n",
            "        [ 0.0023, -0.0017,  0.0134,  ...,  0.0026,  0.0254, -0.0213]],\n",
            "       requires_grad=True)\n",
            "fc1.bias, Parameter containing:\n",
            "tensor([-2.3306e-03,  4.6307e-03,  3.0030e-03, -1.3117e-04,  3.4978e-03,\n",
            "        -1.9317e-03,  4.4098e-04,  4.6615e-03, -6.1244e-03,  1.1251e-02,\n",
            "        -1.1943e-02, -1.9878e-03,  9.8318e-03,  7.9422e-03, -9.0229e-03,\n",
            "         5.1771e-03,  7.0859e-03,  1.1251e-02,  1.3188e-03, -3.5731e-03,\n",
            "        -4.3617e-03, -5.7624e-03, -1.0600e-02, -9.3219e-03, -8.7381e-03,\n",
            "         1.0119e-02,  6.8806e-03, -9.8544e-03,  8.1965e-03,  1.1029e-02,\n",
            "         1.1432e-02,  1.0723e-02, -6.4041e-03, -1.5181e-03,  6.8997e-03,\n",
            "         3.0484e-03, -6.8396e-03,  1.0990e-02,  1.4433e-03, -5.9178e-03,\n",
            "        -9.0978e-03, -6.4471e-03, -8.7323e-03, -1.7954e-03,  4.3922e-03,\n",
            "         4.0950e-03,  6.2734e-05,  1.1644e-02,  9.7678e-03, -5.8409e-03,\n",
            "         5.3057e-03,  7.0714e-03,  6.3016e-03,  1.5839e-03,  1.2283e-03,\n",
            "         7.1483e-03,  1.0704e-02,  6.0792e-03, -8.0906e-03,  6.7497e-03,\n",
            "        -5.1666e-03,  9.2827e-03, -1.0819e-02, -1.1918e-02,  1.1379e-02,\n",
            "         5.5782e-03,  3.6606e-04, -5.4458e-03,  6.9642e-04,  3.1387e-03,\n",
            "         7.8344e-03, -9.9243e-03,  6.6712e-03, -9.0553e-03,  1.2414e-02,\n",
            "        -1.1881e-02,  1.9544e-03,  6.2550e-03,  1.9287e-03, -9.6230e-04,\n",
            "        -4.8226e-03, -1.0735e-02, -1.0297e-02, -4.5551e-03, -9.4629e-03,\n",
            "         5.0462e-04,  1.5729e-03, -7.4470e-03,  9.2264e-03, -9.2438e-03,\n",
            "        -3.9043e-03, -5.4858e-03,  1.2007e-02,  7.5508e-03, -9.3459e-03,\n",
            "         6.5774e-03, -1.1244e-03, -1.0735e-03, -8.6397e-04, -1.1418e-02,\n",
            "         5.8387e-03, -6.0743e-03,  1.1906e-02, -7.0759e-03,  2.3044e-04,\n",
            "         7.3527e-03,  3.7336e-03,  3.2447e-03,  6.9604e-03,  2.3047e-03,\n",
            "         1.0852e-02,  1.2013e-02, -2.5111e-03,  1.1527e-02, -4.8060e-03,\n",
            "        -1.0096e-02, -8.9742e-03,  1.0104e-02,  1.0113e-02, -5.8777e-03,\n",
            "        -9.3179e-03, -9.0757e-03,  3.2501e-03, -1.5824e-03, -1.6348e-03,\n",
            "        -6.3731e-03, -3.1486e-03,  5.6746e-03, -6.0758e-03,  1.0100e-02,\n",
            "         4.9929e-03,  8.2806e-03,  1.0981e-02,  4.9161e-03,  8.9866e-03,\n",
            "         3.0043e-03, -9.9888e-04, -1.0213e-02, -7.0832e-03,  9.3998e-03,\n",
            "        -6.9763e-03, -8.7628e-03,  9.5771e-03, -6.9973e-03,  5.1515e-03,\n",
            "        -6.6990e-03,  1.1893e-02, -6.7669e-04, -8.1859e-04, -1.2583e-02,\n",
            "        -1.1517e-02, -2.1468e-03, -1.1057e-02,  5.6037e-04,  3.6609e-03,\n",
            "         1.0876e-02,  1.2247e-02,  5.1502e-03,  2.8754e-03,  6.7268e-03,\n",
            "        -1.2475e-02, -7.0568e-03, -1.1439e-02, -1.8757e-03, -3.8207e-03,\n",
            "         2.0606e-03, -1.0555e-02,  1.7590e-04, -9.3255e-03, -4.5240e-04,\n",
            "        -5.9825e-04, -1.9608e-03, -9.5422e-03,  9.5992e-03,  6.0161e-03,\n",
            "        -6.7434e-03,  1.2495e-02, -8.1404e-06,  1.0823e-02,  5.5927e-03,\n",
            "         1.0033e-02,  4.8258e-03, -6.0707e-03,  1.0013e-02, -3.4946e-04,\n",
            "         9.8115e-04, -8.1718e-03, -5.0818e-03, -7.3276e-03,  6.4963e-03,\n",
            "        -3.3210e-03,  2.8365e-03, -3.4142e-03,  1.0543e-02,  1.1819e-02,\n",
            "         3.5164e-03, -1.8627e-03, -1.3616e-03,  7.7981e-03,  5.8258e-03,\n",
            "         1.1396e-02,  2.2034e-03,  1.4787e-03,  1.8373e-03,  4.7614e-03,\n",
            "         6.2224e-03,  4.4788e-03, -9.4057e-03,  4.1127e-03,  8.6577e-03,\n",
            "         6.2454e-03, -2.7780e-03,  1.1323e-03,  1.3567e-04, -7.3111e-03,\n",
            "         1.1561e-02, -4.0964e-03, -2.9757e-03,  6.8651e-04, -4.7655e-03,\n",
            "         5.9386e-03,  2.0328e-03, -2.6661e-03,  9.0683e-03,  2.8568e-03,\n",
            "         4.2144e-04, -6.8246e-03, -6.5503e-03,  7.2173e-03,  1.0315e-02,\n",
            "        -7.3531e-03,  4.5495e-03, -4.6348e-03, -1.2053e-02, -9.9514e-03,\n",
            "        -7.3350e-03, -4.1315e-03,  8.6358e-03, -4.4443e-03, -2.6870e-03,\n",
            "        -4.0379e-03, -1.0960e-02, -1.1199e-02, -3.6972e-03,  1.0248e-02,\n",
            "         7.1104e-03,  4.2134e-03,  8.8987e-03,  6.4566e-03,  7.6441e-03,\n",
            "        -1.1489e-02, -4.5482e-03, -5.3142e-03,  7.4175e-03,  1.1585e-02,\n",
            "         1.6361e-03,  8.2434e-03, -3.3466e-03, -2.4339e-03,  4.2876e-03,\n",
            "         1.0629e-02,  4.4352e-03, -9.3799e-03, -4.5577e-04, -9.3719e-04,\n",
            "         5.2070e-03,  9.7227e-03,  6.9826e-03, -2.6867e-03,  9.3474e-03,\n",
            "        -5.7973e-03, -1.2174e-02, -1.0263e-02, -9.7301e-03, -4.1798e-03,\n",
            "        -1.0398e-02,  6.8978e-03, -7.8970e-03,  8.2622e-03,  1.0960e-02,\n",
            "        -8.0301e-03,  8.9917e-03, -6.4106e-03, -9.3915e-03,  1.2493e-02,\n",
            "        -4.9129e-03, -1.7886e-03, -9.5450e-03,  3.6154e-03,  4.4051e-03,\n",
            "         7.6888e-03, -8.5381e-03, -1.1392e-02,  5.8444e-03,  1.1443e-02,\n",
            "         6.6122e-03,  2.9507e-03,  1.1711e-02, -1.0686e-02,  7.6991e-03,\n",
            "        -9.8916e-03,  9.0406e-03, -5.5588e-03,  1.2230e-03,  8.0549e-03,\n",
            "        -1.2096e-02,  8.3797e-03, -1.0259e-02,  3.7859e-03, -8.3869e-03,\n",
            "         1.2519e-02, -5.2481e-03, -5.7983e-03,  2.7732e-03, -5.8551e-03,\n",
            "         7.4751e-03, -5.0999e-03, -9.2662e-03,  4.2526e-04, -3.0458e-04,\n",
            "        -9.1208e-03,  1.2260e-03,  1.0882e-03,  2.8194e-03, -1.5945e-03,\n",
            "        -2.1593e-03, -1.1520e-02, -4.9761e-03, -1.0615e-02,  1.0667e-02,\n",
            "         8.2151e-03, -1.0388e-02, -6.6072e-03,  6.1508e-03, -3.2969e-03,\n",
            "        -9.3614e-03, -1.0644e-02, -1.9959e-03,  2.3785e-04, -9.9647e-03,\n",
            "         2.5496e-03, -1.0903e-02,  2.9974e-03, -3.0086e-04, -8.0015e-03,\n",
            "        -8.7330e-03,  5.7152e-03,  7.3722e-03,  6.2280e-04, -1.0860e-02,\n",
            "         1.0044e-02,  4.0902e-04,  6.2199e-03,  2.6272e-03, -4.0606e-03,\n",
            "         5.8729e-03, -1.0705e-02,  1.1320e-02,  2.1445e-04,  7.8618e-04,\n",
            "         8.7163e-03,  7.7550e-03,  1.1262e-02,  6.2563e-03, -3.9740e-03,\n",
            "        -1.0762e-02, -2.6647e-03,  6.5431e-04,  4.4160e-03, -1.2113e-02,\n",
            "        -1.0902e-02, -3.5074e-03,  5.6137e-03,  1.1664e-02, -1.1489e-02,\n",
            "         7.8491e-03, -1.1084e-02,  1.2383e-02,  2.1698e-03, -6.4433e-03,\n",
            "         9.5600e-04,  1.0659e-02, -1.1699e-03, -1.0349e-02,  1.2865e-03,\n",
            "         1.2105e-02,  1.1228e-02,  9.4546e-03, -2.5706e-03,  7.5221e-03,\n",
            "         4.7540e-03, -1.5112e-03, -8.6310e-03, -1.8718e-03, -4.2337e-03,\n",
            "         1.2385e-02, -8.5641e-03,  3.5663e-03, -6.0785e-03,  1.0221e-02,\n",
            "        -2.4743e-03, -9.2664e-03, -8.7523e-03,  3.6294e-03, -9.9640e-03,\n",
            "         2.5909e-03, -4.1775e-03, -5.6321e-03, -1.1839e-02,  1.7222e-03,\n",
            "         9.9091e-04,  3.0189e-03, -2.7538e-03, -1.2050e-02,  9.2892e-03,\n",
            "         1.0358e-02,  1.0841e-02,  2.7494e-03,  3.0885e-03, -2.3055e-03,\n",
            "         3.5526e-03, -8.9687e-03, -9.5410e-03, -5.1488e-04, -4.5386e-03,\n",
            "         1.2581e-02,  2.7753e-03, -4.2833e-03,  1.1881e-03,  6.0209e-03,\n",
            "        -4.3051e-03,  1.7889e-03,  5.5485e-03,  5.3261e-03, -2.2507e-03,\n",
            "        -1.0734e-02,  1.0982e-02, -2.2982e-03,  5.6356e-03,  2.5544e-03,\n",
            "        -5.6889e-03,  1.1283e-02, -1.0227e-02, -5.6066e-03,  5.3498e-03,\n",
            "        -3.0789e-04, -2.2202e-03, -8.9083e-03, -1.0620e-02,  3.6011e-03,\n",
            "         1.8856e-03,  7.3212e-03,  4.2004e-03, -3.8676e-03, -8.3150e-03,\n",
            "         1.1862e-03, -1.0265e-02, -2.3771e-03, -2.6687e-03,  7.7963e-04,\n",
            "         1.0050e-02,  7.6091e-03,  7.8991e-03,  9.4919e-03, -1.2143e-02,\n",
            "        -8.7900e-03,  1.0266e-02, -3.1616e-03, -5.3235e-03, -5.0391e-03,\n",
            "         7.8807e-03, -1.2562e-02,  1.1748e-03, -9.0796e-04, -1.0652e-02,\n",
            "         2.8466e-05,  6.6718e-03, -1.1131e-03,  7.3824e-03, -1.2565e-02,\n",
            "        -6.7582e-03, -6.6293e-03, -8.4713e-04,  6.8328e-04,  3.0757e-03,\n",
            "         1.2068e-02,  8.2444e-04,  1.2888e-03,  3.5436e-03, -3.2855e-03,\n",
            "         5.4444e-03, -1.1928e-02, -5.3133e-03, -4.0907e-04,  6.5884e-03,\n",
            "         2.0802e-03, -3.3129e-03, -4.4586e-03, -7.4672e-03,  9.2496e-03,\n",
            "        -2.0980e-03, -1.0878e-02, -1.2276e-02, -1.0470e-02, -1.0397e-02,\n",
            "        -3.5357e-03,  6.4944e-03, -4.2485e-03,  1.2260e-02,  9.7757e-03,\n",
            "        -1.2170e-02, -6.3482e-04], requires_grad=True)\n",
            "classifier.weight, Parameter containing:\n",
            "tensor([[-0.0291,  0.0266,  0.0046,  ...,  0.0420, -0.0244,  0.0118],\n",
            "        [-0.0081,  0.0377,  0.0040,  ...,  0.0394, -0.0178,  0.0197],\n",
            "        [ 0.0378,  0.0112,  0.0073,  ...,  0.0038,  0.0103,  0.0161],\n",
            "        ...,\n",
            "        [-0.0027, -0.0182,  0.0116,  ...,  0.0419, -0.0274, -0.0309],\n",
            "        [-0.0212, -0.0006,  0.0388,  ..., -0.0239,  0.0288,  0.0331],\n",
            "        [ 0.0326,  0.0351, -0.0070,  ...,  0.0417,  0.0278, -0.0340]],\n",
            "       requires_grad=True)\n",
            "classifier.bias, Parameter containing:\n",
            "tensor([ 0.0053,  0.0090,  0.0137, -0.0404,  0.0431, -0.0378, -0.0432, -0.0271,\n",
            "         0.0135, -0.0390], requires_grad=True)\n",
            "new_fc.weight, Parameter containing:\n",
            "tensor([[-9.0142e-05,  8.1405e-03, -3.0186e-02,  ..., -1.5223e-02,\n",
            "         -9.8945e-03,  2.1718e-02],\n",
            "        [ 4.2647e-02,  7.5294e-03, -1.0101e-02,  ...,  2.3259e-02,\n",
            "          1.1376e-02,  3.5204e-02],\n",
            "        [-9.1189e-03, -2.9082e-03,  6.3419e-03,  ..., -3.3283e-02,\n",
            "          2.8702e-02, -1.4840e-02],\n",
            "        ...,\n",
            "        [-2.8096e-02, -9.1608e-03,  4.1286e-02,  ..., -2.8758e-03,\n",
            "         -1.4088e-02, -7.2409e-03],\n",
            "        [ 4.1453e-02, -1.1573e-02, -5.6941e-03,  ..., -2.2185e-02,\n",
            "         -3.1968e-03,  2.3865e-02],\n",
            "        [-4.5448e-03,  4.8099e-03, -2.0739e-02,  ..., -2.4239e-02,\n",
            "          4.1714e-02, -3.9329e-02]], requires_grad=True)\n",
            "new_fc.bias, Parameter containing:\n",
            "tensor([ 0.0278, -0.0379, -0.0038, -0.0045, -0.0121, -0.0301,  0.0121, -0.0098,\n",
            "         0.0376,  0.0330,  0.0258, -0.0170, -0.0179, -0.0256,  0.0113, -0.0142,\n",
            "        -0.0136, -0.0105, -0.0257, -0.0165,  0.0137, -0.0204,  0.0431, -0.0339,\n",
            "        -0.0205, -0.0410, -0.0246, -0.0062, -0.0286,  0.0050, -0.0395,  0.0005,\n",
            "        -0.0404,  0.0360,  0.0162,  0.0401, -0.0006, -0.0376,  0.0060,  0.0279,\n",
            "        -0.0290, -0.0320,  0.0219, -0.0084, -0.0340,  0.0021,  0.0143,  0.0274,\n",
            "         0.0262, -0.0347, -0.0038, -0.0284, -0.0423,  0.0414, -0.0068, -0.0242,\n",
            "        -0.0316, -0.0413,  0.0399,  0.0157, -0.0293, -0.0317, -0.0264,  0.0199,\n",
            "         0.0050,  0.0185,  0.0055, -0.0098, -0.0257, -0.0168, -0.0136, -0.0097,\n",
            "         0.0393, -0.0014, -0.0146,  0.0364, -0.0360,  0.0279,  0.0098, -0.0273,\n",
            "         0.0051,  0.0403, -0.0343, -0.0244, -0.0170, -0.0372, -0.0076, -0.0087,\n",
            "         0.0037, -0.0040, -0.0267, -0.0392, -0.0065, -0.0043, -0.0018, -0.0140,\n",
            "         0.0349, -0.0300,  0.0412,  0.0409,  0.0110, -0.0382,  0.0260,  0.0012,\n",
            "         0.0313,  0.0069,  0.0160,  0.0048, -0.0289, -0.0441, -0.0378,  0.0166,\n",
            "        -0.0224,  0.0138, -0.0375, -0.0363,  0.0034, -0.0246,  0.0438, -0.0299,\n",
            "         0.0180,  0.0396, -0.0174, -0.0420, -0.0054,  0.0070, -0.0019, -0.0304,\n",
            "        -0.0281, -0.0109,  0.0195,  0.0122,  0.0429,  0.0063, -0.0189,  0.0407,\n",
            "        -0.0076,  0.0133, -0.0265, -0.0211,  0.0366,  0.0125, -0.0211,  0.0419,\n",
            "         0.0417, -0.0274, -0.0345,  0.0297, -0.0052, -0.0038,  0.0078,  0.0021,\n",
            "        -0.0428, -0.0218,  0.0402,  0.0171,  0.0009, -0.0282, -0.0246,  0.0218,\n",
            "        -0.0398,  0.0362, -0.0280,  0.0036, -0.0421,  0.0215,  0.0236, -0.0375,\n",
            "         0.0111, -0.0147,  0.0070,  0.0219, -0.0305,  0.0404,  0.0065, -0.0171,\n",
            "         0.0427, -0.0166,  0.0258, -0.0256, -0.0080, -0.0051,  0.0276,  0.0218,\n",
            "        -0.0420,  0.0226, -0.0303,  0.0152, -0.0105,  0.0093,  0.0437,  0.0073,\n",
            "        -0.0404, -0.0310,  0.0074,  0.0319, -0.0368,  0.0040,  0.0052,  0.0230,\n",
            "         0.0355, -0.0104, -0.0426,  0.0214, -0.0169, -0.0263,  0.0355, -0.0223,\n",
            "        -0.0352,  0.0018,  0.0367,  0.0206, -0.0002, -0.0350, -0.0179, -0.0221,\n",
            "         0.0102, -0.0259,  0.0221, -0.0179, -0.0093, -0.0359, -0.0060,  0.0039,\n",
            "         0.0425,  0.0230,  0.0067,  0.0340, -0.0315, -0.0439,  0.0117,  0.0017,\n",
            "        -0.0262, -0.0331,  0.0381, -0.0141, -0.0378, -0.0101, -0.0386,  0.0011,\n",
            "         0.0200, -0.0239, -0.0356,  0.0364, -0.0209,  0.0313,  0.0019,  0.0286,\n",
            "        -0.0062, -0.0427, -0.0049, -0.0181,  0.0285,  0.0177,  0.0373,  0.0019,\n",
            "         0.0152,  0.0334, -0.0325, -0.0146, -0.0393, -0.0384, -0.0269,  0.0050,\n",
            "        -0.0341, -0.0410, -0.0223, -0.0065, -0.0098, -0.0185, -0.0306,  0.0350,\n",
            "         0.0271, -0.0179, -0.0257,  0.0397, -0.0229, -0.0338, -0.0123, -0.0427,\n",
            "         0.0316, -0.0275, -0.0302, -0.0143, -0.0162,  0.0261, -0.0184, -0.0256,\n",
            "         0.0320,  0.0425, -0.0402, -0.0390,  0.0346, -0.0428, -0.0395, -0.0205,\n",
            "         0.0434,  0.0076, -0.0431,  0.0130,  0.0211,  0.0199,  0.0076, -0.0127,\n",
            "        -0.0419, -0.0041,  0.0316,  0.0344, -0.0090,  0.0150, -0.0293,  0.0004,\n",
            "         0.0174,  0.0328, -0.0148,  0.0402,  0.0008,  0.0299,  0.0216,  0.0093,\n",
            "         0.0170,  0.0288,  0.0169,  0.0214, -0.0225,  0.0020,  0.0330, -0.0022,\n",
            "         0.0315, -0.0385, -0.0282,  0.0373, -0.0429,  0.0167,  0.0228, -0.0254,\n",
            "        -0.0095,  0.0139, -0.0290,  0.0089, -0.0014,  0.0208, -0.0385, -0.0165,\n",
            "         0.0127,  0.0406,  0.0064,  0.0180,  0.0417,  0.0003, -0.0259, -0.0089,\n",
            "         0.0254,  0.0375, -0.0087,  0.0377,  0.0422,  0.0373, -0.0053, -0.0358,\n",
            "        -0.0222,  0.0365,  0.0304,  0.0121, -0.0101,  0.0015,  0.0247,  0.0429,\n",
            "         0.0426, -0.0358, -0.0005,  0.0099,  0.0240, -0.0204, -0.0306,  0.0177,\n",
            "        -0.0339,  0.0032, -0.0288, -0.0212, -0.0378, -0.0106,  0.0411, -0.0397,\n",
            "        -0.0028,  0.0360,  0.0152, -0.0118,  0.0084, -0.0277,  0.0407, -0.0312,\n",
            "         0.0016, -0.0374, -0.0365, -0.0394,  0.0422,  0.0140,  0.0327, -0.0044,\n",
            "        -0.0209,  0.0434, -0.0145,  0.0186, -0.0299, -0.0155, -0.0384, -0.0190,\n",
            "        -0.0015,  0.0353, -0.0415, -0.0125,  0.0092, -0.0272,  0.0269, -0.0254,\n",
            "         0.0185, -0.0097, -0.0216,  0.0142, -0.0227, -0.0308, -0.0183, -0.0211,\n",
            "         0.0189, -0.0438,  0.0284, -0.0096,  0.0106,  0.0045, -0.0153, -0.0232,\n",
            "        -0.0031, -0.0296,  0.0181,  0.0101, -0.0429, -0.0124,  0.0087, -0.0110,\n",
            "         0.0266, -0.0350, -0.0330, -0.0251,  0.0402, -0.0158, -0.0383, -0.0176,\n",
            "         0.0363, -0.0210, -0.0201, -0.0164, -0.0313,  0.0379, -0.0037,  0.0442,\n",
            "        -0.0115, -0.0321, -0.0385, -0.0430,  0.0154,  0.0417, -0.0170, -0.0055,\n",
            "         0.0366, -0.0019, -0.0265, -0.0044, -0.0036,  0.0071, -0.0146,  0.0256,\n",
            "         0.0292,  0.0255,  0.0100,  0.0097,  0.0342, -0.0078,  0.0082,  0.0194,\n",
            "         0.0411, -0.0164, -0.0052, -0.0101, -0.0407,  0.0190,  0.0291, -0.0353,\n",
            "        -0.0360,  0.0104,  0.0153,  0.0205,  0.0304,  0.0357, -0.0174, -0.0306,\n",
            "        -0.0307, -0.0434,  0.0370, -0.0395,  0.0039, -0.0342, -0.0141,  0.0217,\n",
            "         0.0107,  0.0353,  0.0210, -0.0325, -0.0340,  0.0278, -0.0078, -0.0220],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2.classifier.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3-kLFCzEWyP",
        "outputId": "798af082-1190-4592-99aa-4ec6b8f92fe0"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0291,  0.0266,  0.0046,  ...,  0.0420, -0.0244,  0.0118],\n",
              "        [-0.0081,  0.0377,  0.0040,  ...,  0.0394, -0.0178,  0.0197],\n",
              "        [ 0.0378,  0.0112,  0.0073,  ...,  0.0038,  0.0103,  0.0161],\n",
              "        ...,\n",
              "        [-0.0027, -0.0182,  0.0116,  ...,  0.0419, -0.0274, -0.0309],\n",
              "        [-0.0212, -0.0006,  0.0388,  ..., -0.0239,  0.0288,  0.0331],\n",
              "        [ 0.0326,  0.0351, -0.0070,  ...,  0.0417,  0.0278, -0.0340]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YPqH4R1FvQ7",
        "outputId": "84458cd5-b260-4560-8059-90180c5c3bc4"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModifiedCNN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=6272, out_features=512, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (new_fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv1_weight = model_task1.conv1.weight.data\n",
        "conv1_bias = model_task1.conv1.bias.data\n",
        "conv2_weight = model_task1.conv2.weight.data\n",
        "conv2_bias = model_task1.conv2.bias.data\n",
        "fc1_weight = model_task1.fc1.weight.data\n",
        "fc1_bias = model_task1.fc1.bias.data\n",
        "\n",
        "model_task2.conv1.weight = nn.Parameter(conv1_weight)\n",
        "model_task2.conv1.bias = nn.Parameter(conv1_bias)\n",
        "model_task2.conv2.weight = nn.Parameter(conv2_weight)\n",
        "model_task2.conv2.bias = nn.Parameter(conv2_bias)\n",
        "model_task2.fc1.weight = nn.Parameter(fc1_weight)\n",
        "model_task2.fc1.bias = nn.Parameter(fc1_bias)"
      ],
      "metadata": {
        "id": "QP3jyon0F1xr"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = model_task1.classifier.in_features\n",
        "out_features = model_task1.classifier.out_features\n",
        "\n",
        "new_weights = torch.zeros_like(model_task2.classifier.weight)\n",
        "new_biases = torch.zeros_like(model_task2.classifier.bias)\n",
        "\n",
        "weight = model_task1.classifier.weight.data\n",
        "bias = model_task1.classifier.bias.data\n",
        "\n",
        "new_weights[:out_features, :] = weight\n",
        "new_biases[:out_features] = bias"
      ],
      "metadata": {
        "id": "S5OqVlx0Dm6D"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2.classifier.weight = nn.Parameter(new_weights)\n",
        "model_task2.classifier.bias = nn.Parameter(new_biases)"
      ],
      "metadata": {
        "id": "jzAMjjeAQMyS"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2.classifier.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnP-icNWEJjw",
        "outputId": "9f8ce86c-dfb3-4867-d2b5-6e538e321a73"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0359,  0.0103, -0.0372,  ...,  0.0387,  0.0418, -0.0350],\n",
              "        [ 0.0887, -0.0033, -0.0498,  ..., -0.0822, -0.0123,  0.0434],\n",
              "        [ 0.0436,  0.0154, -0.0520,  ...,  0.0721, -0.0191, -0.0233],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_task2 = model_task2.to(device)\n",
        "print(\"New head numbers: \", model_task2.classifier.out_features)"
      ],
      "metadata": {
        "id": "ZCbEgdnwNBSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb20a35-4b27-478a-e53e-11aa424980df"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New head numbers:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_task1.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "juo2AE9aQzJr"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initially freeze all the layers except for classifier\n",
        "for name, param in model_task2.named_parameters():\n",
        "    if name not in ['classifier.weight', 'classifier.bias', 'new_fc.weight', 'new_fc.bias']:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "nPEBysFpSD28"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, _ in model_task2.named_parameters():\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1wZkydiwoB6",
        "outputId": "2b6be1ef-feaf-44d0-842e-c03fad221ce2"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight\n",
            "conv1.bias\n",
            "conv2.weight\n",
            "conv2.bias\n",
            "fc1.weight\n",
            "fc1.bias\n",
            "classifier.weight\n",
            "classifier.bias\n",
            "new_fc.weight\n",
            "new_fc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in training and testing"
      ],
      "metadata": {
        "id": "GsAe0iSPTSiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(alpha, T):\n",
        "    size = len(train_dataloader_task2.dataset)\n",
        "    # We set net_new to evaluation mode to prevent it from being updated\n",
        "    # while computing the distillation loss from the old model\n",
        "\n",
        "    model_task2.train()\n",
        "    for batch, (X, y) in enumerate(train_dataloader_task2):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        outputs = model_task2(X)\n",
        "        soft_y = model_task1(X)\n",
        "\n",
        "        loss1 = loss_fn(outputs, y)\n",
        "\n",
        "        outputs_S = nn.functional.softmax(outputs[:, :out_features] / T, dim=1)\n",
        "        outputs_T = nn.functional.softmax(soft_y[:, :out_features] / T, dim=1)\n",
        "\n",
        "        loss2 = outputs_T.mul(-1 * torch.log(outputs_S))\n",
        "        loss2 = loss2.sum(1)\n",
        "        loss2 = loss2.mean() * T * T\n",
        "\n",
        "        loss = loss1 + alpha * loss2\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch+1) * len(X)\n",
        "            print(f\"Loss: {loss:>7f}, {current:>5d}/{size:>5d}\")\n"
      ],
      "metadata": {
        "id": "Tqex4EciTVVN"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(alpha, T):\n",
        "  size = len(test_dataloader_task2.dataset)\n",
        "  num_batches = len(test_dataloader_task2)\n",
        "  model_task2.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader_task2:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      outputs = model_task2(X)\n",
        "      soft_y = model_task1(X)\n",
        "\n",
        "      loss1 = loss_fn(outputs, y)\n",
        "\n",
        "      outputs_S = nn.functional.softmax(outputs[:, :out_features] / T, dim=1)\n",
        "      outputs_T = nn.functional.softmax(soft_y[:, :out_features] / T, dim=1)\n",
        "\n",
        "      loss2 = outputs_T.mul(-1 * torch.log(outputs_S))\n",
        "      loss2 = loss2.sum(1)\n",
        "      loss2 = loss2.mean() * T * T\n",
        "\n",
        "      loss = loss1 * alpha + loss2 * (1 - alpha)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      correct += predicted.eq(y).sum().item()\n",
        "      # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}, Avg Loss: {test_loss:>8f}\")\n"
      ],
      "metadata": {
        "id": "f31nqZx2WRdr"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(epoch):\n",
        "    model_task2.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(test_dataloader_task1):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model_task2(X)\n",
        "            _, predicted_old = outputs.max(1)\n",
        "            total += len(y)\n",
        "            correct += predicted_old.eq(y).sum().item()\n",
        "        print(f\"Validation Acc: {100. * correct / total}\\n\")"
      ],
      "metadata": {
        "id": "DbtfMR9VJW0Z"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 2\n",
        "alpha = 0.9\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model_task2.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch+1}: ----------------------\")\n",
        "    if epoch > 2:\n",
        "        for param in model_task2.parameters():\n",
        "            param.requires_grad = True\n",
        "        optimizer = torch.optim.SGD(model_task2.parameters(), lr=0.0001, momentum=0.9)\n",
        "    train(alpha, T)\n",
        "    test(alpha, T)\n",
        "    val(epoch)"
      ],
      "metadata": {
        "id": "tJwIbbRBXj-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e4358c-16fb-422c-ee40-1dc5f6ce049f"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: ----------------------\n",
            "Loss: 9.026002,    64/24000\n",
            "Loss: 6.970753,  6464/24000\n",
            "Loss: 5.930604, 12864/24000\n",
            "Loss: 5.666660, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 18.5, Avg Loss: 1.763167\n",
            "Validation Acc: 38.483333333333334\n",
            "\n",
            "Epoch 2: ----------------------\n",
            "Loss: 4.964440,    64/24000\n",
            "Loss: 4.754537,  6464/24000\n",
            "Loss: 5.003913, 12864/24000\n",
            "Loss: 4.465080, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 67.5, Avg Loss: 1.214632\n",
            "Validation Acc: 4.266666666666667\n",
            "\n",
            "Epoch 3: ----------------------\n",
            "Loss: 4.393607,    64/24000\n",
            "Loss: 4.129972,  6464/24000\n",
            "Loss: 4.054672, 12864/24000\n",
            "Loss: 4.213043, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 80.8, Avg Loss: 1.017046\n",
            "Validation Acc: 0.016666666666666666\n",
            "\n",
            "Epoch 4: ----------------------\n",
            "Loss: 3.811729,    64/24000\n",
            "Loss: 3.227595,  6464/24000\n",
            "Loss: 2.818718, 12864/24000\n",
            "Loss: 2.401695, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 91.2, Avg Loss: 0.506622\n",
            "Validation Acc: 0.0\n",
            "\n",
            "Epoch 5: ----------------------\n",
            "Loss: 2.101202,    64/24000\n",
            "Loss: 1.723558,  6464/24000\n",
            "Loss: 1.645300, 12864/24000\n",
            "Loss: 1.627644, 19264/24000\n",
            "Test Error: \n",
            " Accuracy: 92.5, Avg Loss: 0.344827\n",
            "Validation Acc: 0.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}